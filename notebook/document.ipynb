{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efa97f8",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Structuring and Ingestion\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e36839",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\n",
    "    page_content=\"This is the content of the document.\",\n",
    "    metadata={\"source\": \"user_upload\", \n",
    "              \"page_number\": 1,\n",
    "              \"author\": \"John Doe\",\n",
    "              \"ingestion_date\": \"2024-06-15\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f2b396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'user_upload', 'page_number': 1, 'author': 'John Doe', 'ingestion_date': '2024-06-15'}, page_content='This is the content of the document.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57405585",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple text file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b49968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created.\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"../data/text_files/python_intro.txt\": \"\"\"Tell me everything about Python programming.\n",
    "    Python is a high-level, interpreted programming language known for its simplicity, readability, and versatility. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is widely used in web development, data analysis, artificial intelligence, scientific computing, automation, and more.\n",
    "\n",
    "Python is popular because of its clean syntax, ease of learning, and extensive ecosystem of libraries and frameworks. Some of its key features include readability, interpreted execution (which makes debugging easier), dynamic typing (no need to declare variable types explicitly), cross-platform support, and strong community support.\n",
    "\n",
    "For data science and machine learning, Python offers powerful libraries like NumPy for numerical computations, Pandas for data manipulation, Matplotlib and Seaborn for visualization, Scikit-learn for machine learning algorithms, and TensorFlow and PyTorch for deep learning.\n",
    "\n",
    "Python is also used for web development through frameworks like Django and Flask, which simplify backend development and integration with databases and frontend technologies. Error handling in Python is managed using exceptions, typically with try and except blocks to catch and respond to runtime errors.\n",
    "\"\"\",\n",
    "\n",
    "\"../data/text_files/machine_learning.txt\": \"\"\"Explain the basics of Machine Learning.\n",
    "Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on enabling computers to learn patterns from data and make predictions or decisions without being explicitly programmed. Instead of following fixed instructions, ML algorithms improve their performance over time as they are exposed to more data. Machine Learning can be broadly categorized into supervised learning, where the model is trained on labeled data to make predictions; unsupervised learning, where the model identifies hidden patterns or groupings in unlabeled data; and reinforcement learning, where the model learns optimal actions through trial and error to maximize rewards.\n",
    "\n",
    "Key features of Machine Learning include the ability to handle large datasets, adaptability to new information, and the capacity to uncover complex relationships that are difficult for humans to identify manually. Popular algorithms include linear and logistic regression, decision trees, random forests, support vector machines, k-means clustering, and neural networks. For deep learning, which is a subfield of ML, algorithms such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are widely used for tasks like image recognition, natural language processing, and speech recognition.\n",
    "\n",
    "Machine Learning is applied in diverse domains such as healthcare (for disease diagnosis and drug discovery), finance (for fraud detection and risk assessment), marketing (for customer segmentation and recommendation systems), autonomous vehicles, robotics, and cybersecurity. Common tools and libraries used in ML include Python along with Scikit-learn, TensorFlow, PyTorch, Keras, Pandas, and NumPy.\n",
    "\n",
    "Developing ML models typically involves data preprocessing, feature engineering, selecting appropriate algorithms, training the model, evaluating its performance using metrics like accuracy, precision, recall, or F1-score, and iteratively tuning the model for better results. Overall, Machine Learning empowers systems to learn from data, make informed decisions, and continuously improve, making it a cornerstone of modern AI applications.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for file_path, content in sample_texts.items():\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(content)\n",
    "print(\"Sample text files created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b88398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Tell me everything about Python programming.\\n    Python is a high-level, interpreted programming language known for its simplicity, readability, and versatility. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is widely used in web development, data analysis, artificial intelligence, scientific computing, automation, and more.\\n\\nPython is popular because of its clean syntax, ease of learning, and extensive ecosystem of libraries and frameworks. Some of its key features include readability, interpreted execution (which makes debugging easier), dynamic typing (no need to declare variable types explicitly), cross-platform support, and strong community support.\\n\\nFor data science and machine learning, Python offers powerful libraries like NumPy for numerical computations, Pandas for data manipulation, Matplotlib and Seaborn for visualization, Scikit-learn for machine learning algorithms, and TensorFlow and PyTorch for deep learning.\\n\\nPython is also used for web development through frameworks like Django and Flask, which simplify backend development and integration with databases and frontend technologies. Error handling in Python is managed using exceptions, typically with try and except blocks to catch and respond to runtime errors.\\n')]\n"
     ]
    }
   ],
   "source": [
    "###Text Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\", encoding =\"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f13f8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Tell me everything about Python programming.\\n    Python is a high-level, interpreted programming language known for its simplicity, readability, and versatility. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is widely used in web development, data analysis, artificial intelligence, scientific computing, automation, and more.\\n\\nPython is popular because of its clean syntax, ease of learning, and extensive ecosystem of libraries and frameworks. Some of its key features include readability, interpreted execution (which makes debugging easier), dynamic typing (no need to declare variable types explicitly), cross-platform support, and strong community support.\\n\\nFor data science and machine learning, Python offers powerful libraries like NumPy for numerical computations, Pandas for data manipulation, Matplotlib and Seaborn for visualization, Scikit-learn for machine learning algorithms, and TensorFlow and PyTorch for deep learning.\\n\\nPython is also used for web development through frameworks like Django and Flask, which simplify backend development and integration with databases and frontend technologies. Error handling in Python is managed using exceptions, typically with try and except blocks to catch and respond to runtime errors.\\n'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Explain the basics of Machine Learning.\\nMachine Learning (ML) is a subset of artificial intelligence (AI) that focuses on enabling computers to learn patterns from data and make predictions or decisions without being explicitly programmed. Instead of following fixed instructions, ML algorithms improve their performance over time as they are exposed to more data. Machine Learning can be broadly categorized into supervised learning, where the model is trained on labeled data to make predictions; unsupervised learning, where the model identifies hidden patterns or groupings in unlabeled data; and reinforcement learning, where the model learns optimal actions through trial and error to maximize rewards.\\n\\nKey features of Machine Learning include the ability to handle large datasets, adaptability to new information, and the capacity to uncover complex relationships that are difficult for humans to identify manually. Popular algorithms include linear and logistic regression, decision trees, random forests, support vector machines, k-means clustering, and neural networks. For deep learning, which is a subfield of ML, algorithms such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are widely used for tasks like image recognition, natural language processing, and speech recognition.\\n\\nMachine Learning is applied in diverse domains such as healthcare (for disease diagnosis and drug discovery), finance (for fraud detection and risk assessment), marketing (for customer segmentation and recommendation systems), autonomous vehicles, robotics, and cybersecurity. Common tools and libraries used in ML include Python along with Scikit-learn, TensorFlow, PyTorch, Keras, Pandas, and NumPy.\\n\\nDeveloping ML models typically involves data preprocessing, feature engineering, selecting appropriate algorithms, training the model, evaluating its performance using metrics like accuracy, precision, recall, or F1-score, and iteratively tuning the model for better results. Overall, Machine Learning empowers systems to learn from data, make informed decisions, and continuously improve, making it a cornerstone of modern AI applications.\\n')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###DIrectory Loader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "directory_loader = DirectoryLoader(\n",
    "    \"../data/text_files\", \n",
    "    glob=\"*.txt\", \n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=False\n",
    "    )\n",
    "\n",
    "documents = directory_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1934e2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 0}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n1\\nA Review on Deep Learning Techniques for\\nVideo Prediction\\nSergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John Alejandro Castro-Vargas,\\nSergio Orts-Escolano, Jose Garcia-Rodriguez, and Antonis Argyros\\nAbstract—The ability to predict, anticipate and reason about future outcomes is a key component of intelligent decision-making\\nsystems. In light of the success of deep learning in computer vision, deep-learning-based video prediction emerged as a promising\\nresearch direction. Deﬁned as a self-supervised learning task, video prediction represents a suitable framework for representation\\nlearning, as it demonstrated potential capabilities for extracting meaningful representations of the underlying patterns in natural videos.\\nMotivated by the increasing interest in this task, we provide a review on the deep learning methods for prediction in video sequences.\\nWe ﬁrstly deﬁne the video prediction fundamentals, as well as mandatory background concepts and the most used datasets. Next, we\\ncarefully analyze existing video prediction models organized according to a proposed taxonomy, highlighting their contributions and\\ntheir signiﬁcance in the ﬁeld. The summary of the datasets and methods is accompanied with experimental results that facilitate the\\nassessment of the state of the art on a quantitative basis. The paper is summarized by drawing some general conclusions, identifying\\nopen research challenges and by pointing out future research directions.\\nIndex Terms—Video prediction, future frame prediction, deep learning, representation learning, self-supervised learning\\n!\\n1\\nINTRODUCTION\\nW\\nILL the car hit the pedestrian? That might be one\\nof the questions that comes to our minds when we\\nobserve Figure 1. Answering this question might be in\\nprinciple a hard task; however, if we take a careful look\\ninto the image sequence we may notice subtle clues that\\ncan help us predicting into the future, e.g., the person’s\\nbody indicates that he is running fast enough so he will\\nbe able to escape the car’s trajectory. This example is just\\none situation among many others in which predicting future\\nframes in video is useful. In general terms, the prediction\\nand anticipation of future events is a key component of\\nintelligent decision-making systems. Despite the fact that\\nwe, humans, solve this problem quite easily and effortlessly,\\nit is extremely challenging from a machine’s point of view.\\nSome of the factors that contribute to such complexity are\\nocclusions, camera movement, lighting conditions, clutter,\\nor object deformations. Even so, video prediction models\\nare able to extract rich spatio-temporal features from natural\\nvideos in a self-supervised fashion. This was fostered by the\\ngreat strides deep learning has made in different research\\nﬁelds such as human action recognition and prediction [1],\\nsemantic segmentation [2], and registration [3], to name\\na few. Because of their ability to learn adequate repre-\\nsentations from high-dimensional data [4], deep learning-\\n•\\nS. Oprea, P. M.-Gonzalez, J.A. C.-Vargas, and J. G.-Rodriguez are with\\nthe Department of Computer Technology. S. O.-Escolano is with the\\nDepartment of Computer Science and Artiﬁcial Intelligence. University\\nof Alicante, San Vicente del Raspeig, E-03690, Spain.\\nE-mail:{soprea,pmartinez,jacastro,jgarcia}@dtic.ua.es, sorts@dccia.ua.es.\\n•\\nA. G.-Garcia is with the Institute of Space Sciences (ICE-CSIC), Campus\\nUAB, Barcelona, E-08193, Spain. E-mail: garciagarcia@ice.csic.es.\\n•\\nA. Argyros is with the Institute of Computer Science, FORTH, Heraklion\\nGR-70013, Greece and with the Computer Science Department, Univer-\\nsity of Crete, Heraklion, Greece. E-mail: argyros@ics.forth.gr.\\nManuscript received April 19, 2005; revised August 26, 2015.\\n(Xt−n, . . . , Xt)\\nContext Frames\\nˆYt+1\\nPredicted Frames\\nˆYt+m\\n. . .\\nFig. 1. A pedestrian appeared from behind the white car with the\\nintention of crossing the street. The autonomous car must make a\\ncall: hit the emergency braking routine or not. This all comes down to\\npredict the next frames ( ˆYt+1, . . . , ˆYt+m) given a sequence of context\\nframes (Xt−n, . . . , Xt), where n and m denote the number of context\\nand predicted frames, respectively. From these predictions at a rep-\\nresentation level (RGB, high-level semantics, etc.) a decision-making\\nsystem would make the car avoid the collision.\\nbased models ﬁt perfectly into the learning by prediction\\nparadigm.\\n1.1\\nApplication Domains\\nVideo prediction methods have been successfully applied\\nin a broad range of application domains such as robotics,\\nautonomous driving, action anticipation, and more. Relying\\non action-conditioned video prediction, robots were able to\\nsuccessfully manipulate previously unseen objects [5]. In\\nthe same domain, video prediction has facilitated decision\\nmaking in vision-based robotic control [6] and motion plan-\\nning [7], [8] and has provided accurate world models, of\\nhigh-dimensional environments, to model-based Reinforce-\\nment Learning (RL) approaches. For instance, video predic-\\ntion has enabled planning in unknown environments [9],\\n[10]. This has helped model-based RL to achieve similar\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 1}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n2\\nor better results compared to model-free approaches, with\\nfewer interactions and improved generalization capabilities.\\nRegarding self-driving cars, the trajectory prediction in\\ntrafﬁc of pedestrians [11] or generic agents [12] is extremely\\nuseful to anticipate future events. Furthermore, the proba-\\nbilistic prediction of multi-modal futures [13] demonstrated\\ngreat success when it comes to trafﬁc uncertainty. Likewise,\\nthe synergy between video prediction and action anticipa-\\ntion was successfully proven with the prediction of visual\\nembeddings [14] and motion representations [15]. Some\\nother tasks in which video prediction has been applied suc-\\ncessfully are: prediction of instance/semantic segmentation\\nmaps [16], [17], [18], anomaly detection [19], precipitation\\nnowcasting [20], [21], and video interpolation [22].\\n1.2\\nReview Scope and Terminology\\nIn this review, we put our focus on deep learning techniques\\nand how they have been extended or applied to video pre-\\ndiction. We limit this review to the future video prediction\\ngiven the context of a sequence of previous frames, leaving\\naside methods that predict future from a static image. In this\\ncontext, the terms video prediction, future frame prediction,\\nnext video frame prediction, future frame forecasting, and\\nfuture frame generation are used interchangeably. To the\\nbest of our knowledge, this is the ﬁrst review in the liter-\\nature that focuses on video prediction using deep learning\\ntechniques.\\n2\\nVIDEO PREDICTION\\nBesides its biological roots, video prediction draws inspi-\\nration from computational models of the predictive coding\\nparadigm [23], [24], [25], [26]. Predictive coding states that\\nhuman brain builds complex mental representations of the\\nphysical and causal rules that govern the world. This arises\\nfrom the conceptual acquisition and the accumulation of\\nbackground knowledge from early ages, primarily through\\nobservation and interaction [27], [28], [29]. From a brain\\nprocessing perspective, these mental representations are\\ncontinuously updated through the prediction of raw sensory\\ninputs. The brain reﬁnes the already understood world\\nmodels from the mismatch between its predictions and the\\nactual sensory input [30].\\n2.1\\nProblem Deﬁnition\\nVideo prediction closely captures the essence of the pre-\\ndictive coding paradigm. On this basis, video prediction\\nis deﬁned as the task of inferring the subsequent frames\\nin a video, based on a sequence of previous frames used\\nas a context. Let Xt ∈Rw×h×c be the t-th frame in the\\nvideo sequence X = (Xt−n, . . . , Xt−1, Xt) with n frames,\\nwhere w, h, and c denote width, height, and number of\\nchannels, respectively. The target is to predict the next m\\nframes Y = ( ˆYt+1, ˆYt+2, . . . , ˆYt+m) from the input X.\\nDifferent from video generation that is mostly uncon-\\nditioned, video prediction is conditioned on a previously\\nlearned representation from a sequence of input frames. At\\na ﬁrst glance, and in the context of learning paradigms, one\\ncan think about the video prediction task as a supervised\\nlearning approach because the target frame acts as a label.\\nHowever, as this information is already available in the\\ninput video sequence, no extra labels or human supervi-\\nsion is needed. Therefore, learning by prediction is a self-\\nsupervised task, ﬁlling the gap between supervised and\\nunsupervised learning.\\nUnder the assumption that good predictions can only be\\nthe result of accurate representations, learning by prediction\\nis a feasible approach to verify how accurately the system\\nhas learned the underlying patterns in the input data. In\\nother words, it represents a suitable framework for rep-\\nresentation learning [31], [32]. Furthermore, because of its\\npotential to extract meaningful representations from video\\nsequences, video prediction is an excellent intermediate step\\nbetween natural videos and decision-making.\\n2.2\\nExploiting the Time Dimension of Videos\\nUnlike static images, videos provide complex transforma-\\ntions and motion patterns ordered in the time dimension.\\nFocusing on a small image patch in the same spatial location\\nthrough consecutive time steps, a wide range of visually\\nsimilar local deformations are identiﬁed due to the temporal\\ncoherence. In contrast, when looking at the big picture, the\\nconsecutive frames are visually different but semantically\\ncoherent. The variability in the visual appearance of a video\\nat different scales, is mainly due to occlusions, changes in\\nthe lighting conditions, and camera motion, among other\\nfactors. From this source of temporally ordered visual cues,\\npredictive models are able to extract representative spatio-\\ntemporal correlations depicting the dynamics in a video\\nsequence. For instance, Agrawal et al. [33] established a\\ndirect link between vision and motion, attempting to reduce\\nsupervision efforts when training deep predictive models.\\nThe importance of the time dimension in video under-\\nstanding models has been well studied [34]. The implicit\\ntemporal ordering in videos, also known as the arrow of\\ntime, indicates whether a video sequence is playing forward\\nor backward. Using this temporal direction as a supervisory\\nsignal [35], [36], [37] further encouraged predictive models\\nto implicitly or explicitly model spatio-temporal correlations\\nof a video sequence to understand the dynamics of a scene.\\nThe time dimension of a video reduces the supervision effort\\nand makes the prediction task self-supervised.\\n2.3\\nDealing with Stochasticity\\nPredicting how a square is moving, could be extremely\\nchallenging even in a deterministic environment such as\\nthe one represented in Figure 2. The lack of contextual\\ninformation and the multiple equally probable outcomes\\nhinder the prediction task. But, what if we use two con-\\nsecutive frames as context? Under this conﬁguration and\\nassuming a physically perfect environment, the square will\\nbe indeﬁnitely moving in the same direction. This represents\\na deterministic outcome, an assumption that many authors\\nmade in order to deal with future uncertainty. Assuming a\\ndeterministic outcome would narrow the prediction space to\\na unique solution. However, this assumption is not suitable\\nfor natural videos. The future is by nature multimodal, since\\nthe probability distribution deﬁning all the possible future\\noutcomes in a context has multiple modes, i.e. there are mul-\\ntiple equally probable and valid outcomes. Furthermore, on\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 2}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n3\\nContext Frame\\nTime\\nDeterministic\\nProbabilistic\\nFig. 2. At top, a deterministic environment where a geometric object, e.g.\\na black square, starts moving following a random direction. At bottom,\\nprobabilistic outcome. Darker areas correspond to higher probability\\noutcomes. As uncertainty is introduced, probabilities get blurry and\\naveraged. Figure inspired by [38].\\nthe basis of a deterministic universe, we indirectly assume\\nthat all possible outcomes are reﬂected in the input data.\\nThese assumptions make the prediction under uncertainty\\nan extremely challenging task.\\nMost of the existing deep learning-based models in the\\nliterature are deterministic. Although the future is uncertain,\\na deterministic prediction would sufﬁce some easily pre-\\ndictable situations. For instance, most of the movement of a\\ncar is largely deterministic, while only a small part is uncer-\\ntain. However, when multiple predictions are equally prob-\\nable, a deterministic model will learn to average between all\\nthe possible outcomes. This averaging effect depends on the\\nloss function and is visually represented in predictions as\\nblurriness, specially on long time horizons. However, it can\\nbe mitigated by constructing a loss function that does not\\nlead to averaging. As deterministic models are unable to\\nhandle real-world settings characterized by chaotic dynam-\\nics, authors considered that incorporating uncertainty to the\\nmodel is a crucial aspect. Probabilistic approaches dealing\\nwith these issues are discussed in Section 4.6.\\n2.4\\nThe Devil is in the Loss Function\\nThe design and selection of the loss function for the video\\nprediction task is of utmost importance. Pixel-wise losses,\\ne.g. ℓ2, ℓ1 and Mean-Squared Error (MSE), are widely used\\nin both unstructured and structured predictions. Although\\nleading to plausible predictions in deterministic scenarios,\\nsuch as synthetic datasets and video games, they struggle\\nwith the inherent uncertainty of natural videos. In a prob-\\nabilistic environment, with different equally probable out-\\ncomes, pixel-wise losses aim to accommodate uncertainty\\nby blurring the prediction, as we can observe in Figure 2. In\\nother words, the deterministic loss functions average out\\nmultiple equally plausible outcomes in a single, blurred\\nprediction. In the pixel space, these losses are unstable\\nto slight deformations and fail to capture discriminative\\nrepresentations to efﬁciently regress the broad range of\\npossible outcomes. This makes difﬁcult to draw predictions\\nmaintaining the consistency with our visual similarity no-\\ntion. A recent study [39] performed an in-depth analysis\\nof the generalization capabilities of different loss functions\\nfor the video prediction task. Besides video prediciton, the\\nimpact of different loss functions was analyzed in image\\nrestoration restoration [40], classiﬁcation [41], camera pose\\nregression [42] and structured prediction [43], among others.\\nThis fosters reasoning about the importance of the loss\\nfunction, particularly when making long-term predictions\\nin high-dimensional and multimodal natural videos.\\nMost of distance-based loss functions, such as based on\\nℓp norm, come from the assumption that data is drawn\\nfrom a Gaussian distribution. But, how these loss func-\\ntions address multimodal distributions? Assuming that\\na pixel is drawn from a bimodal distribution with two\\nequally likely modes Mo1 and Mo2, the mean value\\nMo = (Mo1 + Mo2)/2 would minimize the ℓp-based losses\\nover the data, even if Mo has very low probability [44]. This\\nsuggests that the average of two equally probable outcomes\\nwould minimize distance-based losses such as, the MSE\\nloss. However, this applies to a lesser extent when using\\nℓ1 norm as the pixel values would be the median of the two\\nequally likely modes in the distribution. In contrast to the ℓ2\\nnorm that emphasizes outliers with the squaring term, the ℓ1\\npromotes sparsity thus making it more suitable for predic-\\ntion in high-dimensional data [44]. Based on the ℓ2 norm,\\nthe MSE is also commonly used in the training of video\\nprediction models. However, it produces low reconstruction\\nerrors by merely averaging all the possible outcomes in\\na blurry prediction as uncertainty is introduced. In other\\nwords, the mean image would minimize the MSE error as\\nit is the global optimum, thus avoiding ﬁner details such\\nas facial features and subtle movements as they are noise\\nfor the model. Most of the video prediction approaches\\nrely on pixel-wise loss functions, obtaining roughly accurate\\npredictions in easily predictable datasets.\\nOne of the ultimate goals of many video prediction ap-\\nproaches is to palliate the blurry predictions when it comes\\nto uncertainty. For this purpose, authors broadly focused on:\\ndirectly improving the loss functions; exploring adversarial\\ntraining; alleviating the training process by reformulating\\nthe problem in a higher-level space; or exploring proba-\\nbilistic alternatives. Some promising results were reported\\nby combining the loss functions with sophisticated regular-\\nization terms, e.g. the Gradient Difference Loss (GDL) to\\nenhance prediction sharpness [44] and the Total Variation\\n(TV) regularization to reduce visual artifacts and enforce\\ncoherence [22]. Perceptual losses were also used to further\\nimprove the visual quality of the predictions [45], [46], [47],\\n[48], [49]. However, in light of the success of the Gener-\\native Adversarial Networks (GANs), adversarial training\\nemerged as a promising alternative to disambiguate be-\\ntween multiple equally probable modes. It was widely used\\nin conjunction with different distance-based losses such as:\\nMSE [50], ℓ2 [51], [52], [53], or a combination of them [44],\\n[54], [55], [56], [57], [58]. To alleviate the training process,\\nmany authors reformulated the optimization process in\\na higher-level space (see Section 4.5). While great strides\\nhave been made to mitigate blurriness, most of the existing\\napproaches still rely on distance-based loss functions. As\\na consequence, the regress-to-the-mean problem remains an\\nopen issue. This has further encouraged authors to reformu-\\nlate existing deterministic models in a probabilistic fashion.\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 3}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n4\\nTABLE 1\\nSummary of the most widely used datasets for video prediction (S/R: Synthetic/Real, st: stereo, de: depth, ss: semantic segmentation,\\nis: instance segmentation, sem: semantic, I/O: Indoor/Outdoor environment, bb: bounding box, Act: Action label, ann: annotated,\\nenv: environment, ToF: Time of Flight, vp: camera viewpoints respect human).\\nprovided data and ground-truth\\nname1\\nyear\\nS/R\\n#videos\\n#frames\\n#ann. frames\\nresolution\\n#classes\\nRGB\\nst\\nde\\nss\\nis\\nother annotations\\nenv.\\nAction and human pose recognition datasets\\nKTH [59]\\n2004\\nR\\n2391\\n250 0002\\n0\\n160 × 120\\n6 (action)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nAct.\\nO\\nWeizmann [60]\\n2007\\nR\\n90\\n90002\\n0\\n180 × 144\\n10 (action)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nAct.\\nO\\nHMDB-51 [61]\\n2011\\nR\\n6766\\n639 300\\n0\\nvar × 240\\n51 (action)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nAct., vp\\nI/O\\nUCF101 [62]\\n2012\\nR\\n13 320\\n2 000 0002\\n0\\n320 × 240\\n101 (action)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nAct.\\nI/O\\nPenn Action D. [63]\\n2013\\nR\\n2326\\n163 841\\n0\\n480 × 270\\n15 (action)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nAct., Human poses, vp\\nI/O\\nHuman3.6M [64]\\n2014\\nSR\\n40002\\n3 600 000\\n0\\n1000x1000\\n15 (action)\\n✓\\n\\x17\\nToF\\n\\x17\\n\\x17\\nAct., Human poses & meshes\\nI/O\\nTHUMOS-15 [65]\\n2017\\nR\\n18 404\\n3 000 0002\\n0\\n320 × 240\\n101 (action)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nAct., Time span\\nI/O\\nDriving and urban scene understanding datasets\\nCamvid [66]\\n2008\\nR\\n5\\n18 202\\n701 (ss)\\n960 × 720\\n32 (sem)\\n✓\\n\\x17\\n\\x17\\n✓\\n\\x17\\n\\x17\\nO\\nCalTech Pedest. [67]\\n2009\\nR\\n137\\n1 000 0002\\n250 000 (bb)\\n640 × 480\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nPedestrian bb & occlusions\\nO\\nKitti [68]\\n2013\\nR\\n151\\n48 791\\n200 (ss)\\n1392 × 512\\n30 (sem)\\n✓\\n✓\\nLiDAR\\n✓\\n✓\\nOdometry\\nO\\nCityscapes [69]\\n2016\\nR\\n50\\n7 000 0002\\n25 000 (ss)\\n2048 × 1024\\n30 (sem)\\n✓\\n✓\\nstereo\\n✓\\n✓\\nOdometry, temp, GPS\\nO\\nComma.ai [70]\\n2016\\nR\\n11\\n522 0002\\n0\\n160 × 320\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nSteering angles & speed\\nO\\nApolloscape [71]\\n2018\\nR\\n4\\n200 000\\n146 997 (ss)\\n3384 × 2710\\n25 (sem)\\n✓\\n✓\\nLiDAR\\n✓\\n✓\\nOdometry, GPS\\nO\\nnuScenes [72]\\n2019\\nR\\n1000\\n1 400 000\\n40 000 (bb, ss)\\n1600 × 900\\n32 (sem)\\n✓\\n\\x17\\nLiDAR\\n✓\\n\\x17\\nRadar, Odometry, GPS\\nO\\nWaymo Open D. [73]\\n2020\\nR\\n1950\\n200 000\\n200 000 (bb)\\n1920 × 1280\\n4 (sem)\\n✓\\n\\x17\\nLiDAR\\n\\x17\\n\\x17\\nOdometry, 2D/3D bb\\nO\\nObject and video classiﬁcation datasets\\nSports1m [74]\\n2014\\nR\\n1 133 158\\nn/a\\n0\\n640 × 360 (var.)\\n487 (sport)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nSport label\\nI/O\\nYouTube8M [75]\\n2016\\nR\\n8 200 000\\nn/a\\n0\\nvariable\\n1000 (topic)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nTopic label, Segment info\\nI/O\\nYFCC100M [76]\\n2016\\nSR\\n8000\\nn/a\\n0\\nvariable\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nUser tags, Localization\\nI/O\\nVideo prediction datasets\\nBouncing balls [77]\\n2008\\nS\\n4000\\n20 000\\n0\\n150 × 150\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n-\\nVan Hateren [78]\\n2012\\nR\\n56\\n3584\\n0\\n128 × 128\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nI/O\\nNORBvideos [79]\\n2013\\nR\\n110 560\\n552 800\\nAll (is)\\n640 × 480\\n5 (object)\\n✓\\n\\x17\\n\\x17\\n\\x17\\n✓\\n\\x17\\nI\\nMoving MNIST [80]\\n2015\\nSR\\ncustom3\\ncustom3\\n0\\n64 × 64\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n-\\nRobotic Pushing [5]\\n2016\\nR\\n57 000\\n1 500 0002\\n0\\n640 × 512\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nArm pose\\nI\\nBAIR Robot [81]\\n2017\\nR\\n45 000\\nn/a\\n0\\nn/a\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nArm pose\\nI\\nRoboNet [82]\\n2019\\nR\\n161 000\\n15 000 000\\n0\\nvariable\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nArm pose\\nI\\nOther-purpose and multi-purpose datasets\\nViSOR [83]\\n2010\\nR\\n1529\\n1 360 0002\\n0\\nvariable\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nUser tags, human bb\\nI/O\\nPROST [84]\\n2010\\nR\\n4 (10)\\n4936 (9296)\\nAll (bb)\\nvariable\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nObject bb\\nI\\nArcade Learning [85]\\n2013\\nS\\ncustom3\\ncustom3\\n0\\n210 × 160\\n-\\n✓\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n-\\nInria 3DMovie v2 [86]\\n2016\\nR\\n27\\n2476\\n235 (is)\\n960 × 540\\n-\\n✓\\n✓\\n\\x17\\n\\x17\\n✓\\nHuman poses, bb\\nI/O\\nRobotrix [87]\\n2018\\nS\\n67\\n3 039 252\\nAll (ss)\\n1920 × 1080\\n39 (sem)\\n✓\\n\\x17\\n✓\\n✓\\n✓\\nNormal maps, 6D poses\\nI\\nUASOL [88]\\n2019\\nR\\n33\\n165 365\\n0\\n2280 × 1282\\n-\\n✓\\n✓\\nstereo\\n\\x17\\n\\x17\\n\\x17\\nO\\n1 some dataset names have been abbreviated to enhance table’s readability.\\n2 values estimated based on the framerate and the total number of frames or videos, as the original values are not provided by the authors.\\n3 custom indicates that as many frames as needed can be generated. This is related to datasets generated from a game, algorithm or simulation, involving interaction or randomness.\\n3\\nDATASETS\\nAs video prediction models are mostly self-supervised, they\\nneed video sequences as input data. However, some video\\nprediction methods rely on extra supervisory signals, e.g.\\nsegmentation maps, and human poses. This makes out-of-\\ndomain video datasets perfectly suitable for video predic-\\ntion. Table 1 shows an overview of the most used datasets\\nfor video prediction. Detailed descriptions for each one of\\nthem can be found in the supplementary material.\\n4\\nVIDEO PREDICTION METHODS\\nIn the video prediction literature we ﬁnd a broad range of\\ndifferent methods and approaches. Early models focused\\non directly predicting raw pixel intensities, by implicitly\\nmodeling scene dynamics and low-level details (Section 4.1).\\nHowever, extracting a meaningful and robust representa-\\ntion from raw videos is challenging, since the pixel space\\nis highly dimensional and extremely variable. From this\\npoint, reducing the supervision effort and the representation\\ndimensionality emerged as a natural evolution. On the one\\nhand, the authors aimed to disentangle the factors of vari-\\nation from the visual content, i.e. factoring the prediction\\nspace. For this purpose, they: (1) formulated the prediction\\nproblem into an intermediate transformation space by ex-\\nplicitly modeling the source of variability as transforma-\\ntions between frames (Section 4.2); (2) separated motion\\nfrom the visual content with a two-stream computation\\n(Section 4.3). On the other hand, some models narrowed\\nthe output space by conditioning the predictions on extra\\nvariables (Section 4.4), or reformulating the problem in a\\nhigher-level space (Section 4.5). High-level representations\\nare increasingly more attractive for intelligent systems to\\nsupport their decision making. For instance, the semantic\\nsegmentation space is easily interpretable as the pixels are\\ncategorical, in contrast to unprocessed videos where pixels\\nrepresent raw intensities. Besides simplifying the prediction\\ntask, some other works addressed the future uncertainty\\nin predictions. As the vast majority of video prediction\\nmodels are deterministic, they are unable to manage proba-\\nbilistic environments. To address this issue, several authors\\nproposed modeling future uncertainty with probabilistic\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 4}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n5\\nVideo Prediction\\nThrough Direct\\nPixel Synthesis\\nImplicit Modeling\\nof Scene Dynamics\\nFactoring the\\nPrediction Space\\nExplicit Motion from\\nContent Separation\\nUsing Explicit\\nTransformations\\nNarrowing the\\nPrediction Space\\nBy Conditioning on\\nExtra Variables\\nTo High-level\\nFeature Space\\nBy Incorporating\\nUncertainty\\nUsing Probabilistic\\nApproaches\\nFig. 3. Classiﬁcation of video prediction models.\\nmodels (Section 4.6).\\nSo far in the literature, there is no speciﬁc taxonomy\\nthat classiﬁes video prediction models. In this review, we\\nhave classiﬁed the existing methods according to the video\\nprediction problem they addressed and following the clas-\\nsiﬁcation illustrated in Figure 3. For simplicity, each sub-\\nsection extends directly the last level in the taxonomy.\\nThe taxonomy is not mutually exclusive, as some methods\\ncan be classiﬁed into several categories since they address\\nmultiple goals. For instance, [17], [55], [89] are probabilistic\\nmodels making predictions in a high-level space as they\\naddress both the future uncertainty and high dimension-\\nality in videos. The category of these models was speciﬁed\\naccording to their main contribution.\\nThe most relevant methods, ordered in a chronologi-\\ncal order, are summarized in Table 2 containing low-level\\ndetails. From these details such as, the backbone architec-\\nture and loss functions, we could easily identify whether\\na model is probabilistic or deterministic. Furthermore, to\\nbetter understand the foundations of such methods, we\\nhave included a section on backbone deep learning architec-\\ntures in the supplementary material. Prediction is a widely\\ndiscussed topic in different ﬁelds and at different levels\\nof abstraction. For instance, the future prediction from a\\nstatic image [90], [91], [92], [93], [94], [95], human action\\nprediction [1], and model-based RL [9], [10], [96] are a\\ndifferent but inspiring research ﬁelds.\\nAlthough related, the aforementioned topics are outside\\nthe scope of this particular review, as it focuses purely on\\nthe video prediction methods using a sequence of previous\\nframes as context.\\n4.1\\nDirect Pixel Synthesis\\nInitial video prediction models attempted to directly predict\\nfuture pixel intensities without any explicit modeling of\\nthe scene dynamics. Ranzato et al. [97] discretized video\\nframes in patch clusters using k-means. They assumed that\\nnon-overlapping patches are equally different in a k-means\\ndiscretized space, yet similarities can be found between\\npatches. The method is a convolutional extension of a\\nRecurrent Neural Network (RNN)-based model [98] mak-\\ning short-term predictions at the patch-level using Cross\\nEntropy (CE) loss. As the full-resolution frame is a com-\\nposition of the predicted patches, some tilling effect can\\nbe noticed. Predictions of large and fast-moving objects\\nare accurate, however, when it comes to small and slow-\\nmoving objects there is still room for improvement. These\\nare common issues for most methods making predictions\\nat the patch-level. Addressing longer-term predictions, Sri-\\nvastava et al. [80] proposed several Autoencoder (AE)-\\nbased approaches incorporating Long Short-Term Mem-\\nory (LSTM) units to model the temporal coherence. Using\\nconvolutional [99] and ﬂow [100] percepts alongside RGB\\nimage patches, authors tested the models on multi-domain\\ntasks and considered both unconditioned and conditioned\\ndecoder versions. The latter only marginally improved the\\nprediction accuracy. Replacing the fully connected LSTMs\\nwith convolutional LSTMs, Shi et al. proposed an end-to-end\\nmodel efﬁciently exploiting spatial correlations [20]. This\\nenhanced prediction accuracy and reduced the number of\\nparameters.\\nInspired by adversarial training: Building on the recent\\nsuccess of the Laplacian Generative Adversarial Networks\\n(LAPGANs), Mathieu et al. proposed the ﬁrst multi-scale\\narchitecture for video prediction that was trained in an\\nadversarial fashion [44]. Their novel GDL regularization\\ncombined with ℓ1-based reconstruction and adversarial\\ntraining represented a leap over the previous state-of-the-\\nart models [80], [97] in terms of prediction sharpness. How-\\never, it was outperformed by the Predictive Coding Net-\\nwork (PredNet) [70] which stacked several convolutional\\nLSTMs (ConvLSTMs) vertically connected by a bottom-\\nup propagation of the local ℓ1 error computed at each\\nlevel. Previously to PredNet, the same authors proposed the\\nPredictive Generative Network (PGN) [50], an end-to-end\\nmodel trained with a weighted combination of adversarial\\nloss and MSE on synthetic data. Using a similar training\\nstrategy as [44], Zhou et al. used a convolutional AE to learn\\nlong-term dependencies from time-lapse videos [101]. Built\\non Progressively Growing GANs (PGGANs) [102], Aigner et\\nal. proposed the FutureGAN [103], a three-dimensional (3d)\\nconvolutional Encoder-decoder (ED)-based model. They\\nused the Wasserstein GAN with gradient penalty (WGAN-\\nGP) loss [104] and conducted experiments on increasingly\\ncomplex datasets. Extending [20], Zhang et al. proposed\\na novel LSTM-based architecture where hidden states are\\nupdated along a z-order curve [105]. Dealing with distortion\\nand temporal inconsistency in predictions and inspired by\\nthe Human Visual System (HVS), Jin et al. [106] ﬁrst incorpo-\\nrated multi-frequency analysis into the video prediction task\\nto decompose images into low and high frequency bands.\\nHigh-ﬁdelity and temporally consistent predictions with the\\nground truth were reported outperforming state of the art.\\nDistortion and blurriness are further accentuated when it\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 5}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n6\\ncomes to predict under fast camera motions. To this end,\\nShouno [107] implemented a hierarchical residual network\\nwith top-down connections. Leveraging parallel prediction\\nat multiple scales, authors reported ﬁner details and textures\\nunder fast and large camera motion.\\nBidirectional ﬂow: Under the assumption that video se-\\nquences are symmetric in time, Kwon et al. [108] explored a\\nretrospective prediction scheme training a generator also on\\nreversed input sequences. Their cycle GAN-based approach\\nensures the consistency of bidirectional prediction through\\nretrospective cycle constraints. Similarly, Hu et al. [58] pro-\\nposed a novel cycle-consistency loss used to train a GAN-\\nbased approach (VPGAN). Future frames are generated\\nfrom a sequence of context frames and their variation in\\ntime, denoted as Z. Under the assumption that Z is sym-\\nmetric in the encoding space, it is manipulated to generate\\ndesirable moving directions. In the same spirit, other works\\nfocused on both, forward and backward predictions [36],\\n[109]. Enabling state sharing between the encoder and de-\\ncoder, Oliu et al. proposed the folded Recurrent Neural\\nNetwork (fRNN) [110], a recurrent AE architecture featuring\\nGated Recurrent Units (GRUs) that implement a bidirec-\\ntional ﬂow of the information. The model demonstrated a\\nstratiﬁed representation, which makes the topology more\\nexplainable, as well as efﬁcient compared to regular AEs in\\nterms or memory consumption and computational require-\\nments.\\nExploiting 3D convolutions: for modeling short-term fea-\\ntures, Wang et al. [111] integrated them into a recurrent net-\\nwork demonstrating state-of-the-art results on both video\\nprediction and early activity recognition. While 3D convo-\\nlutions efﬁciently preserves local dynamics, RNNs enables\\nlong-range video reasoning. Their eidetic 3d LSTM (E3d-\\nLSTM) network features a gated-controlled self-attention\\nmodule, i.e. eidetic 3D memory, that effectively manages\\nhistorical memory records across multiple time steps. Out-\\nperforming previous works, Yu et al. proposed the Condi-\\ntionally Reversible Network (CrevNet) [112] consisting of\\ntwo modules, an invertible AE and a Reversible Predictive\\nModel (RPM). While the bijective two-way AE ensures no\\ninformation loss and reduces the memory consumption, the\\nRPM extends the reversibility from spatial to temporal do-\\nmain. Some other works used 3D convolutional operations\\nto model the time dimension [103].\\nAnalyzing the previous works, Byeon et al. [113] identi-\\nﬁed a lack of spatial-temporal context in the representations,\\nfact that leads to blurry results when dealing with uncer-\\ntainty. Although authors addressed this contextual limita-\\ntion with dilated convolutions and multi-scale architectures,\\nthe context representation progressively vanishes in long-\\nterm predictions. To address this issue, they proposed a\\ncontext-aware model that efﬁciently aggregates per-pixel\\ncontextual information at each layer and in multiple direc-\\ntions. The core of their proposal is a context-aware layer\\nconsisting of two blocks, one aggregating the information\\nfrom multiple directions and the other blending them into a\\nuniﬁed context.\\nIt\\nIt+1\\nIt+1(x, y) = f(It(x + u, y + v))\\n(x, y)\\n(x + u, y + v)\\n(a)\\nIt\\nIt+1\\n(x, y)\\n(x, y)\\nIt+1(x, y) = K(x, y) ∗P(x, y)\\nP(x, y)\\n(b)\\nFig. 4. Representation of transformation-based approaches. (a) Vector-\\nbased with a bilinear interpolation. (b) Kernel-based applying transfor-\\nmations as a convolutional operation. Figure inspired by [114].\\n4.2\\nUsing Explicit Transformations\\nLet X = (Xt−n, . . . , Xt−1, Xt) be a video sequence of n\\nframes, where t denotes time. Instead of learning the vi-\\nsual appearance, transformation-based approaches assume\\nthat visual information is already available in the input\\nsequence. To deal with the strong similarity and pixel redun-\\ndancy between successive frames, these methods explicitly\\nmodel the transformations that takes a frame at time t to the\\nframe at t+1. These models are formally deﬁned as follows:\\nYt+1 = T (G (Xt−n:t) , Xt−n:t) ,\\n(1)\\nwhere G is a learned function that outputs future trans-\\nformation parameters, which applied to the last observed\\nframe Xt using the function T , generates the future frame\\nprediction Yt+1. According to the classiﬁcation of Reda\\net al. [114], T function can be deﬁned as a vector-based\\nresampling such as bilinear sampling, or adaptive kernel-\\nbased resampling, e.g. using convolutional operations. For\\ninstance, a bilinear sampling operation is deﬁned as:\\nYt+1(x, y) = f (Xt(x + u, y + v)) ,\\n(2)\\nwhere f is a bilinear interpolator such as [22], [115], [116],\\n(u, v) is a motion vector predicted by G, and Xt(x, y)\\nis a pixel value at (x,y) in the last observed frame Xt.\\nApproaches following this formulation are categorized as\\nvector-based resampling operations and are depicted in\\nFigure 4a. On the other side, in the kernel-based resampling,\\nthe G function predicts the kernel K(x, y) which is applied\\nas a convolution operation using T , as depicted in Figure 4b\\nand is mathematically represented as follows:\\nYt+1(x, y) = K(x, y) ∗Pt(x, y),\\n(3)\\nwhere K(x, y) ∈RNxN is the 2D kernel predicted by the\\nfunction G and Pt(x, y) is an N ×N patch centered at (x, y).\\nCombining kernel and vector-based resampling into a\\nhybrid solution, Reda et al. [114] proposed the Spatially\\nDisplaced Convolution (SDC) module that synthesizes high-\\nresolution images applying a learned per-pixel motion vec-\\ntor and kernel at a displaced location in the source image.\\nTheir 3D Convolutional Neural Network (CNN) model\\ntrained on synthetic data and featuring the SDC modules,\\nreported promising predictions of a high-ﬁdelity.\\n4.2.1\\nVector-based Resampling\\nBilinear models use multiplicative interactions to extract\\ntransformations from pairs of observations in order to relate\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 6}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n7\\nimages, such as Gated Autoencoders (GAEs) [117]. Inspired\\nby these models, Michalski et al. proposed the Predictive\\nGating Pyramid (PGP) [118] consisting of a recurrent pyra-\\nmid of stacked GAEs. To the best of our knowledge, this\\nwas the ﬁrst attempt to predict future frames in the afﬁne\\ntransform space. Multiple GAEs are stacked to represent a\\nhierarchy of transformations and capture higher-order de-\\npendencies. From the experiments on predicting frequency\\nmodulated sin-waves, authors stated that standard RNNs\\nwere outperformed in terms of accuracy. However, no per-\\nformance comparison was conducted on videos.\\nBased on the Spatial Transformer (ST) module [119]:\\nTo provide spatial transformation capabilities to existing\\nCNNs, Jaderberg et al. [119] proposed the ST module. It\\nregresses different afﬁne transformation parameters for each\\ninput, to be applied as a single transformation to the whole\\nfeature map(s) or image(s). Moreover, it can be incorporated\\nat any part of the CNNs and it is fully differentiable. The\\nST module is the essence of vector-based resampling ap-\\nproaches for video prediction. As an extension, Patraucean\\net al. [66] modiﬁed the grid generator to consider per-\\npixel transformations instead of a single dense transfor-\\nmation map for the entire image. They nested a LSTM-\\nbased temporal encoder into a spatial AE, proposing the\\nAE-convLSTM-ﬂow architecture. The prediction is gener-\\nated by resampling the current frame with the ﬂow-based\\npredicted transformation. Using the components of the AE-\\nconvLSTM-ﬂow architecture, Lu et al. [120] assembled an ex-\\ntrapolation module which is unfolded in time for multi-step\\nprediction. Their Flexible Spatio-semporal Network (FSTN)\\nfeatures a novel loss function using the DeePSiM perceptual\\nloss [45] in order to mitigate blurriness. An exhaustive\\nexperimentation and ablation study was carried out, testing\\nmultiple combinations of loss functions. Also inspired by\\nthe ST module for the volume sampling layer, Liu et al.\\nproposed the Deep Voxel Flow (DVF) architecture [22]. It\\nconsists of a multi-scale ﬂow-based ED model originally\\ndesigned for the video frame interpolation task, but also\\nevaluated on a predictive basis reporting sharp results.\\nLiang et al. [56] use a ﬂow-warping layer based on a bilinear\\ninterpolation. Finn et al. proposed the Spatial Transformer\\nPredictor (STP) motion-based model [5] producing 2D afﬁne\\ntransformations for bilinear sampling. Pursuing efﬁciency,\\nAmersfoort et al. [121] proposed a CNN designed to predict\\nlocal afﬁne transformations of overlapping image patches.\\nUnlike the ST module, authors estimated transformations\\nof input frames off-line and at a patch level. As the model\\nis parameter-efﬁcient, it was unfolded in time for multi-\\nstep prediction. This resembles RNNs as the parameters are\\nshared over time and the local afﬁne transforms play the\\nrole of recurrent states.\\n4.2.2\\nKernel-based Resampling\\nAs a promising alternative to the vector-based resampling,\\nrecent approaches synthesize pixels by convolving input\\npatches with a predicted kernel. However, convolutional\\noperations are limited in learning spatial invariant repre-\\nsentations of complex transformations. Moreover, due to\\ntheir local receptive ﬁelds, global spatial information is not\\nfully preserved. Using larger kernels would help to pre-\\nserve global features, but in exchange for a higher memory\\nconsumption. Pooling layers are another alternative, but\\nloosing spatial resolution. Preserving spatial resolution at\\na low computational cost is still an open challenge for\\nfuture video frame prediction task. Transformation layers\\nused in vector-based resampling [22], [66], [119] enabled\\nCNNs to be spatially invariant and also inspired kernel-\\nbased architectures.\\nInspired by the Convolutional Dynamic Neural Advec-\\ntion (CDNA) module [5]: In addition to the STP vector-\\nbased model, Finn et al. [5] proposed two different kernel-\\nbased motion prediction modules outperforming previous\\napproaches [44], [122], (1) the Dynamic Neural Advection\\n(DNA) module predicting different distributions for each\\npixel and (2) the CDNA module that instead of predicting\\ndifferent distributions for each pixel, it predicts multiple\\ndiscrete distributions that are convolutionally applied to\\nthe input. While, CDNA and STP mask out objects that are\\nmoving in consistent directions, the DNA module produces\\nper-pixel motion. Similar to the CDNA module, Klein et\\nal. proposed the Dynamic Convolutional Layer (DCL) [123]\\nfor short-range weather prediction. Likewise, Brabandere\\net al. [124] proposed the Dynamic Filter Networks (DFN)\\ngenerating sample (for each image) and position-speciﬁc\\n(for each pixel) kernels. This enabled sophisticated and\\nlocal ﬁltering operations in comparison with the ST module,\\nthat is limited to global spatial transformations. Different\\nto the CDNA model, the DFN uses a softmax layer to\\nﬁlter values of greater magnitude, thus obtaining sharper\\npredictions. Moreover, temporal correlations are exploited\\nusing a parameter-efﬁcient recurrent layer, much simpler\\nthan [20], [80]. Exploiting adversarial training, Vondrick et\\nal. proposed a conditional Generative Adversarial Network\\n(cGAN)-based model [125] consisting of a discriminator\\nsimilar to [126], and a CNN generator featuring a trans-\\nformer module inspired by the CDNA model. Different from\\nthe CDNA model, transformations are not applied recur-\\nrently on a per-frame basis. To deal with in-the-wild videos\\nand make predictions invariant to camera motion, authors\\nstabilized the input videos. However, no performance com-\\nparison with previous works has been conducted. Improv-\\ning [127], Luc et al. [128] proposed the Transformation-based\\n& TrIple Video Discriminator GAN (TrIVD-GAN-FP) fea-\\nturing a novel recurrent unit that computes the parameters\\nof a transformation used to warp previous hidden states\\nwithout any supervision. These Transformation-based Spa-\\ntial Recurrent Units (TSRUs) are generic modules and can\\nreplace any traditional recurrent unit in currently existent\\nvideo prediction approaches.\\nObject-centric representation: Instead of focusing on the\\nwhole input, Chen et al. [51] modeled individual motion of\\nlocal objects, i.e. object-centered representations. Based on\\nthe ST module and a pyramid-like sampling [129], authors\\nimplemented an attention mechanism for object selection.\\nMoreover, transformation kernels were generated dynami-\\ncally as in the DFN, to then apply them to the last patch\\ncontaining an object. Although object-centered predictions\\nis novel, performance drops when dealing with multiple\\nobjects and occlusions as the attention module fails to dis-\\ntinguish them correctly.\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 7}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n8\\n4.3\\nExplicit Motion from Content Separation\\nDrawing inspiration from two-stream architectures for ac-\\ntion recognition [130], and unconditioned video generation\\n[131], authors decided to factorize the video into content\\nand motion to process each on a separate pathway. By\\nfactoring videos, the prediction is performed on a lower-\\ndimensional temporal dynamics separately from the spatial\\nlayout. Although this makes end-to-end training difﬁcult,\\nsplitting the prediction task into more tractable problems\\ndemonstrated good results.\\nThe Motion-content Network (MCnet) [132] was the ﬁrst\\nend-to-end model in disentangling scene dynamics from\\nthe visual appearance, i.e. motion-content factorization. It\\nproved better generalization capabilities and stable long-\\nterm predictions compared to [44], [80]. In a similar fashion,\\nyet working in a higher-level pose space, Denton et al. pro-\\nposed Disentangled-representation Net (DRNET) [133] us-\\ning a novel adversarial loss —it isolates the scene dynamics\\nfrom the visual content, considered as the discriminative\\ncomponent— to completely disentangle motion dynam-\\nics from content. Outperforming [44], [132], the DRNET\\ndemonstrated a clean motion from content separation by\\nreporting plausible long-term predictions on both synthetic\\nand natural videos. To improve prediction variability, Liang\\net al. [56] fused the future-frame and future-ﬂow predic-\\ntion into a uniﬁed architecture with a shared probabilistic\\nmotion encoder. Aiming to mitigate the ghosting effect in\\ndisoccluded regions, Gae et al. [134] proposed a two-staged\\napproach consisting of a separate computation of ﬂow and\\npixel predictions. As they focused on inpainting occluded\\nregions of the image using ﬂow information, they improved\\nresults on disoccluded areas avoiding undesirable artifacts\\nand enhancing sharpness. Wu et al. [135] proposed a two-\\nstaged architecture that ﬁrstly predicts the static background\\nto then, using this information, predict the moving objects in\\nthe foreground. Final output is generated through composi-\\ntion and by means of a video inpainting module. Reported\\npredictions are quite accurate, yet performance was not\\ncontrasted with the latest video prediction models.\\nAlthough previous approaches disentangled motion\\nfrom content, they have not performed an explicit de-\\ncomposition of videos into primitive object representations.\\nAddressing this issue, Hsieh et al. proposed the Decomposi-\\ntional Disentangled Predictive Autoencoder (DDPAE) [136]\\nthat decomposes videos into components featuring low-\\ndimensional temporal dynamics. For instance, on the Mov-\\ning MNIST dataset, DDPAE ﬁrst decomposes images into\\nindividual digits. After that, each digit is factorized into\\nits visual appearance and spatial location, being the latter\\neasier to predict. Although experiments were performed\\nonly on synthetic data, this model is a promising baseline\\nencouraging predictive models to explore visual representa-\\ntion decomposition [137], [138], [139].\\n4.4\\nConditioned on Extra Variables\\nConditioning the prediction on extra variables such as ve-\\nhicle odometry or robot state, among others, would narrow\\nthe prediction space. These variables have a direct inﬂuence\\non the dynamics of the scene, providing valuable informa-\\ntion that facilitates the prediction task. For instance, the\\nmotion captured by a camera placed on the dashboard of\\nan autonomous vehicle is directly inﬂuenced by the wheel-\\nsteering and acceleration. Without explicitly exploiting this\\ninformation, we rely blindly on the model’s capabilities\\nto correlate the wheel-steering and acceleration with the\\nperceived motion.\\nFollowing this paradigm, Oh et al. ﬁrst performed long-\\nterm video predictions conditioned by control inputs from\\nAtari games [122]. Although the proposed ED-based models\\nreported very long-term predictions (+100), performance\\ndrops when dealing with small objects (e.g. bullets in Space\\nInvaders) and uncertainty. However, ℓ2 loss leads to accu-\\nrate and long-term predictions for deterministic synthetic\\nvideos, such as those extracted from Atari video games.\\nBuilt on [122], Chiappa et al. [140] proposed alternative\\narchitectures and training schemes alongside an in-depth\\nperformance analysis for both short and long-term pre-\\ndiction. Similar to [122], Kaiser et al. [10] recently used a\\nconvolutional ED to learn a world model of Atari games in\\na self-supervised fashion. This is part of their model-based\\nRL approach called Simulated Policy Learning (SimPLe)\\nfocused on learning to play Atari games. In the experi-\\nments, SimPLe outperforms previous model-free algorithms\\nrequiring only 1-2 hours of in-game interactions. Similar\\nmodel-based control from visual inputs performed well in\\nrestricted scenarios [141], but was inadequate for uncon-\\nstrained environments.\\nDeterministic approaches are unable to deal with natural\\nvideos in the absence of control variables. To address this\\nlimitation, the models proposed by Finn et al. [5] success-\\nfully made predictions on natural images, conditioned on\\nthe robot state and robot-object interactions performed in a\\ncontrolled scenario. These models predict per-pixel trans-\\nformations conditioned by the previous frame, to ﬁnally\\ncombine them using a composition mask. They outper-\\nformed [44], [122] on both conditioned and unconditioned\\npredictions, however the quality of long-term predictions\\ndegrades over time because of the blurriness caused by\\nthe MSE loss function. Furthermore, Dosovitskiy et al. [142]\\nproposed a sensorimotor control model which enables in-\\nteraction in complex and dynamic 3d environments. The\\napproach is a RL-based technique, with the difference\\nthat instead of building upon a monolithic state and a\\nscalar reward, the authors consider high-dimensional input\\nstreams, such as raw visual input, alongside a stream of\\nmeasurements or player statistics. Although the outputs\\nare future measurements instead of visual predictions, it\\nwas proven that using multivariate data beneﬁts decision-\\nmaking over conventional scalar reward approaches. The\\nsynergy between model-based RL [9], [10], [96] and video\\nprediction is well deﬁned as the latter aims to model an\\naccurate representation of high-dimensional environments,\\nwhile the former uses the learned world models as a context\\nfor decision-making.\\n4.5\\nIn the High-level Feature Space\\nDespite the vast work on video prediction models, there\\nis still room for improvement in natural video prediction.\\nTo deal with the curse of dimensionality, authors reduced\\nthe prediction space to higher-level representations, such\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 8}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n9\\nas semantic and instance segmentation, and human pose.\\nSince the pixels are categorical, the semantic space greatly\\nsimpliﬁes the prediction task, yet unexpected deformations\\nin semantic maps and disocclusions, i.e. initially occluded\\nscene entities become visible, induce uncertainty. However,\\nhigh-level prediction spaces are more tractable and consti-\\ntute good intermediate representations. By bypassing the\\nprediction in the raw pixel space, models reported longer-\\nterm and more accurate predictions.\\n4.5.1\\nSemantic Segmentation\\nBy decomposing the visual scene into semantic entities, such\\nas pedestrians, vehicles and obstacles, the output space is\\nnarrowed to high-level scene properties. This intermediate\\nrepresentation represents a more tractable space as pixel\\nvalues of a semantic map are categorical. In other words,\\nscene dynamics are modeled at the semantic entity level.\\nThis has encouraged authors to (1) leverage future predic-\\ntion to improve parsing results [52] and (2) directly predict\\nsegmentation maps into the future [16], [57], [143].\\nExploring the scene parsing in future frames, Jin et al.\\nproposed the Parsing with prEdictive feAtuRe Learning\\n(PEARL) framework [52] which was the ﬁrst to explore the\\npotential of a GAN-based predictive model to improve per-\\npixel segmentation. Speciﬁcally, this framework conducts\\ntwo complementary predictive learning tasks. Firstly, it cap-\\ntures the temporal context from input data by using a single-\\nframe prediction network. Then, these temporal features\\nare embedded into a frame parsing network through a\\ntransform layer for generating per-pixel future segmenta-\\ntions. Although the prediction model was not compared\\nwith existing approaches, PEARL outperforms the tradi-\\ntional parsing methods by generating temporally consistent\\nsegmentations. In a similar fashion, Luc et al. [57] extended\\nthe msCNN model of [44] to the novel task of predicting\\nsemantic segmentations of future frames, using softmax pre-\\nactivations instead of raw pixels as input. The use of inter-\\nmediate features or higher-level data as input is a common\\npractice in the video prediction performed in the high-level\\nfeature space. Some authors refer to this type or input data\\nas percepts. Luc et al. explored different combinations of\\nloss functions, inputs (using RGB information alongside\\npercepts), and outputs (autoregressive and batch models).\\nResults on short, medium and long-term predictions are\\nsound, however, the models are not end-to-end and they\\ndo not capture explicitly the temporal continuity across\\nframes. To address this limitation and extending [52], Jin\\net al. ﬁrst proposed a model for jointly predicting motion\\nﬂow and scene parsing [144]. Flow-based representations\\nimplicitly draw temporal correlations from the input data,\\nthus producing temporally consistent segmentations. Per-\\npixel accuracy improved when segmenting small objects,\\ne.g. pedestrians and trafﬁc signs, which are more likely to\\nvanish in long-term predictions. Similarly, except that time\\ndimension is modeled with a LSTMs instead of motion ﬂow\\nestimation, Nabavi et al. proposed a simple bidirectional ED-\\nLSTM [145] using segmentation masks as input. Although\\nthe literature on knowledge distillation [146], [147] stated\\nthat softmax pre-activations carry more information than\\nclass labels, this model outperforms [57], [144] on short-term\\npredictions.\\nUsing motion ﬂow estimation alongside LSTM-based\\ntemporal modeling, Terwilliger et al. [18] proposed a novel\\nmethod performing a LSTM-based feature-ﬂow aggrega-\\ntion. Authors further simplify the semantic space by disen-\\ntangling motion from semantic entities [132], achieving low\\noverhead and efﬁciency. Therefore, they segment the current\\nframe and perform future optical ﬂow prediction, which\\nare ﬁnally combined with a novel end-to-end warp layer.\\nAn improvement on short-term predictions was reported\\nover previous works [57], [144], yet performing worse on\\nmid-term predictions. Similar to [18], F2MF model [148]\\npredict semantic segmented frames by wrapping past con-\\nvolutional features into the future using a regressed dense\\ndisplacement ﬁeld. To deal with disocclusions, authors\\ncomplemented the main model with a classical feature-to-\\nfeature forecast module similar to [16], [149]. F2MF outper-\\nformed previous works on the CityScapes dataset without\\nusing structure information [150] or precomputed optical\\nﬂow [18].\\nA different approach was proposed by Vora et al. [150]\\nwhich ﬁrst incorporated structure information to predict\\nfuture 3D segmented point clouds. Their geometry-based\\nmodel consists of several derivable sub-modules: (1) the\\npixel-wise segmentation and depth estimation modules\\nwhich are jointly used to generate the 3d segmented point\\ncloud of the current RGB frame; and (2) an LSTM-based\\nmodule trained to predict future camera ego-motion trajec-\\ntories. The future 3d segmented point clouds are obtained\\nby transforming the previous point clouds with the pre-\\ndicted ego-motion. Their short-term predictions improved\\nthe results of [57], however, the use of structure information\\nfor longer-term predictions is not clear.\\nThe main disadvantage of two-staged, i.e. not end-to-\\nend, approaches [18], [57], [144], [145], [150] is that their per-\\nformance is constrained by external supervisory signals, e.g.\\noptical ﬂow [151], segmentation [152] and intermediate fea-\\ntures or percepts [153]. Breaking this trend, Chiu et al. [149]\\nﬁrst solved both the semantic segmentation and forecasting\\nproblems in a single end-to-end trainable model by using\\nraw pixels as input. This ED architecture is based on two\\nnetworks: the student, performing the forecasting task, and\\nthe teacher guiding the student using a novel knowledge\\ndistillation loss. An in-depth ablation study was performed,\\nvalidating the performance of the ED architectures as well\\nas the 3D convolution used for capturing the temporal scale\\ninstead of a LSTM or ConvLSTM, as in previous works.\\nAvoiding the ﬂood of deterministic models, Bhat-\\ntacharyya et al. proposed a Bayesian formulation of the\\nResNet model in a novel architecture to capture model and\\nobservation uncertainty [17]. As a main contribution, their\\ndropout-based Bayesian approach leverages synthetic likeli-\\nhoods [154] to encourage prediction diversity and deal with\\nmulti-modal outcomes. Since Cityscapes sequences have\\nbeen recorded in the frame of reference of a moving vehicle,\\nauthors conditioned the predictions on vehicle odometry.\\n4.5.2\\nInstance Segmentation\\nWhile great strides have been made in predicting future\\nsegmentation maps, the authors attempted to make predic-\\ntions at a semantically richer level, i.e. future prediction of\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 9}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n10\\nsemantic instances. Predicting future instance-level segmen-\\ntations is a challenging and weakly unexplored task. This\\nis because instance labels are inconsistent and variable in\\nnumber across the frames in a video sequence. Since the\\nrepresentation of semantic segmentation prediction models\\nis of ﬁxed-size, they cannot directly address semantics at the\\ninstance level.\\nTo overcome this limitation and introducing the novel\\ntask of predicting instance segmentations, Luc et al. [16]\\npredict ﬁxed-sized feature pyramids, i.e. features at multiple\\nscales, used by the Mask R-CNN [155] network. The com-\\nbination of dilated convolutions and multi-scale, efﬁciently\\npreserve high-resolution details improving the results over\\nprevious methods [57]. To further improve predictions, Sun\\net al. [156] focused on modeling not only the spatio-temporal\\ncorrelations between the pyramids, but also the intrinsic\\nrelations among the feature layers inside them. That is,\\nenriching the contextual information using the proposed\\nContext Pyramid ConvLSTMs (CP-ConvLSTMs). Although\\nthe authors have not shown any long-term predictions\\nnor compared with semantic segmentation models, their\\napproach is the state of the art in the task of predicting\\ninstance segmentations.\\n4.5.3\\nOther High-level Spaces\\nAlthough semantic and instance segmentation spaces were\\nthe most used in video prediction, other high-level spaces\\nsuch as human pose and keypoints, represent a promising\\navenue.\\nHuman Pose: As the human pose is a low-dimensional\\nand interpretable structure, it represents a cheap supervi-\\nsory signal for predictive models. This has fostered pose-\\nguided prediction methods, in which pixel-level predictions\\nare conditioned by intermediate representations of human\\nposes. However, most of these methods are limited to videos\\nwith human presence.\\nFrom a supervised prediction of human poses, Villegas et\\nal. [54] regress future frames through analogy making [157].\\nAlthough background is not considered in the prediction,\\nauthors compared the model against [20], [44] reporting\\nlong-term results. To make the model unsupervised on the\\nhuman pose, Wichers et al. [53] adopted different training\\nstrategies: end-to-end prediction minimizing the ℓ2 loss,\\nand through analogy making, constraining the predicted\\nfeatures to be close to the outputs of the future encoder.\\nDifferent from [54], predictions are made in the feature\\nspace. As a probabilistic alternative, Walker et al. [55] fused\\na conditioned Variational Autoencoder (cVAE)-based prob-\\nabilistic pose predictor with a GAN. While the probabilistic\\npredictor enhances the diversity in the predicted poses,\\nthe adversarial network ensures prediction realism. As this\\nmodel struggles with long-term predictions, Fushishita et\\nal. [158] addressed long-term video prediction of multiple\\noutcomes avoiding the error accumulation and vanishing\\ngradients by using a unidimensional CNN trained in an\\nadversarial fashion. To enable multiple predictions, they\\nhave used additional inputs ensuring trajectory and behav-\\nior variability at a human pose level. To better preserve the\\nvisual appearance in the predictions than [54], [132], [159],\\nTang et al. [160] ﬁrstly predict human poses using a LSTM-\\nbased model to then synthesize pose-conditioned future\\nframes using a combination of different networks: a global\\nGAN modeling the time-invariant background alongside\\na coarse human pose, a local GAN reﬁning the coarse-\\npredicted human pose, and a 3D-AE to ensure temporal\\nconsistency across frames.\\nKeypoint-based representations: The keypoint coordinate\\nspace is a meaningful, tractable and structured represen-\\ntation for prediction, ensuring stable learning. It enforces\\nmodel’s internal representation to contain object-level in-\\nformation. This leads to better results on tasks requiring\\nobject-level understanding such as, trajectory prediction,\\naction recognition and reward prediction. As keypoints are\\na natural representation of dynamic objects, Minderer et\\nal. [89] reformulated the prediction task in the keypoint\\ncoordinate space. They proposed an AE architecture with\\na bottleneck consisting of a Variational Recurrent Neural\\nNetwork (VRNN) that predicts dynamics in the keypoint\\nspace. Although this model qualitatively outperforms the\\nStochastic Video Generation (SVG) [161], Stochastic Adver-\\nsarial Video Prediction (SAVP) [159] and Encoder Predictor\\nwith Visual Analogy (EPVA) [53] models, the quantitative\\nevaluation reported similar results.\\n4.6\\nIncorporating Uncertainty\\nAlthough high-level representations signiﬁcantly reduce the\\nprediction space, the underlying distribution still has mul-\\ntiple modes. In other words, different plausible outcomes\\nwould be equally probable for the same input. Addressing\\nmultimodal distributions is not straightforward for regres-\\nsion and classiﬁcation approaches, as they regress to the\\nmean and aim to discretize a continuous high-dimensional\\nspace, respectively. To deal with the inherent unpredictabil-\\nity of natural videos, some works introduced latent vari-\\nables into existing deterministic models or directly relied on\\ngenerative models such as GANs and Variational Autoen-\\ncoders (VAEs).\\nInspired by DVF, Xue et al. [186] proposed a cVAE-\\nbased [187], [188] multi-scale model featuring a novel cross\\nconvolutional layer trained to regress the difference image\\nor Eulerian motion [189]. Background on natural videos is\\nnot uniform, however the model implicitly assumes that the\\ndifference image would accurately capture the movement\\nin foreground objects. Introducing latent variables into a\\nconvolutional AE, Goroshin et al. [175] proposed a proba-\\nbilistic model for learning linearized feature representations\\nto linearly extrapolate the predicted frame in a feature space.\\nUncertainty is introduced to the loss by using a cosine\\ndistance as an explicit curvature penalty. Authors focused\\non evaluating the linearization properties, yet the model\\nwas not contrasted to previous works. Extending [92],\\n[186], Fragkiadaki et al. [177] proposed several architectural\\nchanges and training schemes to handle marginalization\\nover stochastic variables, such as sampling from the prior\\nand variational inference. Their stochastic ED architecture\\npredicts future optical ﬂow, i.e., dense pixel motion ﬁeld,\\nused to spatially transform the current frame into the next\\nframe prediction. To introduce uncertainty in predictions,\\nthe authors proposed the k-best-sample-loss (MCbest) that\\ndraws K outcomes penalizing those similar to the ground-\\ntruth.\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 10}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n11\\nTABLE 2\\nSummary of video prediction models (c: convolutional; r: recurrent; v: variational; ms: multi-scale; st: stacked; bi: bidirectional; P: Percepts; M:\\nMotion; PL: Perceptual Loss; AL: Adversarial Loss; S/R: using Synthetic/Real datasets; SS: Semantic Segmentation; D: Depth; S: State; Po:\\nPose; O: Odometry; IS: Instance Segmentation; MS: Multi-Step prediction; npf: num. of predicted frames, ⋆1-5, ⋆⋆5-10, ⋆⋆⋆10-100, ⋆⋆⋆⋆\\nover 100 frames; ood: tested on out-of-domain tasks).\\ndetails\\nevaluation\\nmethod\\nyear\\narchitecture\\ndatasets (train, valid, test)\\ninput\\noutput\\nMS\\nloss function\\nS/R\\nnpf\\nood\\ncode\\nDirect Pixel Synthesis\\nRanzato et al. [97]\\n2014\\nrCNN\\n[62], [78]\\nRGB\\nRGB\\n\\x17\\nCE\\nR\\n⋆\\n\\x17\\n\\x17\\nSrivastava et al. [80]\\n2015\\nLSTM-AE\\n[61], [62], [74], [80]\\nRGB,P\\nRGB\\n✓\\nCE, ℓ2\\nSR\\n⋆⋆⋆\\n✓\\n✓\\nPGN [50]\\n2015\\nLSTM-cED\\n[77]\\nRGB\\nRGB\\n\\x17\\nMSE, AL\\nS\\n⋆\\n\\x17\\n\\x17\\nShi et al. [20]\\n2015\\ncLSTM\\n[80]\\nRGB\\nRGB\\n\\x17\\nCE\\nS\\n⋆⋆⋆\\n✓\\n\\x17\\nBeyondMSE [44]\\n2016\\nmsCNN\\n[62], [74]\\nRGB\\nRGB\\n✓\\nℓ1, GDL, AL\\nR\\n⋆⋆\\n\\x17\\n✓\\nPredNet [70]\\n2017\\nstLSTMs\\n[64], [67], [68], [162]\\nRGB\\nRGB\\n✓\\nℓ1,ℓ2\\nSR\\n⋆⋆\\n✓\\n✓\\nContextVP [113]\\n2018\\nMD-LSTM\\n[62], [64], [67], [68]\\nRGB\\nRGB\\n✓\\nℓ1, GDL\\nR\\n⋆⋆\\n\\x17\\n\\x17\\nfRNN [110]\\n2018\\ncGRU-AE\\n[59], [62], [80]\\nRGB\\nRGB\\n✓\\nℓ1\\nSR\\n⋆⋆⋆\\n\\x17\\n✓\\nE3d-LSTM [111]\\n2019\\nr3D-CNN\\n[59], [80], [163], [164]\\nRGB\\nRGB\\n✓\\nℓ1, ℓ2, CE\\nSR\\n⋆⋆⋆\\n✓\\n✓\\nKwon et al. [108]\\n2019\\ncycleGAN\\n[62], [67], [68], [165], [166]\\nRGB\\nRGB\\n✓\\nℓ1, LoG, AL\\nR\\n⋆⋆⋆\\n\\x17\\n\\x17\\nZnet [105]\\n2019\\ncLSTM\\n[59], [80]\\nRGB\\nRGB\\n✓\\nℓ2, BCE, AL\\nSR\\n⋆⋆⋆\\n\\x17\\n\\x17\\nVPGAN [58]\\n2019\\nGAN\\n[59], [81]\\nRGB,Z\\nRGB\\n✓\\nℓ1, Lcycle, AL\\nR\\n⋆⋆⋆\\n\\x17\\n\\x17\\nJin et al. [106]\\n2020\\ncED-GAN\\n[59], [67], [68], [81]\\nRGB\\nRGB\\n✓\\nℓ2, GDL, AL\\nR\\n⋆⋆⋆\\n\\x17\\n\\x17\\nShouno et al. [107]\\n2020\\nGAN\\n[67], [68]\\nRGB\\nRGB\\n✓\\nLp, AL, PL\\nR\\n⋆⋆⋆\\n\\x17\\n\\x17\\nCrevNet [112]\\n2020\\n3d-cED\\n[67], [68], [80], [167]\\nRGB\\nRGB\\n✓\\nMSE\\nSR\\n⋆⋆⋆\\n✓\\n✓\\nUsing Explicit Transformations\\nPGP [118]\\n2014\\nst-rGAEs\\n[77], [79]\\nRGB\\nRGB\\n✓\\nℓ2\\nSR\\n⋆\\n\\x17\\n\\x17\\nPatraucean et al. [66]\\n2015\\nLSTM-cAE\\n[61], [80], [83], [84]\\nRGB\\nRGB\\n\\x17\\nℓ2, ℓδ\\nSR\\n⋆\\n✓\\n✓\\nDFN [124]\\n2016\\nr-cED\\n[62], [80]\\nRGB\\nRGB\\n✓\\nBCE\\nSR\\n⋆⋆⋆\\n✓\\n✓\\nAmersfoort et al. [121]\\n2017\\nCNN\\n[62], [80]\\nRGB\\nRGB\\n✓\\nMSE\\nSR\\n⋆⋆\\n\\x17\\n\\x17\\nFSTN [120]\\n2017\\nLSTM-cED\\n[62], [74], [80], [83], [84]\\nRGB\\nRGB\\n✓\\nℓ2, ℓδ, PL\\nSR\\n⋆⋆⋆\\n\\x17\\n\\x17\\nVondrick et al. [125]\\n2017\\ncGAN\\n[76]\\nRGB\\nRGB\\n✓\\nCE, AL\\nR\\n⋆⋆⋆\\n✓\\n\\x17\\nChen et al. [51]\\n2017\\nrCNN-ED\\n[62], [80]\\nRGB\\nRGB\\n✓\\nCE, ℓ2, GDL, AL\\nSR\\n⋆⋆\\n\\x17\\n\\x17\\nDVF [22]\\n2017\\nms-cED\\n[62], [65]\\nRGB\\nRGB\\n✓\\nℓ1, TV\\nR\\n⋆\\n✓\\n✓\\nSDC-Net [114]\\n2018\\nCNN\\n[67], [75]\\nRGB,M\\nRGB\\n✓\\nℓ1, PL\\nSR\\n⋆⋆\\n✓\\n\\x17\\nTrIVD-GAN-FP [128]\\n2020\\nDVD-GAN\\n[62], [81], [168]\\nRGB\\nRGB\\n✓\\nLhinge [56]\\nR\\n⋆⋆⋆\\n\\x17\\n\\x17\\nExplicit Motion from Content Separation\\nMCnet [132]\\n2017\\nLSTM-cED\\n[59], [60], [62], [74]\\nRGB\\nRGB\\n✓\\nℓp, GDL, AL\\nR\\n⋆⋆⋆\\n\\x17\\n✓\\nDual-GAN [56]\\n2017\\nVAE-GAN\\n[62], [65], [67], [68]\\nRGB\\nRGB\\n✓\\nℓ1, KL, AL\\nR\\n⋆⋆\\n\\x17\\n\\x17\\nDRNET [133]\\n2017\\nLSTM-ED\\n[59], [80], [169], [170]\\nRGB\\nRGB\\n✓\\nℓ2, CE, AL\\nSR\\n⋆⋆⋆⋆\\n✓\\n✓\\nDPG [134]\\n2019\\ncED\\n[67], [171], [172]\\nRGB\\nRGB\\n✓\\nℓp, TV, PL, CE\\nSR\\n⋆⋆\\n\\x17\\n\\x17\\nConditioned on Extra Variables\\nOh et al. [122]\\n2015\\nrED\\n[85]\\nRGB,A\\nRGB\\n✓\\nℓ2\\nS\\n⋆⋆⋆⋆\\n✓\\n✓\\nFinn et al. [5]\\n2016\\nst-cLSTMs\\n[5], [64]\\nRGB,A,S\\nRGB\\n✓\\nℓ2\\nR\\n⋆⋆⋆\\n\\x17\\n✓\\nIn the High-level Feature Space\\nVillegas et al. [54]\\n2017\\nLSTM-cED\\n[63], [64]\\nRGB,Po\\nRGB,Po\\n✓\\nℓ2, PL, AL [45]\\nR\\n⋆⋆⋆⋆\\n✓\\n\\x17\\nPEARL [52]\\n2017\\ncED\\n[69], [173]\\nRGB\\nSS\\n\\x17\\nℓ2, AL\\nR\\n⋆\\n✓\\n\\x17\\nS2S [57]\\n2017\\nmsCNN\\n[69], [173]\\nP\\nSS\\n✓\\nℓ1, GDL, AL\\nR\\n⋆⋆⋆\\n\\x17\\n✓\\nWalker et al. [55]\\n2017\\ncVAE\\n[62], [63]\\nRGB,Po\\nRGB\\n✓\\nℓ2, CE, KL, AL\\nR\\n⋆⋆⋆\\n✓\\n\\x17\\nJin et al. [144]\\n2017\\ncED\\n[69], [162]\\nRGB,P\\nSS,M\\n✓\\nℓ1, GDL, CE\\nR\\n⋆⋆⋆\\n✓\\n\\x17\\nEPVA [53]\\n2018\\nLSTM-ED\\n[64]\\nRGB\\nRGB\\n✓\\nℓ2, AL\\nSR\\n⋆⋆⋆⋆\\n✓\\n✓\\nNabavi et al. [145]\\n2018\\nbiLSTM-cED\\n[69]\\nP\\nSS\\n✓\\nCE\\nR\\n⋆⋆\\n\\x17\\n\\x17\\nF2F et al. [16]\\n2018\\nst-msCNN\\n[69]\\nP\\nP,SS,IS\\n✓\\nℓ2\\nR\\n⋆⋆⋆\\n✓\\n✓\\nVora et al. [150]\\n2018\\nLSTM\\n[69]\\nego-M\\nego-M\\n\\x17\\nℓ1\\nR\\n⋆\\n✓\\n\\x17\\nChiu et al. [149]\\n2019\\n3D-cED\\n[69], [71]\\nRGB\\nSS\\n\\x17\\nCE, MSE\\nR\\n⋆⋆\\n\\x17\\n\\x17\\nBayes-WD-SL [17]\\n2019\\nbayesResNet\\n[69]\\nSS,O\\nSS\\n✓\\nKL\\nSR\\n⋆⋆⋆\\n✓\\n✓\\nSun et al. [156]\\n2019\\nst-ms-cLSTM\\n[69], [86]\\nP\\nP,IS\\n✓\\nℓ2, [155]\\nR\\n⋆⋆\\n\\x17\\n\\x17\\nTerwilliger et al. [18]\\n2019\\nM-cLSTM\\n[69]\\nRGB,P\\nSS\\n✓\\nCE, ℓ1\\nR\\n⋆⋆⋆\\n\\x17\\n✓\\nStruct-VRNN [89]\\n2019\\ncVRNN\\n[64], [174]\\nRGB\\nRGB\\n✓\\nℓ2, KL\\nSR\\n⋆⋆\\n✓\\n✓\\nF2MF [148]\\n2020\\n[18]\\n[69]\\nRGB\\nRGB\\n✓\\nℓ2\\nR\\n⋆⋆\\n\\x17\\n\\x17\\nIncorporating Uncertainty\\nGoroshin et al. [175]\\n2015\\ncAE\\n[169], [176]\\nRGB\\nRGB\\n\\x17\\nℓ2, penalty\\nSR\\n⋆\\n\\x17\\n\\x17\\nFragkiadaki et al. [177]\\n2017\\nvED\\n[64], [178]\\nRGB\\nRGB\\n\\x17\\nKL, MCbest\\nR\\n⋆\\n✓\\n\\x17\\nEEN [179]\\n2017\\nvED\\n[180], [181], [182]\\nRGB\\nRGB\\n✓\\nℓ1, ℓ2\\nSR\\n⋆⋆\\n\\x17\\n✓\\nSV2P [38]\\n2018\\nCDNA\\n[5], [64], [81]\\nRGB\\nRGB\\n✓\\nℓp, KL\\nSR\\n⋆⋆⋆\\n\\x17\\n✓\\nSVG [161]\\n2018\\nLSTM-cED\\n[59], [80], [81]\\nRGB\\nRGB\\n✓\\nℓ2, KL\\nSR\\n⋆⋆⋆⋆\\n\\x17\\n✓\\nCastrejon et al. [183]\\n2019\\nvRNN\\n[69], [80], [81]\\nRGB\\nRGB\\n✓\\nKL\\nSR\\n⋆⋆⋆\\n\\x17\\n\\x17\\nHu et al. [13]\\n2020\\ncED\\n[69], [71], [184], [185]\\nRGB\\nSS,D,M\\n✓\\nCE, ℓδ, Ld, Lc, Lp\\nR\\n⋆⋆⋆\\n✓\\n\\x17\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 11}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n12\\nIncorporating latent variables into the deterministic\\nCDNA architecture for the ﬁrst time, Babaeizadeh et\\nal. proposed the Stochastic Variational Video Prediction\\n(SV2P) [38] model handling natural videos. Their time-\\ninvariant posterior distribution is approximated from the\\nentire input video sequence. Moreover, with the explicit\\nmodeling of uncertainty using latent variables, the de-\\nterministic CDNA model is outperformed. By combin-\\ning a standard deterministic architecture (LSTM-ED) with\\nstochastic latent variables, Denton et al. proposed the SVG\\nnetwork [161]. Different from SV2P, the prior is sam-\\npled from a time-varying posterior distribution, i.e. it is a\\nlearned-prior instead of ﬁxed-prior sampled from the same\\ndistribution. Most of the VAEs use a ﬁxed Gaussian as\\na prior, sampling randomly at each time step. Exploiting\\nthe temporal dependencies, a learned-prior predicts high\\nvariance in uncertain situations, and a low variance when\\na deterministic prediction sufﬁces. The SVG model is easier\\nto train and reported sharper predictions in contrast to [38].\\nBuilt upon SVG, Villegas et al. [190] implemented a baseline\\nto perform an in-depth empirical study on the importance\\nof the inductive bias, stochasticity, and model’s capacity\\nin the video prediction task. Different from previous ap-\\nproaches, Henaff et al. proposed the Error Encoding Net-\\nwork (EEN) [179] that incorporates uncertainty by feeding\\nback the residual error —the difference between the ground\\ntruth and the deterministic prediction— encoded as a low-\\ndimensional latent variable. In this way, the model implicitly\\nseparates the input video into deterministic and stochastic\\ncomponents.\\nOn the one hand, latent variable-based approaches cover\\nthe space of possible outcomes, yet predictions lack of\\nrealism. On the other hand, GANs struggle with uncertainty,\\nbut predictions are more realistic. Searching for a trade-off\\nbetween VAEs and GANs, Lee et al. [159] proposed the\\nSAVP model. It was the ﬁrst to combine latent variable\\nmodels with GANs to improve variability in video pre-\\ndictions, while maintaining realism. Under the assumption\\nthat blurry predictions of VAEs are a sign of underﬁt-\\nting, Castrejon et al. extended the VRNNs to leverage a\\nhierarchy of latent variables and better approximate data\\nlikelihood [183]. Although the backpropagation through a\\nhierarchy of conditioned latents is not straightforward, sev-\\neral techniques alleviated this issue such as, KL beta warm-\\nup, dense connectivity pattern between inputs and latents,\\nand Ladder Variational Autoencoders (LVAEs) [191]. As\\nmost of the probabilistic approaches fail in approximating\\nthe true distribution of future frames, Pottorff et al. [192]\\nreformulated the video prediction task without making any\\nassumption about the data distribution. They proposed the\\nInvertible Linear Embedding (ILE) that enables exact maxi-\\nmum likelihood learning of video sequences, by combining\\nan invertible neural network [193], also known as reversible\\nﬂows, and a linear time-invariant dynamic system. The ILE\\nhandles nonlinear motion in the pixel space and scales\\nbetter to longer-term predictions compared to adversarial\\nmodels [44]. Also based on Glow model [193] and shar-\\ning goals with [192], VideoFlow [194] approaches exact\\nlikelihood maximization using normalized ﬂows through\\ninvertible transformations. These ﬂow-based architectures\\npresent several advantages such as, exact log-likelihood\\nevaluation and faster sampling than autoregressive models,\\nwhile still producing high-quality long-term and stochastic\\npredictions.\\nWhile previous variational approaches [159], [161] fo-\\ncused on predicting a single frame of low resolution in\\nrestricted, predictable or simulated datasets, Hu et al. [13]\\njointly predict full-frame ego-motion, static scene, and object\\ndynamics on complex real-world urban driving. Featuring\\na novel spatio-temporal module, their ﬁve-component ar-\\nchitecture learns rich representations that incorporate both\\nlocal and global spatio-temporal context. The model outper-\\nformed existing spatio-temporal architectures, by predicting\\nsemantic segmentation, depth and optical ﬂow. However, no\\nperformance comparison with [159], [161] has been carried\\nout.\\n5\\nPERFORMANCE EVALUATION\\nThis section presents the results of the previously analyzed\\nvideo prediction models on the most popular datasets on\\nthe basis of the metrics described below.\\n5.1\\nMetrics and Evaluation Protocols\\nFor a fair evaluation of video prediction systems, multiple\\naspects of prediction need to be addressed such as whether\\nthe predicted sequences look realistic, are plausible and\\ncover all possible outcomes. To the best of our knowledge,\\nthere are no evaluation protocols and metrics that evaluate\\npredictions by fulﬁlling all these aspects simultaneously.\\nThe most widely used evaluation protocols for video\\nprediction rely on image similarity-based metrics such\\nas, Mean-Squared Error (MSE), Structural Similarity Index\\nMeasure (SSIM) [195], and Peak Signal to Noise Ratio\\n(PSNR). However, evaluating a prediction according to the\\nmismatch between its visual appearance and the ground\\ntruth is not always reliable. In practice, these metrics pe-\\nnalize all predictions that deviate from the ground truth.\\nIn other words, they prefer blurry predictions nearly ac-\\ncommodating the exact ground truth than sharper and\\nplausible but imperfect generations [159], [183], [196]. Pixel-\\nwise metrics do not always reﬂect how accurately a model\\nhas captured the dynamic features and their temporal vari-\\nability in a video. In addition, the precision of a metric is\\ninﬂuenced by the loss function used to train the model. For\\ninstance, models minimizing the MSE loss function would\\nblindly perform well on the PSNR metric as it is based on\\nMSE. Suffering from similar problems, SSIM measures the\\nsimilarity between two images, from −1 (very dissimilar)\\nto +1 (the same image). As a difference, it measures simi-\\nlarities on image patches instead of performing pixel-wise\\ncomparison. These metrics are easily fooled by learning to\\nmatch the background in predictions. To address this issue,\\nsome methods [18], [44], [57], [148] also evaluated predic-\\ntions only on the dynamic parts of the sequence avoiding\\nthe background inﬂuence.\\nAs the pixel space is multimodal and high-dimensional,\\nit is challenging to evaluate how accurately a predicted\\nsequence covers the full distribution of possible outcomes.\\nAddressing this issue, some probabilistic approaches [159],\\n[161], [183] assessed prediction coverage by sampling mul-\\ntiple random predictions; to then search for the best match\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 12}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n13\\nwith the ground truth sequence using common metrics. This\\nis the most widely used evaluation protocol in probabilistic\\nmodels. Other methods [106], [107], [183] also reported their\\nresults using perceptual metrics such as: Learned Percep-\\ntual Image Patch Similarity (LPIPS) [196] which is a linear\\nweighted ℓ2 distance between deep features of images,\\nFr´echet Video Distance (FVD) [197] measuring prediction\\nrealism at a distribution level using a 3D CNN to capture\\nthe temporal coherence across a video sequence, and DeeP-\\nSiM [45]. Moreover, Lee et al. [159] used the VGG Cosine\\nSimilarity metric that performs cosine similarity to the fea-\\ntures extracted by VGG network [99] from the predictions.\\nAmong other metrics we have the Inception Score\\n(IS) [198] introduced to deal with GANs mode collapse\\nproblem by measuring the diversity of generated samples;\\nmeasuring sharpness based on difference of gradients [44];\\nParzen window [199], yet deﬁcient for high-dimensional im-\\nages; and the Laplacian of Gaussians (LoG) [200], [201] used\\nin [108]. In the semantic segmentation space, authors used\\nthe popular Intersection over Union (IoU) metric. IS was\\nalso widely used to report results on different methods [55],\\n[126], [132], [133]. Differently, on the basis of the EPVA\\nmodel [53] a quantitative evaluation was performed, based\\non the conﬁdence of an external method trained to identify\\nwhether the generated video contains a recognizable person.\\nTo support quantitative evaluation, a qualitative assessment\\nbased on a visual inspection could be carried out via Ama-\\nzon Mechanical Turk (AMT) workers.\\n5.2\\nResults\\nIn this section we report the quantitative results of the\\nmost relevant methods reviewed in the previous sections.\\nTo achieve a wide comparison, we limited the quantitative\\nresults to the most common metrics and datasets. We have\\ndistributed the results in different tables, given the large\\nvariation in the evaluation protocols of the video prediction\\nmodels.\\nMany authors evaluated their methods on the Moving\\nMNIST synthetic environment. Although it represents a\\nrestricted and quasi-deterministic scenario, long-term pre-\\ndictions are still challenging. The black and homogeneous\\nbackground induce methods to accurately extrapolate black\\nframes and vanish the predicted digits in the long-term hori-\\nzon. Under this conﬁguration, the CrevNet model demon-\\nstrated a leap over the previous state of the art. As the\\nsecond best, the E3d-LSTM network reported stable errors\\nin both short-term and longer-term predictions showing\\nthe advantages of their memory attention mechanism. It\\nalso reported the second best results on the KTH dataset,\\nafter [106] which achieved the best overall performance and\\ndemonstrated quality predictions on natural videos.\\nPerforming short-term predictions on the KTH dataset,\\nthe Recurrent Ladder Network (RLN) outperformed MCnet\\nand fRNN by a slight margin. The RLN architecture draws\\nsimilarities with fRNN, except that while the former uses\\nbridge connections, the latter relies on state sharing that im-\\nproves memory consumption. On the Moving MNIST and\\nUCF-101 datasets, fRNN outperformed RLN. Other inter-\\nesting methods to highlight are PredRNN and PredRNN++,\\nboth providing close results to E3d-LSTM. State-of-the-art\\nTABLE 3\\nResults on M-MNIST (Moving MNIST). Predicting the next y frames\\nfrom x context frames (x →y). † results reported by Oliu et al. [110], ‡\\nresults reported by Wang et al. [111], ∗results reported by Wang et al.\\n[202], ◁results reported by Wang et al. [203]. MSE represents per-pixel\\naverage MSE (10−3). MSE⋄represents per-frame error.\\nM-MNIST\\nM-MNIST\\n(10 →10)\\n(10 →30)\\nmethod\\nMSE↓\\nMSE⋄↓\\nSSIM↑\\nPSNR↑\\nCE↓\\nMSE⋄↓\\nSSIM↑\\nBeyondMSE [44]\\n27.48†\\n122.6∗\\n0.713∗\\n15.969†\\n-\\n-\\n-\\nSrivastava et al. [80]\\n17.37†\\n118.3∗\\n0.690∗\\n18.183†\\n341.2\\n180.1◁\\n0.583◁\\nShi et al. [20]\\n-\\n96.5‡\\n0.713‡\\n-\\n367.2∗\\n156.2◁\\n0.597◁\\nDFN [124]\\n-\\n89.0‡\\n0.726‡\\n-\\n285.2\\n149.5◁\\n0.601◁\\nCDNA [5]\\n-\\n84.2‡\\n0.728‡\\n-\\n346.6∗\\n142.3◁\\n0.609◁\\nVLN [204]\\n-\\n-\\n-\\n-\\n187.7\\nPatraucean et al. [66]\\n43.9\\n-\\n-\\n-\\n179.8\\n-\\n-\\nMCnet [132]†\\n42.54\\n-\\n-\\n13.857\\n-\\n-\\n-\\nRLN [205]†\\n42.54\\n-\\n-\\n13.857\\n-\\n-\\n-\\nPredNet [70]†\\n41.61\\n-\\n-\\n13.968\\n-\\n-\\n-\\nfRNN [110]\\n9.47\\n68.4‡\\n0.819‡\\n21.386\\n-\\n-\\n-\\nPredRNN [202]\\n-\\n56.8\\n0.867\\n-\\n97.0\\n-\\n-\\nVPN [206]\\n-\\n64.1‡\\n0.870‡\\n-\\n87.6\\n129.6◁\\n0.620◁\\nZnet [105]\\n-\\n50.5\\n0.877\\n-\\n-\\n-\\n-\\nPredRNN++ [203]\\n-\\n46.5\\n0.898\\n-\\n-\\n91.1\\n0.733\\nE3d-LSTM [111]\\n-\\n41.3\\n0.910\\n-\\n-\\n-\\n-\\nCrevNet [112]\\n-\\n22.3\\n0.949\\n-\\n-\\n-\\n-\\nTABLE 4\\nResults on KTH dataset. Predicting the next y frames from x context\\nframes (x →y). † results reported by Oliu et al. [110], ‡ results\\nreported by Wang et al. [111], ∗results reported by Zhang et al. [105],\\n◁results reported by Jin et al. [106]. Per-pixel average MSE (10−3).\\nBest results are represented in bold.\\nKTH\\nKTH\\nKTH\\n(10 →10)\\n(10 →20)\\n(10 →40)\\nmethod\\nMSE↓\\nPSNR↑\\nSSIM↑\\nPSNR↑\\nSSIM↑\\nPSNR↑\\nSrivastava et al. [80]†\\n9.95\\n21.22\\n-\\n-\\n-\\n-\\nPredNet [70]†\\n3.09\\n28.42\\n-\\n-\\n-\\n-\\nBeyondMSE [44]†\\n1.80\\n29.34\\n-\\n-\\n-\\n-\\nfRNN [110]\\n1.75\\n29.299\\n0.771◁\\n26.12◁\\n0.678◁\\n23.77◁\\nMCnet [132]\\n1.65†\\n30.95†\\n0.804‡\\n25.95‡\\n0.73◁\\n23.89◁\\nRLN [205]†\\n1.39\\n31.27\\n-\\n-\\n-\\n-\\nShi et al. [20]‡\\n-\\n-\\n0.712\\n23.58\\n0.639\\n22.85\\nSAVP [159]◁\\n-\\n-\\n0.746\\n25.38\\n0.701\\n23.97\\nVPN [206]∗\\n-\\n-\\n0.746\\n23.76\\n-\\n-\\nDFN [124]‡\\n-\\n-\\n0.794\\n27.26\\n0.652\\n23.01\\nfRNN [110]‡\\n-\\n-\\n0.771\\n26.12\\n0.678\\n23.77\\nZnet [105]\\n-\\n-\\n0.817\\n27.58\\n-\\n-\\nSV2P invariant [38]◁\\n-\\n-\\n0.826\\n27.56\\n0.778\\n25.92\\nSV2P variant [38]◁\\n-\\n-\\n0.838\\n27.79\\n0.789\\n26.12\\nPredRNN [202]\\n-\\n-\\n0.839\\n27.55\\n0.703‡\\n24.16‡\\nVarNet [207]◁\\n-\\n-\\n0.843\\n28.48\\n0.739\\n25.37\\nSAVP-VAE [159]◁\\n-\\n-\\n0.852\\n27.77\\n0.811\\n26.18\\nPredRNN++ [203]\\n-\\n-\\n0.865\\n28.47\\n0.741‡\\n25.21‡\\nMSNET [208]\\n-\\n-\\n0.876\\n27.08\\n-\\n-\\nE3d-LSTM [111]\\n-\\n-\\n0.879\\n29.31\\n0.810\\n27.24\\nJin et al. [106]\\n-\\n-\\n0.893\\n29.85\\n0.851\\n27.56\\nresults using different metrics were reported on Caltech\\nPedestrian by Kwon et al. [108], CrevNet [112], and Jin et\\nal. [106]. The former, by taking advantage of its retrospective\\nprediction scheme, was also the overall winner on the UCF-\\n101 dataset. The latter outperformed state of the art on\\nall metrics except on LPIPS, as predictions of probabilistic\\napproaches are clearer and realist but less consistent with\\nthe ground truth. However [106] is absolute the winner on\\nthe BAIR Push dataset.\\nOn the one hand, some approaches have been evalu-\\nated on other datasets: SDC-Net [114] outperformed [44],\\n[132] on YouTube8M, TrIVD-GAN-FP outperformed [127],\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 13}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n14\\nTABLE 5\\nResults on Caltech Pedestrian. Predicting the next y frames from x\\ncontext frames (x →y). † reported by Kwon et al. [108], ‡ reported by\\nReda et al. [114], ∗reported by Gao et al. [134], ◁reported by Jin et al.\\n[106]. Per-pixel average MSE (10−3). Best results in bold.\\nCaltech Pedestrian\\n(10 →1)\\nmethod\\nMSE↓\\nSSIM↑\\nPSNR↑\\nLPIPS↓\\nBeyondMSE [44]‡\\n3.42\\n0.847\\n-\\n-\\nMCnet [132]‡\\n2.50\\n0.879\\n-\\n-\\nDVF [22]∗\\n-\\n0.897\\n26.2\\n5.57◁\\nDual-GAN [56]\\n2.41\\n0.899\\n-\\n-\\nCtrlGen [94]∗\\n-\\n0.900\\n26.5\\n6.38◁\\nPredNet [70]†\\n2.42\\n0.905\\n27.6\\n7.47◁\\nContextVP [113]\\n1.94\\n0.921\\n28.7\\n6.03◁\\nGAN-VGG [107]\\n-\\n0.916\\n-\\n3.61\\nG-VGG [107]\\n-\\n0.917\\n-\\n3.52\\nSDC-Net [114]\\n1.62\\n0.918\\n-\\n-\\nKwon et al. [108]\\n1.61\\n0.919\\n29.2\\n-\\nDPG [134]\\n−\\n0.923\\n28.2\\n5.04◁\\nG-MAE [107]\\n-\\n0.923\\n-\\n4.30\\nGAN-MAE [107]\\n-\\n0.923\\n-\\n4.09\\nCrevNet [112]\\n-\\n0.925\\n29.3\\n-\\nJin et al. [106]\\n-\\n0.927\\n29.1\\n5.89\\nTABLE 6\\nResults on UCF-101 dataset. Predicting the next x frames from y\\ncontext frames (x →y). † results reported by Oliu et al. [110]. Per-pixel\\naverage MSE (10−3). Best results are represented in bold.\\nUCF-101\\nUCF-101\\n(10 →10)\\n(4 →1)\\nmethod\\nMSE↓\\nPSNR↑\\nMSE↓\\nSSIM↑\\nPSNR↑\\nSrivastava et al. [80]†\\n148.66\\n10.02\\n-\\n-\\n-\\nPredNet [70]†\\n15.50\\n19.87\\n-\\n-\\n-\\nBeyondMSE [44]†\\n9.26\\n22.78\\n-\\n-\\n-\\nMCnet [132]\\n9.40†\\n23.46†\\n-\\n0.91\\n31.0\\nRLN [205]†\\n9.18\\n23.56\\n-\\n-\\n-\\nfRNN [110]\\n9.08\\n23.87\\n-\\n-\\n-\\nBeyondMSE [44]\\n-\\n-\\n-\\n0.92\\n32\\nDual-GAN [56]\\n-\\n-\\n-\\n0.94\\n30.5\\nDVF [22]\\n-\\n-\\n-\\n0.94\\n33.4\\nContextVP [113]\\n-\\n-\\n-\\n0.92\\n34.9\\nKwon et al. [108]\\n-\\n-\\n1.37\\n0.94\\n35.0\\n[209] on Kinetics-600 test set [168], E3d-LSTM compared\\ntheir method with [110], [202], [203], [206] on the TaxiBJ\\ndataset [163], and CrevNet [112] on Trafﬁc4cast [167]. On the\\nother hand, some explored out-of-domain tasks [20], [111],\\n[112], [124], [125] (see ood column in Table 2).\\n5.2.1\\nResults on Probabilistic Approaches\\nProbabilistic video prediction methods have been mainly\\nevaluated on the Stochastic Moving MNIST, Bair Push and\\nCityscapes datasets. Different from the original Moving\\nMNIST dataset, the stochastic version includes uncertain\\ndigit trajectories, i.e. the digits bounce off the border with\\na random new direction. On this dataset, both versions of\\nCastrejon et al. models (1L, without a hierarchy of latents,\\nand 3L with a 3-level hierarchy of latents) outperform SVG\\nby a large margin. On the Bair Push dataset, SAVP reported\\nsharper and more realistic-looking predictions than SVG\\nwhich suffer of blurriness. However, both models were\\noutperformed by [183] as well on the Cityscapes dataset.\\nThe model based on a 3-level hierarchy of latents [183]\\nTABLE 7\\nResults on SM-MNIST (Stochastic Moving MNIST), BAIR Push and\\nCityscapes datasets. † results reported by Castrejon et al. [183]. ‡\\nresults reported by Jin et al. [106].\\nSM-MNIST\\nBAIR Push\\nCityscapes\\n(5 →10)\\n(2 →28)\\n(2 →28)\\nmethod\\nFVD↓\\nSSIM↑\\nFVD↓\\nSSIM↑\\nPSNR↑\\nFVD↓\\nSSIM↑\\nSVG [161]\\n90.81†\\n0.688†\\n256.62†\\n0.816†\\n17.72‡\\n1300.26†\\n0.574†\\nSAVP [159]\\n-\\n-\\n143.43†\\n0.795†\\n18.42‡\\n-\\n-\\nSAVP-VAE [159]\\n-\\n-\\n-\\n0.815‡\\n19.09‡\\n-\\n-\\nSV2P inv. [38]‡\\n-\\n-\\n-\\n0.817\\n20.36\\n-\\n-\\nvRNN 1L [183]\\n63.81\\n0.763\\n149.22\\n0.829\\n-\\n682.08\\n0.609\\nvRNN 3L [183]\\n57.17\\n0.760\\n143.40\\n0.822\\n-\\n567.51\\n0.628\\nJin et al. [106]\\n-\\n-\\n-\\n0.844\\n21.02\\n-\\n-\\nTABLE 8\\nResults on Cityscapes dataset. Predicting the next y semantic\\nsegmented frames from 4 context frames (4 →y). ‡ IoU results on\\neight moving objects classes. † results reported by Chiu et al. [149]\\nCityscapes\\n(4 →1)\\n(4 →3)\\n(4 →9)\\n(4 →10)\\nmethod\\nIoU↑\\nIoU↑\\nIoU↑\\nIoU↑\\nS2S [57]‡\\n-\\n55.3\\n40.8\\n-\\nS2S-maskRCNN [16]‡\\n-\\n55.4\\n42.4\\n-\\nS2S [57]\\n62.6†\\n59.4\\n47.8\\n-\\nNabavi et al. [145]\\n71.37\\n60.06\\n-\\n-\\nF2F [16]\\n-\\n61.2\\n41.2\\n-\\nVora et al. [150]\\n-\\n61.47\\n45.4\\n-\\nS2S-Res101-FCN [144]\\n-\\n62.6\\n-\\n50.8\\nTerwilliger et al. [18]‡\\n-\\n65.1\\n46.3\\n-\\nChiu et al. [149]\\n72.43\\n65.53\\n50.52\\nJin et al. [144]\\n-\\n66.1\\n-\\n53.9\\nTerwilliger et al. [18]\\n73.2\\n67.1\\n51.5\\n52.5\\nBayes-WD-SL [17]\\n75.3\\n66.7\\n52.5\\n-\\nF2MF [148]‡\\n−\\n67.7\\n54.6\\n−\\nF2MF [148]\\n−\\n69.6\\n57.9\\n−\\noutperform previous works on all three datasets, showing\\nthe advantages of the extra expressiveness of this model.\\n5.2.2\\nResults on the High-level Prediction Space\\nMost of the methods have chosen the semantic segmentation\\nspace to make predictions. Although they relied on differ-\\nent datasets for training, performance results were mostly\\nreported on the Cityscapes dataset using the IoU metric.\\nAuthors explored short-term (next-frame prediction), mid-\\nterm (+3 time steps in the future) and long-term (up to +10\\ntime step in the future) predictions. On the semantic seg-\\nmentation prediction space, Bayes-WD-SL [17], F2MF [148],\\nand Jin et al. [52] reported the best results. Among these\\nmethods, it is noteworthy that Bayes-WD-SL was the only\\none to explore prediction diversity on the basis of a Bayesian\\nformulation.\\nIn the instance segmentation space, the F2F pioneering\\nmethod [16] was outperformed by Sun et al. [156] on short\\nand mid-term predictions using the AP50 and AP evalua-\\ntion metrics. On the other hand, in the keypoint coordinate\\nspace, the seminal model of Minderer et al. [89] qualitatively\\noutperformed SVG [161], SAVP [159] and EPVA [53], yet\\npixel-wise metrics reported similar results. In the human\\npose space, and by regressing future frames from human\\npose predictions, Tang et al. [160] outperformed SAVP [159],\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 14}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n15\\nMCnet [132] and [54] on the basis of the PSNR and SSIM\\nmetrics on the Penn Action and J-HMDB [210] datasets.\\n6\\nDISCUSSION\\nThe video prediction literature ranges from a direct syn-\\nthesis of future pixel intensities, to complex probabilistic\\nmodels addressing prediction uncertainty. The range be-\\ntween these approaches consists of methods that try to\\nfactorize or narrow the prediction space. Simplifying the\\nprediction task has been a natural evolution of video predic-\\ntion models, inﬂuenced by several open research challenges\\ndiscussed below. Due to the curse of dimensionality and\\nthe inherent pixel variability, developing a robust prediction\\nbased on raw pixel intensities is overly-complicated. This\\noften leads to the regression-to-the-mean problem, visually\\nrepresented as blurriness. Making parametric models larger\\nwould improve the quality of predictions, yet this is cur-\\nrently incompatible with high-resolution predictions due\\nto memory constraints. Transformation-based approaches\\npropagate pixels from previous frames based on estimated\\nﬂow maps. In this case, prediction quality is directly in-\\nﬂuenced by the accuracy of the estimated ﬂow. Similarly,\\nthe prediction in a high-level space is mostly conditioned\\nby the quality of some extra supervisory signals such as\\nsemantic maps and human poses, to name a few. Erroneous\\nsupervision signals would harm prediction quality.\\nAnalyzing the impact of the inductive bias on the per-\\nformance of a video prediction model, Villegas et al. [190]\\ndemonstrated the maximization of the SVG model [161]\\nperformance with minimal inductive bias (e.g. segmentation\\nor instance maps, optical ﬂow, adversarial losses, etc.) by in-\\ncreasing progressively the scale of computation. A common\\nassumption when addressing the prediction task in a high-\\nlevel feature space, is the direct improvement of long-term\\npredictions as a result of simplifying the prediction space.\\nEven if the complexity of the prediction space is reduced,\\nit is still multimodal when dealing with natural videos.\\nFor instance, when it comes to long-term predictions in the\\nsemantic segmentation space, most of the models reported\\npredictions only up to ten time steps into the future. This\\ndirectly suggests that the choice of the prediction space is\\nstill an unsolved problem. Finding a trade-off between the\\ncomplexity of the prediction space and the output quality is\\nchallenging. An overly-simpliﬁed representation could limit\\nthe prediction on complex data such as natural videos. Al-\\nthough abstract predictions sufﬁce for many of the decision-\\nmaking systems based on visual reasoning, prediction in\\npixel space is still being addressed.\\nFrom the analysis performed in this review and in line\\nwith the conclusions extracted from [190] we state that:\\n(1) including recurrent connections and stochasticity in a\\nvideo prediction model generally lead to improved perfor-\\nmance; (2) increasing model capacity while maintaining a\\nlow inductive bias also improves prediction performance;\\n(3) multi-step predictions conditioned by previously gen-\\nerated outputs are prone to accumulate errors, diverging\\nfrom the ground truth when addressing long-term hori-\\nzons; (4) methods predicted further in the future without\\nrelying on high-level feature spaces; (5) combining pixel-\\nwise losses with adversarial training somewhat mitigates\\nthe regression-to-the-mean issue.\\n6.1\\nResearch Challenges\\nDespite the wealth of currently existing video prediction\\napproaches and the signiﬁcant progress made in this ﬁeld,\\nthere is still room to improve state-of-the-art algorithms. To\\nfoster progress, open research challenges must be clearly\\nidentiﬁed and disentangled. So far in this review, we have\\nalready discussed about: (1) the importance of spatio-\\ntemporal correlations as a self-supervisory signal for pre-\\ndictive models; (2) how to deal with future uncertainty and\\nmodel the underlying multimodal distributions of natural\\nvideos; (3) the over-complicated task of learning meaningful\\nrepresentations and deal with the curse of dimensionality;\\n(4) pixel-wise loss functions and blurry results when dealing\\nwith equally probable outcomes, i.e. probabilistic environ-\\nments. These issues deﬁne the open research challenges in\\nvideo prediction.\\nCurrently existing methods are limited to short-term\\nhorizons. While frames in the immediate future are ex-\\ntrapolated with high accuracy, in the long term horizon\\nthe prediction problem becomes multimodal by nature.\\nInitial solutions consisted on conditioning the prediction on\\npreviously predicted frames. However, these autoregressive\\nmodels tend to accumulate prediction errors that progres-\\nsively diverge the generated prediction from the expected\\noutcome. On the other hand, due to memory issues, there\\nis a lack of resolution in predictions. Authors tried to\\naddress this issue by composing the full-resolution image\\nfrom small predicted patches. However, as the results are\\nnot convincing because of the annoying tilling effect, most\\nof the available models are still limited to low-resolution\\npredictions. In addition to the lack of resolution and long-\\nterm predictions, models are still prone to the regress-to-the-\\nmean problem that consists on averaging the output frame\\nto accommodate multiple equally probable outcomes. This\\nis directly related to the pixel-wise loss functions, that focus\\nthe learning process on the visual appearance. The choice\\nof the loss function is an open research problem with a\\ndirect inﬂuence on the prediction quality. Finally, the lack\\nof reliable and fair evaluation models makes the qualitative\\nevaluation of video prediction challenging and represents\\nanother potential open problem.\\n6.2\\nFuture Directions\\nBased on the in-depth analysis conducted in this review, we\\npresent some future promising research directions.\\nConsider alternative loss functions: Pixel-wise loss func-\\ntions are widely used in the video prediction task, causing\\nblurry predictions when dealing with uncontrolled environ-\\nments or long-term horizon. In this regard, great efforts have\\nbeen made in the literature to identify adequate loss func-\\ntions for the prediction task. However, despite the existing\\nwide spectrum of loss functions, most models still blindly\\nrely on deterministic loss functions.\\nAlternatives to RNNs: Currently, RNNs are still widely\\nused in this ﬁeld to model temporal dependencies, and\\nachieved state-of-the-art results on different benchmarks\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 15}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n16\\n[110], [111], [202], [203]. Nevertheless, some methods also\\nrelied on 3D convolutions to further enhance video predic-\\ntion [111], [149] representing a promising avenue.\\nUse synthetically generated videos: Simplifying the pre-\\ndiction is a current trend in the video prediction litera-\\nture. A vast amount of video prediction models explored\\nhigher-level features spaces to reformulate the prediction\\ntask into a more tractable problem. However, this mostly\\nconditions the prediction to the accuracy of an external\\nsource of supervision such as optical ﬂow, human pose, pre-\\nactivations (percepts) extracted from supervised networks,\\nand more. This issue could be alleviated by taking advan-\\ntage of existing fully-annotated and photorealistic synthetic\\ndatasets or by using data generation tools. Video prediction\\nin photorealistic synthetic scenarios has not been explored\\nin the literature.\\nEvaluation metrics: Since the most widely used evaluation\\nprotocols for video prediction rely on image similarity-\\nbased metrics, the need for fairer evaluation metrics is\\nimminent. A fair metric should not penalize predictions that\\ndeviate from the ground truth at the pixel level, if their\\ncontent represents a plausible future prediction in a higher\\nlevel, i.e., the dynamics of the scene correspond to the reality\\nof the labels. In this regard, some methods evaluate the\\nsimilarity between distributions or at a higher-level. How-\\never, there is still room for improvement in the evaluation\\nprotocols for video prediction and generation [211].\\n7\\nCONCLUSION\\nIn this review, after reformulating the predictive learning\\nparadigm in the context of video prediction, we have closely\\nreviewed the fundamentals on which it is based: exploiting\\nthe time dimension of videos, dealing with stochasticity, and\\nthe importance of the loss functions in the learning pro-\\ncess. Moreover, an analysis of the backbone deep learning-\\nbased architectures for this task was performed in order to\\nprovide the reader the necessary background knowledge.\\nThe core of this study encompasses the analysis and clas-\\nsiﬁcation of more than 50 methods and the datasets they\\nhave used. Methods were analyzed from three perspectives:\\nmethod description, contribution over the previous works\\nand performance results. They have also been classiﬁed\\naccording to a proposed taxonomy based on their main\\ncontribution. In addition, we have presented a comparative\\nsummary of the datasets and methods in tabular form so\\nas the reader, at a glance, could identify low-level details.\\nIn the end, we have discussed the performance results on\\nthe most popular datasets and metrics to ﬁnally provide\\nuseful insight in shape of future research directions and\\nopen problems. In conclusion, video prediction is a promis-\\ning avenue for the self-supervised learning of rich spatio-\\ntemporal correlations, providing prediction capabilities to\\nexisting intelligent decision-making systems. While great\\nstrides have been made, there is still room for improvement\\nin video prediction using deep learning techniques.\\nACKNOWLEDGMENTS\\nThis work has been funded by the Spanish Government\\nPID2019-104818RB-I00 grant for the MoDeaAS project, sup-\\nported with Feder funds. This work has also been sup-\\nported by two Spanish national grants for PhD studies,\\nFPU17/00166, and ACIF/2018/197 respectively. We also\\nacknowledge Zuria Bauer and Victor Villena-Martinez for\\ntheir valuable discussion and support on this work.\\nREFERENCES\\n[1]\\nY. Kong and Y. Fu, “Human action recognition and prediction: A\\nsurvey,” arXiv:1806.11230, 2018.\\n[2]\\nA. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez,\\nP. Martinez-Gonzalez, and J. Garcia-Rodriguez, “A survey on\\ndeep learning techniques for image and video semantic segmen-\\ntation,” Applied Soft Computing, vol. 70, Sep. 2018.\\n[3]\\nV. Villena-Martinez, S. Oprea, M. Saval-Calvo, J. A. L´opez,\\nA. F. Guill´o, and R. B. Fisher, “When deep learning meets data\\nalignment: A review on deep registration networks (DRNs),”\\narXiv:2003.03167, 2020.\\n[4]\\nY. LeCun, Y. Bengio, and G. E. Hinton, “Deep learning,” Nature,\\nvol. 521, no. 7553, 2015.\\n[5]\\nC. Finn, I. J. Goodfellow, and S. Levine, “Unsupervised Learning\\nfor Physical Interaction through Video Prediction,” in NeurIPS,\\n2016.\\n[6]\\nF. Ebert, C. Finn, S. Dasari, A. Xie, A. X. Lee, and S. Levine,\\n“Visual foresight: Model-based deep reinforcement learning for\\nvision-based robotic control,” arxiv:1812.00568, 2018.\\n[7]\\nH. S. Koppula and A. Saxena, “Anticipating human activities\\nusing object affordances for reactive robotic response,” TPAMI,\\nvol. 38, no. 1, pp. 14–29, 2016.\\n[8]\\nA. Xie, F. Ebert, S. Levine, and C. Finn, “Improvisation through\\nphysical understanding: Using novel objects as tools with visual\\nforesight,” in Robotics: Science and Systems, 2019.\\n[9]\\nD. Hafner, T. P. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee,\\nand J. Davidson, “Learning latent dynamics for planning from\\npixels,” in ICML, ser. Proceedings of Machine Learning Research,\\nvol. 97, 2019, pp. 2555–2565.\\n[10]\\nL. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell,\\nK. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine,\\nA. Mohiuddin, R. Sepassi, G. Tucker, and H. Michalewski,\\n“Model based reinforcement learning for atari,” in ICLR, 2020.\\n[11]\\nA. Bhattacharyya, M. Fritz, and B. Schiele, “Long-Term On-Board\\nPrediction of People in Trafﬁc Scenes Under Uncertainty,” in\\nCVPR, 2018.\\n[12]\\nC. Choi, “Shared cross-modal trajectory prediction for au-\\ntonomous driving,” arxiv:2004.00202, 2020.\\n[13]\\nA. Hu, F. Cotter, N. Mohan, C. Gurau, and A. Kendall, “Prob-\\nabilistic future prediction for video scene understanding,” in\\nECCV, 2020, pp. 767–785.\\n[14]\\nH. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Predict-\\ning the future: A jointly learnt model for action anticipation,” in\\nICCV, 2019, pp. 5561–5570.\\n[15]\\nC. R. Opazo, B. Fernando, and H. Li, “Action anticipation by\\npredicting future dynamic images,” in ECCV Workshops (3), 2018,\\npp. 89–105.\\n[16]\\nP. Luc, C. Couprie, Y. LeCun, and J. Verbeek, “Predicting Future\\nInstance Segmentation by Forecasting Convolutional Features,”\\nin ECCV, 2018, pp. 593–608.\\n[17]\\nA. Bhattacharyya, M. Fritz, and B. Schiele, “Bayesian prediction\\nof future street scenes using synthetic likelihoods,” in ICLR, 2019.\\n[18]\\nA. Terwilliger, G. Brazil, and X. Liu, “Recurrent ﬂow-guided\\nsemantic forecasting,” in WACV, 2019.\\n[19]\\nW. Liu, W. Luo, D. Lian, and S. Gao, “Future frame prediction for\\nanomaly detection - A new baseline,” in CVPR, 2018.\\n[20]\\nX. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong, and W. Woo,\\n“Convolutional LSTM network: A machine learning approach for\\nprecipitation nowcasting,” in NeurIPS, 2015.\\n[21]\\nX. Shi, Z. Gao, L. Lausen, H. Wang, D.-Y. Yeung, W.-k. Wong,\\nand W.-c. WOO, “Deep learning for precipitation nowcasting: A\\nbenchmark and a new model,” in NeurIPS, 2017.\\n[22]\\nZ. Liu, R. A. Yeh, X. Tang, Y. Liu, and A. Agarwala, “Video frame\\nsynthesis using deep voxel ﬂow,” in ICCV, 2017.\\n[23]\\nW. R. Softky, “Unsupervised pixel-prediction,” in NeurIPS, 1995.\\n[24]\\nR. P. N. Rao and D. H. Ballard, “Predictive coding in the vi-\\nsual cortex: a functional interpretation of some extra-classical\\nreceptive-ﬁeld effects,” Nature Neuroscience, vol. 2, no. 1, 1999.\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 16}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n17\\n[25]\\nG. Deco and B. Sch¨urmann, “Predictive coding in the visual\\ncortex by a recurrent network with gabor receptive ﬁelds,” Neural\\nProcessing Letters, vol. 14, no. 2, 2001.\\n[26]\\nA. Hollingworth, “Constructing visual representations of natural\\nscenes: the roles of short- and long-term visual memory.” Journal\\nof experimental psychology. Human perception and performance, vol.\\n30 3, 2004.\\n[27]\\nA. Cleeremans and J. L. McClelland, “Learning the structure of\\nevent sequences.” Journal of Experimental Psychology: General, vol.\\n120, no. 3, 1991.\\n[28]\\nA. Cleeremans and J. Elman, Mechanisms of implicit learning:\\nConnectionist models of sequence processing.\\nMIT press, 1993.\\n[29]\\nR. Baker, M. Dexter, T. E. Hardwicke, A. Goldstone, and\\nZ. Kourtzi, “Learning to predict: Exposure to temporal sequences\\nfacilitates prediction of future events,” Vision Research, vol. 99,\\n2014.\\n[30]\\nH. E. M. den Ouden, P. Kok, and F. P. de Lange, “How prediction\\nerrors shape perception, attention, and motivation,” in Front.\\nPsychology, 2012.\\n[31]\\nY. Bengio, A. C. Courville, and P. Vincent, “Representation learn-\\ning: A review and new perspectives,” TPAMI, vol. 35, no. 8, 2013.\\n[32]\\nX. Wang and A. Gupta, “Unsupervised Learning of Visual Rep-\\nresentations Using Videos,” in ICCV, 2015.\\n[33]\\nP. Agrawal, J. Carreira, and J. Malik, “Learning to see by mov-\\ning,” in ICCV, 2015.\\n[34]\\nD.-A.\\nHuang,\\nV.\\nRamanathan,\\nD.\\nMahajan,\\nL.\\nTorresani,\\nM. Paluri, L. Fei-Fei, and J. Carlos Niebles, “What makes a video\\na video: Analyzing temporal information in video understanding\\nmodels and datasets,” in CVPR, June 2018.\\n[35]\\nL. C. Pickup, Z. Pan, D. Wei, Y. Shih, C. Zhang, A. Zisserman,\\nB. Sch¨olkopf, and W. T. Freeman, “Seeing the arrow of time,” in\\nCVPR, 2014.\\n[36]\\nI. Misra, C. L. Zitnick, and M. Hebert, “Shufﬂe and learn: Unsu-\\npervised learning using temporal order veriﬁcation,” in ECCV,\\n2016.\\n[37]\\nD. Wei, J. J. Lim, A. Zisserman, and W. T. Freeman, “Learning\\nand using the arrow of time,” in CVPR, 2018.\\n[38]\\nM. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and\\nS. Levine, “Stochastic variational video prediction,” in ICLR,\\n2018.\\n[39]\\nS. Aigner and M. K¨orner, “The importance of loss functions for\\nincreasing the generalization abilities of a deep learning-based\\nnext frame prediction model for trafﬁc scenes,” Machine Learning\\nand Knowledge Extraction, vol. 2, no. 2, pp. 78–98, 2020.\\n[40]\\nH. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for\\nimage restoration with neural networks,” Trans. on Computational\\nImaging, vol. 3, no. 1, 2017.\\n[41]\\nK. Janocha and W. M. Czarnecki, “On loss functions for deep\\nneural networks in classiﬁcation,” arXiv:1702.05659, 2017.\\n[42]\\nA. Kendall and R. Cipolla, “Geometric loss functions for camera\\npose regression with deep learning,” in CVPR, 2017.\\n[43]\\nJ.-J. Hwang, T.-W. Ke, J. Shi, and S. X. Yu, “Adversarial structure\\nmatching for structured prediction tasks,” in CVPR, 2019.\\n[44]\\nM. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video\\nprediction beyond mean square error,” in ICLR (Poster), 2016.\\n[45]\\nA. Dosovitskiy and T. Brox, “Generating images with perceptual\\nsimilarity metrics based on deep networks,” in NIPS, 2016.\\n[46]\\nJ. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-\\ntime style transfer and super-resolution,” in ECCV, 2016.\\n[47]\\nC. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham,\\nA. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi,\\n“Photo-realistic single image super-resolution using a generative\\nadversarial network,” in CVPR, 2017.\\n[48]\\nM. S. M. Sajjadi, B. Sch¨olkopf, and M. Hirsch, “Enhancenet: Single\\nimage super-resolution through automated texture synthesis,” in\\nICCV, 2017.\\n[49]\\nJ. Zhu, P. Kr¨ahenb¨uhl, E. Shechtman, and A. A. Efros, “Generative\\nvisual manipulation on the natural image manifold,” in ECCV,\\nser. Lecture Notes in Computer Science, vol. 9909, 2016.\\n[50]\\nW. Lotter, G. Kreiman, and D. D. Cox, “Unsupervised learn-\\ning of visual structure using predictive generative networks,”\\narXiv:1511.06380, 2015.\\n[51]\\nX. Chen, W. Wang, J. Wang, and W. Li, “Learning object-centric\\ntransformation for video prediction,” in ACM-MM, ser. MM ’17.\\nNew York, NY, USA: ACM, 2017.\\n[52]\\nX. Jin, X. Li, H. Xiao, X. Shen, Z. Lin, J. Yang, Y. Chen, J. Dong,\\nL. Liu, Z. Jie, J. Feng, and S. Yan, “Video Scene Parsing with\\nPredictive Feature Learning,” in ICCV, 2017.\\n[53]\\nN. Wichers, R. Villegas, D. Erhan, and H. Lee, “Hierarchical\\nlong-term video prediction without supervision,” in ICML, ser.\\nProceedings of Machine Learning Research, vol. 80, 2018.\\n[54]\\nR. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee, “Learn-\\ning to generate long-term future via hierarchical prediction,” in\\nICML, 2017.\\n[55]\\nJ. Walker, K. Marino, A. Gupta, and M. Hebert, “The pose knows:\\nVideo forecasting by generating pose futures,” in ICCV, 2017.\\n[56]\\nX. Liang, L. Lee, W. Dai, and E. P. Xing, “Dual motion GAN for\\nfuture-ﬂow embedded video prediction,” in ICCV, 2017.\\n[57]\\nP. Luc, N. Neverova, C. Couprie, J. Verbeek, and Y. LeCun,\\n“Predicting Deeper into the Future of Semantic Segmentation,”\\nin ICCV, 2017.\\n[58]\\nZ. Hu and J. Wang, “A novel adversarial inference framework for\\nvideo prediction with action control,” in ICCV Workshops, 2019.\\n[59]\\nC. Sch¨uldt, I. Laptev, and B. Caputo, “Recognizing human ac-\\ntions: A local SVM approach,” in ICPR, 2004.\\n[60]\\nL. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri,\\n“Actions as space-time shapes,” TPAMI, vol. 29, no. 12, 2007.\\n[61]\\nH. Kuehne, H. Jhuang, E. Garrote, T. A. Poggio, and T. Serre,\\n“HMDB: A large video database for human motion recognition,”\\nin ICCV, 2011.\\n[62]\\nK. Soomro, A. R. Zamir, and M. Shah, “UCF101: A dataset of 101\\nhuman actions classes from videos in the wild,” arXiv:1212.0402,\\n2012.\\n[63]\\nW. Zhang, M. Zhu, and K. G. Derpanis, “From actemes to\\naction: A strongly-supervised representation for detailed action\\nunderstanding,” in ICCV, 2013.\\n[64]\\nC. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Hu-\\nman3.6m: Large scale datasets and predictive methods for 3d\\nhuman sensing in natural environments,” TPAMI, vol. 36, no. 7,\\n2014.\\n[65]\\nH. Idrees, A. R. Zamir, Y. Jiang, A. Gorban, I. Laptev, R. Suk-\\nthankar, and M. Shah, “The THUMOS challenge on action recog-\\nnition for videos ”in the wild”,” CVIU, vol. 155, 2017.\\n[66]\\nV. Patraucean, A. Handa, and R. Cipolla, “Spatio-temporal video\\nautoencoder with differentiable memory,” (ICLR) Workshop, 2015.\\n[67]\\nP. Doll´ar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detec-\\ntion: A benchmark,” in CVPR, 2009.\\n[68]\\nA. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets\\nrobotics: The kitti dataset,” IJRR, vol. 32, no. 11, 2013.\\n[69]\\nM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\\nR. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes\\ndataset for semantic urban scene understanding,” in CVPR, 2016.\\n[70]\\nW. Lotter, G. Kreiman, and D. Cox, “Deep Predictive Coding\\nNetworks for Video Prediction and Unsupervised Learning,” in\\nICLR (Poster), 2017.\\n[71]\\nX. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin,\\nand R. Yang, “The apolloscape dataset for autonomous driving,”\\narXiv: 1803.06184, 2018.\\n[72]\\nH. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,\\nA. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A\\nmultimodal dataset for autonomous driving,” arXiv:1903.11027,\\n2019.\\n[73]\\nP. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik,\\nP. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han,\\nJ. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao,\\nA. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, “Scalabil-\\nity in perception for autonomous driving: Waymo open dataset,”\\nin CVPR, 2020.\\n[74]\\nA. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and\\nF. Li, “Large-scale video classiﬁcation with convolutional neural\\nnetworks,” in CVPR, 2014.\\n[75]\\nS. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,\\nB. Varadarajan, and S. Vijayanarasimhan, “Youtube-8m: A large-\\nscale video classiﬁcation benchmark,” arXiv:1609.08675, 2016.\\n[76]\\nB. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,\\nD. Poland, D. Borth, and L. Li, “YFCC100M: the new data in\\nmultimedia research,” Commun. ACM, vol. 59, no. 2, 2016.\\n[77]\\nI. Sutskever, G. E. Hinton, and G. W. Taylor, “The recurrent\\ntemporal restricted boltzmann machine,” in NIPS, 2008.\\n[78]\\nC. F. Cadieu and B. A. Olshausen, “Learning intermediate-level\\nrepresentations of form and motion from natural movies,” Neural\\nComputation, vol. 24, no. 4, 2012.\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 17}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n18\\n[79]\\nR. Memisevic and G. Exarchakis, “Learning invariant features by\\nharnessing the aperture problem,” in ICML, vol. 28, 2013.\\n[80]\\nN. Srivastava, E. Mansimov, and R. Salakhutdinov, “Unsuper-\\nvised Learning of Video Representations using LSTMs,” in ICML,\\n2015.\\n[81]\\nF. Ebert, C. Finn, A. X. Lee, and S. Levine, “Self-supervised\\nvisual planning with temporal skip connections,” in CoRL, ser.\\nProceedings of Machine Learning Research, vol. 78, 2017.\\n[82]\\nS. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper,\\nS. Singh, S. Levine, and C. Finn, “Robonet: Large-scale multi-\\nrobot learning,” arXiv:1910.11215, 2019.\\n[83]\\nR. Vezzani and R. Cucchiara, “Video surveillance online repos-\\nitory (visor): an integrated framework,” Multimedia Tools Appl.,\\nvol. 50, no. 2, 2010.\\n[84]\\nJ. Santner, C. Leistner, A. Saffari, T. Pock, and H. Bischof, “PROST:\\nparallel robust online simple tracking,” in CVPR, 2010.\\n[85]\\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The\\narcade learning environment: An evaluation platform for general\\nagents,” J. Artif. Intell. Res., vol. 47, 2013.\\n[86]\\nG. Seguin, P. Bojanowski, R. Lajugie, and I. Laptev, “Instance-\\nlevel video segmentation from object tracks,” in CVPR, 2016.\\n[87]\\nA. Garcia-Garcia, P. Martinez-Gonzalez, S. Oprea, J. A. Castro-\\nVargas, S. Orts-Escolano, J. Garcia-Rodriguez, and A. Jover-\\nAlvarez, “The robotrix: An extremely photorealistic and very-\\nlarge-scale indoor dataset of sequences with robot trajectories\\nand interactions,” in IROS, 2018, pp. 6790–6797.\\n[88]\\nZ. Bauer, F. Gomez-Donoso, E. Cruz, S. Orts-Escolano, and M. Ca-\\nzorla, “UASOL, a large-scale high-resolution outdoor stereo\\ndataset,” Scientiﬁc Data, vol. 6, no. 1, 2019.\\n[89]\\nM. Minderer, C. Sun, R. Villegas, F. Cole, K. P. Murphy, and\\nH. Lee, “Unsupervised learning of object structure and dynamics\\nfrom videos,” in NeurIPS, 2019.\\n[90]\\nC. Vondrick, H. Pirsiavash, and A. Torralba, “Anticipating Visual\\nRepresentations from Unlabeled Video,” in CVPR, 2016.\\n[91]\\nD. Jayaraman and K. Grauman, “Look-ahead before you leap:\\nEnd-to-end active recognition by forecasting the effect of mo-\\ntion,” in ECCV, vol. 9909, 2016.\\n[92]\\nJ. Walker, C. Doersch, A. Gupta, and M. Hebert, “An Uncertain\\nFuture: Forecasting from Static Images Using Variational Autoen-\\ncoders,” in ECCV, 2016.\\n[93]\\nB. Chen, W. Wang, and J. Wang, “Video imagination from a sin-\\ngle image with transformation generation,” in ACM Multimedia,\\n2017.\\n[94]\\nZ. Hao, X. Huang, and S. J. Belongie, “Controllable video gener-\\nation with sparse trajectories,” in CVPR, 2018.\\n[95]\\nY. Ye, M. Singh, A. Gupta, and S. Tulsiani, “Compositional video\\nprediction,” in ICCV, October 2019.\\n[96]\\nD. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi, “Dream to\\ncontrol: Learning behaviors by latent imagination,” in ICLR, 2020.\\n[97]\\nM. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and\\nS. Chopra, “Video (language) modeling: a baseline for generative\\nmodels of natural videos,” arXiv:1412.6604, 2014.\\n[98]\\nT. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock´y, and S. Khu-\\ndanpur, “Recurrent neural network based language model,” in\\nINTERSPEECH, 2010.\\n[99]\\nK. Simonyan and A. Zisserman, “Very deep convolutional net-\\nworks for large-scale image recognition,” in ICLR, 2015.\\n[100] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, “High accuracy\\noptical ﬂow estimation based on a theory for warping,” in ECCV,\\nT. Pajdla and J. Matas, Eds., vol. 3024, 2004.\\n[101] Y. Zhou and T. L. Berg, “Learning Temporal Transformations\\nfrom Time-Lapse Videos,” in ECCV, 2016.\\n[102] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing\\nof gans for improved quality, stability, and variation,” in ICLR,\\n2018.\\n[103] S. Aigner and M. K¨orner, “Futuregan: Anticipating the future\\nframes of video sequences using spatio-temporal 3d convolutions\\nin progressively growing autoencoder gans,” arXiv:1810.01325,\\n2018.\\n[104] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.\\nCourville, “Improved training of wasserstein gans,” in NIPS,\\n2017.\\n[105] J. Zhang, Y. Wang, M. Long, W. Jianmin, and P. S. Yu, “Z-order\\nrecurrent neural networks for video prediction,” in ICME, 2019.\\n[106] B. Jin, Y. Hu, Q. Tang, J. Niu, Z. Shi, Y. Han, and X. Li, “Exploring\\nspatial-temporal multi-frequency analysis for high-ﬁdelity and\\ntemporal-consistency video prediction,” arXiv:2002.09905, 2020.\\n[107] O. Shouno, “Photo-realistic video prediction on natural videos of\\nlargely changing frames,” arXiv:2003.08635, 2020.\\n[108] Y.-H. Kwon and M.-G. Park, “Predicting future frames using\\nretrospective cycle gan,” in CVPR, 2019.\\n[109] R. Hou, H. Chang, B. Ma, and X. Chen, “Video prediction with\\nbidirectional constraint network,” in FG, 2019.\\n[110] M. Oliu, J. Selva, and S. Escalera, “Folded recurrent neural\\nnetworks for future video prediction,” in ECCV, 2018.\\n[111] Y. Wang, L. Jiang, M.-H. Yang, L.-J. Li, M. Long, and L. Fei-Fei,\\n“Eidetic 3d LSTM: A model for video prediction and beyond,” in\\nICLR, 2019.\\n[112] W. Yu, Y. Lu, S. Easterbrook, and S. Fidler, “Efﬁcient and\\ninformation-preserving future frame prediction and beyond,” in\\nICLR, 2020.\\n[113] W. Byeon, Q. Wang, R. K. Srivastava, and P. Koumoutsakos,\\n“Contextvp: Fully context-aware video prediction,” in CVPR\\n(Workshops), 2018.\\n[114] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan, A. Tao,\\nand B. Catanzaro, “SDC-Net: Video prediction using spatially-\\ndisplaced convolution,” in ECCV, 2018.\\n[115] R. Memisevic and G. E. Hinton, “Learning to represent spa-\\ntial transformations with factored higher-order boltzmann ma-\\nchines,” Neural Computation, vol. 22, no. 6, 2010.\\n[116] R. Memisevic, “Gradient-based learning of higher-order image\\nfeatures,” in ICCV, 2011.\\n[117] ——, “Learning to relate images,” TPAMI, vol. 35, no. 8, 2013.\\n[118] V. Michalski, R. Memisevic, and K. Konda, “Modeling deep tem-\\nporal dependencies with recurrent grammar cells,” in NeurIPS,\\n2014.\\n[119] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n“Spatial Transformer Networks,” in NeurIPS, 2015.\\n[120] C. Lu, M. Hirsch, and B. Sch¨olkopf, “Flexible Spatio-Temporal\\nNetworks for Video Prediction,” in CVPR, 2017.\\n[121] J. R. van Amersfoort, A. Kannan, M. Ranzato, A. Szlam, D. Tran,\\nand S. Chintala, “Transformation-based models of video se-\\nquences,” arXiv:1701.08435, 2017.\\n[122] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. P. Singh, “Action-\\nConditional Video Prediction using Deep Networks in Atari\\nGames,” in NeurIPS, 2015.\\n[123] B. Klein, L. Wolf, and Y. Afek, “A dynamic convolutional layer\\nfor short rangeweather prediction,” in CVPR, 2015.\\n[124] B. D. Brabandere, X. Jia, T. Tuytelaars, and L. V. Gool, “Dynamic\\nﬁlter networks,” in NeurIPS, 2016.\\n[125] C. Vondrick and A. Torralba, “Generating the Future with Ad-\\nversarial Transformers,” in CVPR, 2017.\\n[126] C. Vondrick, H. Pirsiavash, and A. Torralba, “Generating Videos\\nwith Scene Dynamics,” in NeurIPS, 2016.\\n[127] A. Clark, J. Donahue, and K. Simonyan, “Adversarial video\\ngeneration on complex datasets,” 2019.\\n[128] P. Luc, A. Clark, S. Dieleman, D. de Las Casas, Y. Doron, A. Cas-\\nsirer, and K. Simonyan, “Transformation-based adversarial video\\nprediction on large-scale data,” arXiv:2003.04035, 2020.\\n[129] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling\\nin deep convolutional networks for visual recognition,” TPAMI,\\nvol. 37, no. 9, 2015.\\n[130] K. Simonyan and A. Zisserman, “Two-stream convolutional net-\\nworks for action recognition in videos,” in NeurIPS, Z. Ghahra-\\nmani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Wein-\\nberger, Eds., 2014.\\n[131] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, “MoCoGAN: De-\\ncomposing motion and content for video generation,” in CVPR,\\nJune 2018.\\n[132] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee, “Decomposing\\nmotion and content for natural video sequence prediction,” in\\nICLR, 2017.\\n[133] E. L. Denton and V. Birodkar, “Unsupervised learning of disen-\\ntangled representations from video,” in NeurIPS, 2017.\\n[134] H. Gao, H. Xu, Q. Cai, R. Wang, F. Yu, and T. Darrell, “Disentan-\\ngling propagation and generation for video prediction,” in ICCV,\\n2019.\\n[135] Y. Wu, R. Gao, J. Park, and Q. Chen, “Future video synthesis with\\nobject motion prediction,” 2020.\\n[136] J. Hsieh, B. Liu, D. Huang, F. Li, and J. C. Niebles, “Learning\\nto decompose and disentangle representations for video predic-\\ntion,” in NeurIPS, 2018.\\n[137] K. Greff, S. van Steenkiste, and J. Schmidhuber, “Neural expecta-\\ntion maximization,” in NIPS, 2017, pp. 6691–6701.\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 18}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n19\\n[138] S. van Steenkiste, M. Chang, K. Greff, and J. Schmidhuber, “Rela-\\ntional neural expectation maximization: Unsupervised discovery\\nof objects and their interactions,” in ICLR (Poster), 2018.\\n[139] S. van Steenkiste, F. Locatello, J. Schmidhuber, and O. Bachem,\\n“Are disentangled representations helpful for abstract visual\\nreasoning?” in NeurIPS, 2019, pp. 14 222–14 235.\\n[140] S. Chiappa, S. Racani`ere, D. Wierstra, and S. Mohamed, “Recur-\\nrent environment simulators,” in ICLR, 2017.\\n[141] K. Fragkiadaki, P. Agrawal, S. Levine, and J. Malik, “Learning\\nvisual predictive models of physics for playing billiards,” in ICLR\\n(Poster), 2016.\\n[142] A. Dosovitskiy and V. Koltun, “Learning to Act by Predicting the\\nFuture,” in ICLR, 2017.\\n[143] P. Luc, “Self-supervised learning of predictive segmentation\\nmodels from video,” Theses, Universit´e Grenoble Alpes, Jun.\\n2019. [Online]. Available: https://tel.archives-ouvertes.fr/tel-\\n02196890\\n[144] X. Jin, H. Xiao, X. Shen, J. Yang, Z. Lin, Y. Chen, Z. Jie, J. Feng,\\nand S. Yan, “Predicting Scene Parsing and Motion Dynamics in\\nthe Future,” in NeurIPS, 2017.\\n[145] S. shahabeddin Nabavi, M. Rochan, and Y. Wang, “Future Seman-\\ntic Segmentation with Convolutional LSTM,” in BMVC, 2018.\\n[146] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in\\nNIPS, 2014.\\n[147] G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge\\nin a neural network,” arXiv:1503.02531, 2015.\\n[148] J. Saric, M. Orsic, T. Antunovic, S. Vrazic, and S. Segvic, “Warp\\nto the future: Joint forecasting of features and feature motion,” in\\nCVPR, 2020, pp. 10 645–10 654.\\n[149] H.-k. Chiu, E. Adeli, and J. C. Niebles, “Segmenting the future,”\\narXiv:1904.10666, 2019.\\n[150] S. Vora, R. Mahjourian, S. Pirk, and A. Angelova, “Future seg-\\nmentation using 3d structure,” arXiv:1811.11358, 2018.\\n[151] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid,\\n“EpicFlow: Edge-preserving interpolation of correspondences for\\noptical ﬂow,” in CVPR, 2015.\\n[152] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing\\nnetwork,” in CVPR, 2017.\\n[153] F. Yu, V. Koltun, and T. A. Funkhouser, “Dilated residual net-\\nworks,” in CVPR, 2017.\\n[154] M. Rosca, B. Lakshminarayanan, D. Warde-Farley, and S. Mo-\\nhamed, “Variational approaches for auto-encoding generative\\nadversarial networks,” arXiv:1706.04987, 2017.\\n[155] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick, “Mask R-CNN,”\\nin ICCV, 2017.\\n[156] J. Sun, J. Xie, J. Hu, Z. Lin, J. Lai, W. Zeng, and W. Zheng, “Pre-\\ndicting future instance segmentation with contextual pyramid\\nconvLSTMs,” in ACM Multimedia.\\nACM, 2019.\\n[157] S. E. Reed, Y. Zhang, Y. Zhang, and H. Lee, “Deep visual analogy-\\nmaking,” in NIPS, 2015.\\n[158] N. Fushishita, A. Tejero-de-Pablos, Y. Mukuta, and T. Harada,\\n“Long-term video generation of multiple futures using human\\nposes,” arXiv:1904.07538, 2019.\\n[159] A. X. Lee, R. Zhang, F. Ebert, P. Abbeel, C. Finn, and S. Levine,\\n“Stochastic adversarial video prediction,” arXiv:1804.01523, 2018.\\n[160] J. Tang, H. Hu, Q. Zhou, H. Shan, C. Tian, and T. Q. S. Quek,\\n“Pose guided global and local gan for appearance preserving\\nhuman video prediction,” in ICIP, Sep. 2019.\\n[161] E. Denton and R. Fergus, “Stochastic video generation with a\\nlearned prior,” in ICML, ser. Proceedings of Machine Learning\\nResearch, J. G. Dy and A. Krause, Eds., vol. 80, 2018.\\n[162] E. Santana and G. Hotz, “Learning a driving simulator,”\\narXiv:1608.01230, 2016.\\n[163] J. Zhang, Y. Zheng, and D. Qi, “Deep spatio-temporal residual\\nnetworks for citywide crowd ﬂows prediction,” in AAAI, 2017.\\n[164] R. Goyal, S. E. Kahou, V. Michalski, J. Materzynska, S. Westphal,\\nH. Kim, V. Haenel, I. Fr¨und, P. Yianilos, M. Mueller-Freitag,\\nF. Hoppe, C. Thurau, I. Bax, and R. Memisevic, “The ”something\\nsomething” video database for learning and evaluating visual\\ncommon sense,” in ICCV, 2017.\\n[165] W. Luo, W. Liu, and S. Gao, “A revisit of sparse coding based\\nanomaly detection in stacked RNN framework,” in ICCV, 2017.\\n[166] M. Ravanbakhsh, M. Nabi, E. Sangineto, L. Marcenaro, C. S.\\nRegazzoni, and N. Sebe, “Abnormal event detection in videos\\nusing generative adversarial nets,” in ICIP, 2017.\\n[167] “Trafﬁc4cast:\\nTrafﬁc\\nmap\\nmovie\\nforecasting,”\\nhttps://www.iarai.ac.at/trafﬁc4cast/, accessed: 2020-04-14.\\n[168] J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisser-\\nman, “A short note about kinetics-600,” arXiv:1808.01340, 2018.\\n[169] Y. LeCun, F. J. Huang, and L. Bottou, “Learning methods for\\ngeneric object recognition with invariance to pose and lighting,”\\nin CVPR, 2004.\\n[170] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. A.\\nFunkhouser, “Semantic scene completion from a single depth\\nimage,” in CVPR, 2017.\\n[171] M. Menze and A. Geiger, “Object scene ﬂow for autonomous\\nvehicles,” in CVPR, 2015.\\n[172] J. Janai, F. G¨uney, A. Ranjan, M. J. Black, and A. Geiger, “Unsu-\\npervised learning of multi-frame optical ﬂow with occlusions,”\\nin ECCV, vol. 11220, 2018.\\n[173] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, “Segmenta-\\ntion and recognition using structure from motion point clouds,”\\nin ECCV, vol. 5302, 2008.\\n[174] E. Zhan, S. Zheng, Y. Yue, L. Sha, and P. Lucey, “Generating\\nmulti-agent trajectories using programmatic weak supervision,”\\nin ICLR, 2019.\\n[175] R. Goroshin, M. Mathieu, and Y. LeCun, “Learning to linearize\\nunder uncertainty,” in NeurIPS, 2015.\\n[176] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun,\\n“Unsupervised learning of spatiotemporally coherent metrics,”\\nin ICCV, 2015.\\n[177] K. Fragkiadaki, J. Huang, A. Alemi, S. Vijayanarasimhan,\\nS.\\nRicco,\\nand\\nR.\\nSukthankar,\\n“Motion\\nprediction\\nun-\\nder\\nmultimodality\\nwith\\nconditional\\nstochastic\\nnetworks,”\\narXiv:1705.02082, 2017.\\n[178] T. Brox and J. Malik, “Object segmentation by long term analysis\\nof point trajectories,” in ECCV, vol. 6315, 2010.\\n[179] M. Henaff, J. J. Zhao, and Y. LeCun, “Prediction under uncer-\\ntainty with error-encoding networks,” arXiv:1711.04994, 2017.\\n[180] P. Agrawal, A. Nair, P. Abbeel, J. Malik, and S. Levine, “Learning\\nto poke by poking: Experiential learning of intuitive physics,” in\\nNeurIPS, 2016, p. 5092–5100.\\n[181] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,\\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\\nreinforcement learning,” in ICML, vol. 48, 2016.\\n[182] J. Zhang and K. Cho, “Query-efﬁcient imitation learning for end-\\nto-end simulated driving,” in AAAI, 2017.\\n[183] L. Castrejon, N. Ballas, and A. Courville, “Improved conditional\\nvrnns for video prediction,” in ICCV, 2019.\\n[184] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and\\nT. Darrell, “BDD100K: A diverse driving video database with\\nscalable annotation tooling,” arXiv:1805.04687, 2018.\\n[185] G. Neuhold, T. Ollmann, S. R. Bul`o, and P. Kontschieder, “The\\nmapillary vistas dataset for semantic understanding of street\\nscenes,” in ICCV, 2017.\\n[186] T. Xue, J. Wu, K. L. Bouman, and B. Freeman, “Visual Dynamics:\\nProbabilistic Future Frame Synthesis via Cross Convolutional\\nNetworks,” in NeurIPS, 2016.\\n[187] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”\\nin ICLR, 2014.\\n[188] X. Yan, J. Yang, K. Sohn, and H. Lee, “Attribute2image: Condi-\\ntional image generation from visual attributes,” in ECCV, 2016.\\n[189] H. Wu, M. Rubinstein, E. Shih, J. V. Guttag, F. Durand, and\\nW. T. Freeman, “Eulerian video magniﬁcation for revealing subtle\\nchanges in the world,” ToG, vol. 31, no. 4, 2012.\\n[190] R. Villegas, A. Pathak, H. Kannan, D. Erhan, Q. V. Le, and H. Lee,\\n“High ﬁdelity video prediction with large stochastic recurrent\\nneural networks,” in NeurIPS, 2019, pp. 81–91.\\n[191] C. K. Sønderby, T. Raiko, L. Maaløe, S. K. Sønderby, and\\nO. Winther, “Ladder variational autoencoders,” in NIPS, 2016.\\n[192] R. Pottorff, J. Nielsen, and D. Wingate, “Video extrapolation with\\nan invertible linear embedding,” arXiv:1903.00133, 2019.\\n[193] D. P. Kingma and P. Dhariwal, “Glow: Generative ﬂow with\\ninvertible 1x1 convolutions,” in NeurIPS, 2018.\\n[194] M. Kumar, M. Babaeizadeh, D. Erhan, C. Finn, S. Levine, L. Dinh,\\nand D. Kingma, “Videoﬂow: A conditional ﬂow-based model for\\nstochastic video generation,” in ICLR, 2020.\\n[195] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image\\nquality assessment: from error visibility to structural similarity,”\\nTrans. on Image Processing, vol. 13, no. 4, 2004.\\n[196] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,\\n“The unreasonable effectiveness of deep features as a perceptual\\nmetric,” in CVPR, 2018.\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20; modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2020-12-14T14:46:25-05:00', 'source': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'file_path': '../data/pdf_files/09.A Review on Deep Learning Techniques for  Video Prediction.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': 'A Review on Deep Learning Techniques for Video Prediction', 'author': '', 'subject': 'IEEE Transactions on Pattern Analysis and Machine Intelligence; ;PP;99;10.1109/TPAMI.2020.3045007', 'keywords': '', 'moddate': '2021-05-28T11:50:31-04:00', 'trapped': '', 'modDate': \"D:20210528115031-04'00'\", 'creationDate': \"D:20201214144625-05'00'\", 'page': 19}, page_content='0162-8828 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3045007, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n20\\n[197] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier,\\nM. Michalski, and S. Gelly, “Towards accurate generative models\\nof video: A new metric & challenges,” arXiv:1812.01717, 2018.\\n[198] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford,\\nand X. Chen, “Improved techniques for training gans,” in NIPS,\\n2016.\\n[199] O. Breuleux, Y. Bengio, and P. Vincent, “Quickly generating\\nrepresentative samples from an rbm-derived process,” Neural\\nComputation, vol. 23, no. 8, 2011.\\n[200] E. Hildreth, “Theory of edge detection,” Proc. of Royal Society of\\nLondon, vol. 207, no. 187-217, 1980.\\n[201] E. L. Denton, S. Chintala, A. Szlam, and R. Fergus, “Deep gen-\\nerative image models using a laplacian pyramid of adversarial\\nnetworks,” in NeurIPS, 2015.\\n[202] Y. Wang, M. Long, J. Wang, Z. Gao, and P. S. Yu, “Predrnn:\\nRecurrent neural networks for predictive learning using spa-\\ntiotemporal lstms,” in NeurIPS, 2017.\\n[203] Y. Wang, Z. Gao, M. Long, J. Wang, and P. S. Yu, “Predrnn++:\\nTowards A resolution of the deep-in-time dilemma in spatiotem-\\nporal predictive learning,” in ICML, ser. Proceedings of Machine\\nLearning Research, vol. 80, 2018.\\n[204] F. Cricri, X. Ni, M. Honkala, E. Aksu, and M. Gabbouj, “Video\\nladder networks,” arXiv:1612.01756, 2016.\\n[205] I. Pr´emont-Schwarz, A. Ilin, T. Hao, A. Rasmus, R. Boney, and\\nH. Valpola, “Recurrent ladder networks,” in NIPS, 2017.\\n[206] N. Kalchbrenner, A. van den Oord, K. Simonyan, I. Danihelka,\\nO. Vinyals, A. Graves, and K. Kavukcuoglu, “Video pixel net-\\nworks,” in ICML, 2017, pp. 1771–1779.\\n[207] B. Jin, Y. Hu, Y. Zeng, Q. Tang, S. Liu, and J. Ye, “Varnet: Exploring\\nvariations for unsupervised video prediction,” in IROS, 2018.\\n[208] J. Lee, J. Lee, S. Lee, and S. Yoon, “Mutual suppression\\nnetwork for video prediction using disentangled features,”\\narXiv:1804.04810, 2018.\\n[209] D. Weissenborn, O. T¨ackstr¨om, and J. Uszkoreit, “Scaling autore-\\ngressive video models,” in ICLR, 2020.\\n[210] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. J. Black, “Towards\\nunderstanding action recognition,” in ICCV, 2013.\\n[211] L. Theis, A. van den Oord, and M. Bethge, “A note on the\\nevaluation of generative models,” in ICLR, 2016.\\nSergiu Oprea is a PhD student at the Depart-\\nment of Computer Technology (DTIC), Univer-\\nsity of Alicante. He received his MSc (Automa-\\ntion and Robotics) and BSc (Computer Science)\\nfrom the same institution in 2017 and 2015 re-\\nspectively. His main research interests include\\nvideo prediction with deep learning, virtual real-\\nity, 3D computer vision, and parallel computing\\non GPUs.\\nPablo Martinez Gonzalez is a PhD student\\nat the Department of Computer Technology\\n(DTIC), University of Alicante. He received his\\nMSc (Computer Graphics, Games and Virtual\\nReality) and BSc (Computer Science) at the\\nRey Juan Carlos University and University of Al-\\nicante, in 2017 and 2015, respectively. His main\\nresearch interests include deep learning, virtual\\nreality and parallel computing on GPUs.\\nAlberto Garcia Garcia is a Postdoctoral Re-\\nsearcher at the Institute of Space Sciences (ICE-\\nCSIC, Barcelona) working on the MAGNESIA\\nERC Consolidator project. He received his PhD\\n(Machine Learning and Computer Vision) from\\nthe University of Alicante in 2019. Previously he\\nwas an intern at NVIDIA Research/Engineering,\\nFacebook Reality Labs, and Oculus Core Tech.\\nHis main research interests include deep learn-\\ning, virtual reality, 3D computer vision, and par-\\nallel computing on GPUs.\\nJohn Alejandro Castro Vargas is a PhD stu-\\ndent at the Department of Computer Technology\\n(DTIC), University of Alicante. He received his\\nMSc (Automation and Robotics) and BSc (Com-\\nputer Science) from the same institution in 2017\\nand 2016 respectively. His main research inter-\\nests include human behavior recognition with\\ndeep learning, virtual reality and parallel comput-\\ning on GPUs.\\nSergio Orts-Escolano received a PhD in Com-\\nputer Science from the University of Alicante\\nin 2014. His research interests include com-\\nputer vision, assistive robotics, 3D sensors, GPU\\ncomputing, virtual/augmented reality and deep\\nlearning. He has authored +50 publications in\\ntop journals and conferences like CVPR, SIG-\\nGRAPH, 3DV, BMVC, IROS, etcetera. He has\\nexperience as a professor in academia and in-\\ndustry, working as a research scientist for com-\\npanies such as Google and Microsoft Research.\\nJose Garcia-Rodriguez received his Ph.D. de-\\ngree, with specialization in Computer Vision and\\nNeural Networks, from the University of Alicante\\n(Spain). He is currently Full Professor at the\\nDepartment of Computer Technology of the Uni-\\nversity of Alicante. His research areas of inter-\\nest include: computer vision, machine learning,\\npattern recognition, robotics, man-machine in-\\nterfaces, ambient intelligence, and parallel and\\nmulticore architectures.\\nAntonis Argyros is a professor of computer\\nscience at the Computer Science Department,\\nUniversity of Crete and a researcher at the Insti-\\ntute of Computer Science, FORTH, in Heraklion,\\nCrete, Greece. His research interests fall in the\\nareas of computer vision and pattern recogni-\\ntion, with emphasis on the analysis of humans\\nin images and videos, human pose analysis,\\nrecognition of human activities and gestures, 3D\\ncomputer vision, as well as image motion and\\ntracking. He is also interested in applications of\\ncomputer vision in the ﬁelds of robotics and smart environments.\\nAuthorized licensed use limited to: Robert Gordon University. Downloaded on May 28,2021 at 15:50:31 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 0}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n1 \\n  \\nAbstract— Video classification task has gained a significant \\nsuccess in the recent years. Specifically, the topic has gained more \\nattention after the emergence of deep learning models as a \\nsuccessful tool for automatically classifying videos. In recognition \\nto the importance of video classification task and to summarize the \\nsuccess of deep learning models for this task, this paper presents a \\nvery comprehensive and concise review on the topic. There are a \\nnumber of existing reviews and survey papers related to video \\nclassification in the scientific literature. However, the existing \\nreview papers are either outdated, and therefore, do not include \\nthe recent state-of-art works or they have some limitations. In \\norder to provide an updated and concise review, this paper \\nhighlights the key findings based on the existing deep learning \\nmodels. The key findings are also discussed in a way to provide \\nfuture research directions. This review mainly focuses on the type \\nof network architecture used, the evaluation criteria to measure \\nthe success, and the data sets used. To make the review self-\\ncontained, the emergence of deep learning methods towards \\nautomatic video classification and the state-of-art deep learning \\nmethods are well explained and summarized. Moreover, a clear \\ninsight of the newly developed deep learning architectures and the \\ntraditional approaches is provided, and the critical challenges \\nbased on the benchmarks are highlighted for evaluating the \\ntechnical progress of these methods. The paper also summarizes \\nthe benchmark datasets and the performance evaluation matrices \\nfor video classification. Based on the compact, complete, and \\nconcise review, the paper proposes new research directions to solve \\nthe challenging video classification problem.  \\n \\nImpact Statement— The importance of accurate video \\nclassification task can be realized by the large amount of video \\ndata available online. Moreover, with an increase in the \\navailability of large-scale video data,  deep learning methods have \\ndemonstrated a high recognition accuracy for classification of \\nvideos. In recognition to the importance of automatic video \\nclassification using deep learning, this paper provides a \\ncomprehensive review on the topic. The paper addresses the \\nlimitations of existing reviews, provides an updated review on the \\nstate-of-art approaches, and offers some useful future research \\ndirections to solve the challenging video classification problem.   \\n \\nIndex Terms— Automatic Video Classification, Deep Learning, \\nHandcrafted Features, Video Processing.  \\nI. INTRODUCTION \\nIDEO classification task has gained a significant success \\nin the recent years. Specifically, the topic has gained more \\n \\nAtiq ur Rehman is with the ICT Division, College of Science and \\nEngineering, Hamad Bin Khalifa University, Doha 34110, Qatar (email: \\natrehman2@hbku.edu.qa; atiqjadoon@gmail.com ). \\nattention after the emergence of deep learning models as a \\nsuccessful tool for automatically classifying videos. The \\nimportance of accurate video classification task can be realized \\nby the large amount of video data available online. People \\naround the world generate and consume a huge amount of video \\ncontent. Currently, on YouTube only, over 1 billion hours of \\nvideo is being watched by different people on every single day. \\nIn recognition to the importance of video classification task, a \\ncombined effort is being made by the researchers for proposing \\nan accurate video classification framework. Companies like \\nGoogle AI are investing in different competitions to solve the \\nchallenging problem under constrained conditions.  To further \\nadvance the progress of automatic video classification task, \\nGoogle AI has released a public dataset called YouTube-8M \\nwith millions of video features and more than 3700 labels. All \\nthese efforts being made demonstrate the need of a powerful \\nvideo classification model. \\nAn Artificial Neural Network (ANN) is an algorithm based \\non the interconnected nodes to recognize the relationships in a \\nset of data. Algorithms based on ANNs have shown a great \\nsuccess in modeling both the linear and the non-linear \\nrelationships in the underlying data. Due to a huge success rate \\nof these algorithms, they are  extensively being used for \\ndifferent real-time applications [1]–[4]. Moreover, with an \\nincrease in the availability of huge datasets, the deep learning \\nmodels have specifically shown a significant improvement in \\nthe classification of videos. This paper reviews studies based on \\ndeep learning approaches for video classification. There are a \\nnumber of existing reviews and survey papers related to video \\nclassification in the scientific literature. However, those review \\npapers are either old and therefore do not include the recent \\nstate-of-art works or they have some limitations. Some of the \\nrecent reviews on video classification with their limitations are \\ndiscussed as follows: (i) Z. Wu [5] presented a concise review \\non video classification specific to deep learning methods. This \\nreview provides a good description on deep learning models, \\nfeature extraction tools, benchmark dataset, and comparison of \\nexisting methods for video classification. However, this review \\nwas conducted in the year 2016 and it does not cover the recent \\nstate-of-art deep learning methods. (ii) Q. Ren [6] conducted a \\nrecent and simple review on video classification methods, \\nhowever the techniques covered in this review are not well \\nSamir Brahim Belhaouari, is also with the ICT Division, College of Science \\nand Engineering, Hamad Bin Khalifa University, Doha 34110, Qatar (email: \\nsbelhaouari@hbku.edu.qa). \\n \\nDeep Learning for Video Classification: A \\nReview  \\nAtiq ur Rehman, Member, IEEE, and Samir Brahim Belhaouari, Senior Member, IEEE \\nV'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 1}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n2 \\ndescribed and the review also lacks in   the description of \\nresearch gaps, benchmark datasets, limitations of existing \\nmethods, and performance metrics. (iii) A more recent review \\nis done by A. Anusya [7], this review covers few methods for \\nvideo classification, clustering, and tagging. However, the \\nreview provided is not comprehensive and lacks in concise \\ninformation, coverage of topic, datasets, analysis of state-of-art \\napproaches, and research limitations. (iv) Rani et al [8] also \\nconducted a recent review on video classification methods, their \\nreview covers some recent video classification approaches and \\nsummary-based description of some recent works. This review \\nhas also some  limitations including the missing analysis of \\nrecent state-of-art approaches, and short description of topics \\ncovered. (v) Y. Li et al [9] have recently conducted a systematic \\nand good review on live sport video classification. This review \\ncovers most of the recent works in live sport video classification \\nincluding the tools, video interaction features, and feature \\nextraction methods. This is a comprehensive review but the \\nfindings are not summarized in tables for research gaps, and \\nadvantages and disadvantages of existing methods for a quick \\nreview. Moreover, this review is more specific to live sport \\nvideo classification. (vi) A recent review is also done by Md \\nIslam et al [10], in this review they have included all the \\nmethods for video classification including deep learning. \\nHowever, as the focus of review is not on deep learning \\napproaches, therefore these methods are not completely \\ncovered in this review.  \\nIn contrast to the existing reviews on classification of videos, \\nthis paper provides a more comprehensive, concise and up-to-\\ndated review of deep learning approaches for video \\nclassification. In this current review, most of the recent state-\\nof-art contributions related to the topic are analyzed and \\ncritically summarized. Deep learning is an emerging and \\nvibrant field for the analysis of videos, therefore we hope this \\nreview will help in stimulating future research along the line. \\nThe following are the key contributions to this review paper:  \\n1. A summary of state-of-art CNN based deep learning \\nmodels for image analysis.  \\n2. An in-depth review of deep learning approaches for \\nvideo classification highlighting the notable findings. \\n3. A summary of breakthrough in automatic video \\nclassification task. \\n4. Analysis of research trends from past towards future.   \\n5. Description of benchmark datasets, evaluations \\nmetrics, and comparison of recent state-of-art deep \\nlearning approaches in terms of performance. \\nThe rest of the paper is organized as follows: Section II \\nreviews some existing Convolutional Neural Networks (CNNs) \\nfor images, Section III provides an in-depth review on Deep \\nLearning models for Video classification, Section IV provides \\na summary for benchmark datasets, evaluation metrics, and \\ncomparison of existing state-of-art for video classification task, \\nand Section V provides conclusion and future research \\ndirections. \\n \\nII. CONVOLUTIONAL NEURAL NETWORKS (CNNS) FOR IMAGE \\nANALYSIS \\nDeep learning models, specifically Convolutional Neural \\nNetworks (CNNs) are well known for understanding images. A \\nnumber of CNN architectures are proposed and developed in \\nthe scientific literature for image analysis. Among these, the \\nmost popular architectures are LeNet-5 [11], AlexNet [12], \\nVGGNet [13], GoogleNet [14], ResNet [15], and DenseNet \\n[16]. The trend that follows from the formerly proposed \\narchitectures towards the recently proposed architectures is to \\ndeepen the network. A summary of these popular CNN \\narchitectures along with trend of deepening the network is \\nshown in Fig. 1. Where, the depth of network increases from \\nleft-most (LeNet-5) to right-most (DenseNet). Deep networks \\nare believed to better approximate the target function and to \\ngenerate better feature representation with more powerful \\ndiscriminatory powers [17]. Although, deeper networks are \\nbetter in terms of having more discriminatory powers, but the \\ndeeper networks require more data for training and more \\nparameters to tune. Finding a professionally labeled huge \\ndataset is still a big challenge faced by the research community \\nand therefore it limits the development of more deeper neural \\nnetworks.  \\n \\nFig. 1: State-of-art image recognition CNN networks. The trend is that the depth \\nand discriminatory powers of network architectures increases from formerly \\nproposed architectures towards the recently proposed architectures.  \\nIII. VIDEO CLASSIFICATION \\nIn this section, a very comprehensive and concise review for \\ndeep learning models employed in video classification task is \\nprovided. This section covers a description on video data \\nmodalities, traditional handcrafted approaches, breakthrough in \\nvideo classification, and recent state-of-art deep learning \\nmodels for video classification.  \\nA. Video data modalities \\nAs compared to images, videos are more challenging to \\nunderstand and classify due to the complex nature of the \\ntemporal content. However, three different modalities i-e visual \\ninformation, audio information, and text information might be \\navailable to classify videos, in contrast to image classification \\nwhere only a single visual modality can be utilized. Based on \\nthe availability of different modalities in videos, the task of \\nclassification can be categorized as Uni-modal video \\nclassification \\nor \\nMulti-modal \\nvideo \\nclassification, \\nas \\nsummarized in Fig. 2. The existing has literature utilized both \\nthese models for the video classification task and it is generally \\nbelieved that models utilizing Multi-modal data perform better \\nthan the models based on Uni-modal data. Moreover, the visual \\nVisual/Image recognition:\\nPopular CNN architectures\\nAlexNet \\nLeNet-5  \\nVGGNet\\nGoogleNet\\nResNet \\nDenseNet \\nDepth of the network architectures has increased over time'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 2}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n3 \\ndescription of a video works better than the text and the audio \\ndescription for the classification purpose of a video. \\n \\nFig. 2. : Different modalities used for classification of videos. \\nB. Traditional Handcrafted Features \\nDuring the earlier developments of the video classification \\ntask, the traditional handcrafted features were combined with \\nstate-of-art machine learning algorithms to classify the videos. \\nSome of the most popular handcrafted feature representation \\ntechniques used in literature are spatiotemporal interest points \\n(STIPs) [18], improved Dense Trajectories (iDT)  [19], SIFT-\\n3D [20], HOG3D [21], Motion Boundary Histogram [22], \\nAction- Bank [23], Cuboids [24], 3D SURF [25], and Dynamic-\\nPoselets [26]. These hand-designed representations use \\ndifferent feature encoding schemes such as the ones based on \\npyramids and histograms. iDT is one of these hand-crafted \\nrepresentations, which is widely considered the state-of-the-art. \\nMany recent competitive studies demonstrated that hand-\\ncrafted features [27]–[30], high-level [31], [32], and mid-level \\n[33], [34] video representations have contributed towards the \\ntask of video classification with deep neural networks. \\nC. Deep Learning frameworks \\nAlong with the development of more powerful deep \\nlearning architectures in the recent years, the trend for video \\nclassification task has followed a shift from traditional \\nhandcrafted approaches to the fully automated deep learning \\napproaches. Among the very common deep learning \\narchitectures used for video classification is a 3D-CNN model. \\nAn example of 3D-CNN architecture used for video \\nclassification is given in Figure 3 [35]. In this architecture, 3D \\nblocks are utilized to capture the video information necessary \\nto classify the video content. One more very common \\narchitecture is a multi-stream architecture, where the spatial and \\ntemporal information is separately processed and the features \\nextracted from different streams are then fused to make a \\ndecision. In order to process the temporal information different \\nmethods are used and the two most common methods are based \\non (i) RNN (mainly LSTM), and (ii) optical flow. An example \\nof multi-stream network model, where the temporal stream is \\nprocessed using optical flow, is shown in Figure 4 [36]. A high \\nlevel overview of the video classification process is shown in \\nFigure 5. Where, the stages of feature extraction and prediction \\nare shown with the most common type of strategies used in the \\nliterature. In the upcoming sections, the breakthrough in video \\nclassification and studies related to classification of videos \\nspecifically using deep learning frameworks are summarized \\ndescribing the success rate of utilizing deep learning \\narchitectures and the associated limitations.  \\n \\nFig. 3: An example of 3D-CNN architecture to classify videos [35]. \\n \\nFig. 4: An example of two stream architecture with optical flow [36]. \\n \\nFigure 5: An overview of video classification process. \\nD. Breakthrough \\nThe breakthrough in recognition of still-images originated \\nwith the introduction of deep learning model called AlexNet \\n[37]. The same concept of still-image recognition using deep \\nlearning is also extended for videos. Where, individual video \\nframes are collectively processed as images by a deep learning \\nmodel to predict the contents of a video. The features from \\nindividual video frames are extracted and then temporal \\nintegration of such features into a fixed-size descriptor using \\npooling is performed. The task is either done using high \\ndimensional feature encoding [38], [39], or through the RNN \\narchitectures [40]–[43].  For un-supervised spatiotemporal \\nfeature learning in 3D convolutions, Restricted Boltzmann \\nMachines [44] and stacked ISA [45] are also studied in parallel. \\nThe 3D CNNs using temporal convolutions to extract temporal \\nfeatures automatically were first proposed by Baccouche et al. \\n[46] and by Ji et al. [47]. \\nE. Basic Deep Learning Architectures for Video Classification \\nThe two most widely used deep learning architectures for \\nvideo classification are Convolutional Neural Network (CNN) \\nand Recurrent Neural Network (RNN). CNNs are mostly used \\nto learn the spatial information from videos, whereas, RNNs are \\nused to learn the temporal information from videos. As, the \\nmain difference between these two architectures is the ability to \\nprocess temporal information or data that comes in sequences. \\nTherefore, both these network architectures are used for \\ncompletely different purposes in general. However, the nature \\nof video data with the presence of both the spatial and the \\ntemporal information demands the use of both these network \\narchitectures to accurately process the two-stream information. \\nVideo Classification based on different \\nModalities\\nUni-Modal\\nMulti-Modal\\nAudio\\nVisual\\nText\\nCombination of Text, Audio, \\nand Visual information.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 3}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n4 \\nThe architecture of a CNN applies different filters in the \\nconvolutional layers to transform the data. RNNs on the other \\nhand reuse the activation functions to generate the next output \\nin a series from the other data points in the sequence. However, \\nthe use of only 2D CNNs alone limit the understanding of video \\nto only spatial domain. RNNs on the other hand have the ability \\nto understand the temporal content of a sequence. Both these \\nbasic architectures, and their enhanced versions, are applied in \\nseveral studies for the task of video classification. \\nF. Developments In Video Classification Over Time  \\nThe existing approaches for video classification are \\ncategorized based on their working principle in Table I. The \\ntrend observed for the classification of videos from the existing \\nliterature is that the recently developed state-of-art deep \\nlearning models are outperforming the earlier handcrafted \\nclassical approaches. This is mainly due to the availability of \\nlarge scale video data for learning deep architectures of neural \\nnetworks. \\nBesides \\nan \\nimprovement \\nin \\nclassification \\nperformance the recently developed models are mostly self-\\nlearned and does not require any manual feature engineering. \\nThis added advantage makes them more feasible for use in real \\napplications. However, the better performing recently \\ndeveloped architectures are deeper as compared to the \\npreviously developed architectures which brings a compromise \\non the computational complexity of the deep architectures. \\nAmong the initially developed hand-crafted representations, \\nimproved Dense Trajectories (iDT) [19] is widely considered \\nthe state-of-the-art. Whereas, many recent competitive studies \\ndemonstrated that hand-crafted features [27]–[30], high-level \\n[31], [32], and mid-level [33], [34] video representations have \\ncontributed towards the task of video classification with deep \\nneural networks. The hand-crafted models were among the very \\nearly developments of video classification problem. Later, 2D-\\nCNNs were proposed for video classification, where image \\nbased CNN models are used to extract frame level features and \\nbased on the frame level CNN features, some state-of-art \\nclassification models (for example SVM) are learned to classify \\nvideos. These 2D CNN models do not require any manual \\nfeature extraction and these models performed better than the \\ncompeting \\nhand-crafted \\napproaches. \\nAfter \\nsuccessful \\ndevelopment of 2D CNN models where features are extracted \\nfrom frame level, the same concept was extended to propose \\n3D-CNNs to extract features from videos. The proposed 3D \\nCNNs are computationally more expensive as compared to the \\n2D CNN models. However, these models consider the time \\nvariations in feature extraction therefore these 3D CNN models \\nare believed to perform better as compared to 2D -CNN models \\nfor video classification.  \\nThe development of 3D CNN models paved the way for \\nfully automatic video classification models using different deep \\nlearning architectures. Among the developments using deep \\nlearning architectures, Spatiotemporal Convolutional Networks \\nare approaches based on integration of temporal and spatial \\ninformation using convolutional networks to perform video \\nclassification. To collect temporal and spatial information, \\nthese methods primarily rely on convolution and pooling layers. \\nStack optical flow is used in two/multi Stream Networks \\nmethods to identify movements in addition to context frame \\nvisuals. Recurrent Spatial Networks use Recurrent Neural \\nNetworks (RNN) to model temporal information in videos, such \\nas LSTM or GRU. The ResNet architecture is used to build \\nmixed convolutional models. They are particularly interested in \\nmodels that utilize 3D convolution in the bottom or top layers \\nbut 2D in the remainder; these are referred to as \"mixed \\nconvolutional\" models. These also include methods based on \\nmixed temporal convolution with different kernel sizes. Besides \\nthese architectures, there are also hybrid approaches based on \\nthe integration of CNN and RNN architectures. A summary of \\nthese architectures in provided in Fig. 6. \\n \\nTABLE I \\nDIFFERENT CATEGORIES OF APPROACHES FOR VIDEO CLASSIFICATION \\nCategories \\nWorking principle \\nReferences \\nHand-crafted approaches \\nThese representations are handcrafted and employ various feature \\nencoding techniques, such as histograms and pyramids. \\n \\nSpatiotemporal Interest Points (STIPs) [18], iDT \\n[19], SIFT-3D [20], HOG3D [21], Motion \\nBoundary Histogram [22], Cuboids [24], Action- \\nBank [23], 3D SURF [25], Dynamic-Poselets \\n[26]. \\n2D- CNNs \\nThese are image based models where frame level feature extraction is \\nperformed using CNN architecture and classification is performed \\nusing state-of-art classification models, for example SVM. \\n[48] \\n3D-CNNs \\n2D image classification extension to 3D for video (For example the \\nInception 3D (I3D) architecture).  \\n[49] \\nSpatiotemporal \\nConvolutional Networks \\nTo aggregate the temporal and the spatial information, these methods \\nprimarily depend on convolution and pooling. \\n[50], [47], [51] \\nRecurrent Spatial Networks \\nTo represent temporal information in videos, recurrent neural \\nnetworks such as LSTM or GRU are used. \\n[52], [40] \\nTwo/multi Stream Networks \\nIn addition to the context frame visuals, these methods use layered \\noptical flow to identify movements. \\n[53], [54], [55], [43] \\nMixed convolutional models \\nModels built with the ResNet architecture in mind. They are \\nparticularly interested in models that utilize 3D convolution in the \\nbottom or top layers but 2D in the remainder; these are referred to as \\n\"mixed convolutional\" models. Or the methods based on mixed \\ntemporal convolution with different kernel sizes. \\n[56], [57] \\nHybrid Approaches \\nThese are models based on integration of CNN and RNN architectures. \\n[58], [59], [60]'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 4}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n5 \\nDifferent deep learning architectures described above \\nemploy different fusion strategies. These fusion strategies are \\neither for the fusion of different features extracted from the \\nvideo or for the fusion of different models used in the \\narchitecture. The fusion strategies mainly used for the extracted \\nfeatures are (i) concatenation, (ii) product, (iii) summation, (iv) \\nmaximum, and. (v) weighted. Where, the concatenation \\napproach simply combines all the features together and all the \\nfeatures are used for classification. The product/summation \\napproach performs the product/summation between the features \\nextracted using different strategies and uses the result of \\nproduct/summation to perform classification. The maximum \\napproach takes the maximum value of the features extracted \\nusing different strategies and uses that for classification. The \\nweighted approach gives different weights to different features \\nand performs the classification using the weighted features. \\nDifferent fusion methods are summarized in Fig. 7. \\nG. Summary of some notable deep learning frameworks \\nA summary of some deep learnings architectures for video \\nclassification is provided in Table II. These studies are \\nsummarized based on the architecture, the datasets, the \\nevaluation metrics, the fusion strategy, and the notable findings. \\nSome notable findings from these studies are as follows: (i) The \\narchitectures employing CNN/RNN for feature extraction have \\nthe ability to perform better than hand-crafted features provided \\nthat enough data is available for training. (ii) Tensor-Train \\nLayer based RNN like LSTM and GRU perform better than the \\nplain RNN architectures for video classification. (iii) It is \\nsometimes necessary to use optical flow for datasets like UCF-\\n101. (iv) It is not always helpful to use optical flow, especially \\nfor the case of videos taken from wild e-g Sports-1M. (v) It is \\nimportant to use a sophisticated sequence processing \\narchitecture like LSTM to take advantage of optical flow. (vi) \\nLSTMs when applied on both the optical flow and the image \\nframes yield the highest performance measure for Sports-1M \\nbenchmark dataset. (vii) Augmenting optical flow and RGB \\ninput helps in improving the performance. (viii) Optical flow \\nmodality provides complementary information. (ix) A high \\ncomputational requirement of optical flow limits its use in real-\\ntime systems. (x) Multi-Stream Multi-Class fusion can perform \\nbetter than Average fusion, Weighted fusion, Kernel average \\nfusion, MKL fusion, and Logistic regression fusion on datasets \\nlike UCF-101 and CCV. (xi) In 3D group convolutional \\nnetworks, the volume of channel interactions play a vital role in \\nachieving a high accuracy. (xii) The Factorization of 3D \\nconvolutions by separating spatiotemporal interactions and \\nchannel interactions can lead to an improvement in accuracy \\nand a decrease in the computational cost. (xiii) 3D channel-\\nseparated convolutions results in a kind of regularization and \\nprevents overfitting. (xiv) Popular frameworks of conventional \\nsemi-supervised algorithms (which were originally developed \\nfor 2D images) are unable to obtain good results for 3D video \\ncategorization. (xv) For semi-supervised learning, a calibrated \\nemployment of the object appearance cues keenly improves the \\naccuracy of the 3D-CNN models. \\n \\n \\n \\nFigure. 6: Summary of video classification approaches. \\n \\nFigure. 7: Different Fusion Types. \\nVideo Classification Models\\nImage based Models \\n(2D CNN)\\nEnd-to-End CNN Models \\n(3D CNN)\\nFrame Level CNN \\nFeature Extraction\\nState-of-art Classification \\nModels, for example: SVM\\n3D kernels for convolution \\nto learn motion information \\nbetween adjacent frames \\nfactorized spatio-temporal \\nconvolutional networks \\na two-stream approach: \\nbreaks down the learning of \\nvideo representation into \\nseparate feature learning of \\nspatial and temporal clues. \\nspatial clues: \\nspatial CNN \\ntemporal clues: \\noptical flow\\nfusion\\nRNN\\nClassical \\nLSTM\\nHybrid Approaches\\nCombination of \\nCNN and RNN \\narchitectures\\nEncoder-\\ndecoder \\nLSTM\\nlearns feature \\nrepresentation in an \\nunsupervised way \\nFusion\\nModality\\n/Features\\nModel\\nDifferent \\narchitecture\\nSame \\narchitecture\\nSummation\\nWeighted\\nConcatenation\\nProduct\\nMaximum'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 5}, page_content=\"> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n6 \\nTABLE II \\nSUMMARY AND FINDINGS OF STUDIES BASED ON DEEP LEARNING MODELS \\nStudy \\nfeatures \\nModel \\nEvaluation \\nDataset \\nProblem \\nFusion \\nFindings \\n[91] \\nAutomatic Spatio-temporal features/ \\nSelf learning. \\nMultiresolution CNN \\narchitecture. \\nBy the fraction of test \\nsamples that contained at \\nleast one of the ground \\ntruth labels in the top k \\npredictions. \\nSports-1M,  \\nUCF-101. \\nMulti-\\nclass \\nSingle frame, \\nEarly Fusion, \\nLate Fusion, \\nSlow Fusion. \\nWhen compared to a multilayer neural network with Rectified Linear Units followed \\nby a Softmax classifier built using histogram features, the Softmax classifier performed \\nbetter (both local features like Texton, HOG, Cuboids etc and global features like color \\nmoments , and Hue- Saturation). \\n[92] \\nVisual \\n(dense \\ntrajectory \\ndescriptors): A 30-d trajectory \\nshape descriptor, a 96-d HOG \\ndescriptor, a 108-d HOF descriptor, \\nand a 108-d MBH descriptor. Audio \\nFeatures: MFCCs and Spectrogram \\nSIFT. \\nDeep neural network \\n(DNN)  \\nmean average precision \\n(mAP). \\nHollywood2, \\nColumbia \\nConsumer \\nVideos \\n(CCV), and \\nCCV+. \\nMulti-\\nclass \\nRegularized \\nfusion \\nof \\nmultiple \\nfeatures. \\nFound better than dense trajectory features, and classification utilizing the basic early \\nfusion technique. \\n[93] \\nTensor-Train Factorization \\nRecurrent \\nneural \\nnetwork (RNN) \\nClassification accuracy. \\nUCF11, \\nHollywood2, \\nYoutube \\nCelebrities \\nFace Data. \\nMulti-\\nclass \\n- \\nTensor-Train Layer based RNN like LSTM and GRU perform better than the plain \\nRNN architectures for video classification. \\n[87] \\nimproved Fisher vector (iFV) and \\nexplicit feature maps to represent \\nfeatures of conv and fc layers. \\nA \\nmultilayer \\nand \\nmultimodal \\nfusion \\nframework of deep \\nneural networks based \\non Fully connected \\n(FC)-RNN \\nClassification accuracy. \\nUCF101, \\nHMDB51. \\nMulti-\\nclass \\nMultilayer \\nand \\nmultimodal \\nfusion \\nframework. \\nWhen compared to enhanced dense trajectories, which require a number of hand-\\ncrafted procedures such as dense point tracking, camera motion estimation, person \\ndetection, and so on, the proposed FC-RNN obtained competitive results. \\n[43] \\nConvolutional \\ntemporal \\nfeature \\npooling architectures (Conv Pooling, \\nLate Pooling, Slow Pooling, Local \\nPooling). \\n \\nTwo \\nCNN \\narchitectures \\n(AlexNet \\nand \\nGoogLeNet), \\nand LSTM. \\nBy the fraction of test \\nsamples that contained at \\nleast one of the ground \\ntruth labels in the top k \\npredictions. \\nUCF101, \\nSports \\n1 \\nmillion. \\n \\nMulti-\\nclass \\nLate fusion \\n(i) UCF-101 necessitates the utilization of optical flow. (ii) Optical flow isn't always \\nbeneficial, especially when the videos are captured in the wild, such as Sports-1M. (iii) \\nTo take use of optical flow, a more advanced sequence processing architecture, such \\nas LSTM, is required. (iv) The maximum documented performance is achieved by \\nusing LSTMs on both image frames and optical flow for the Sports-1M benchmark. \\n[78] \\nSpatiotemporal feature learning: \\na \\nSMART \\nblock \\nto \\nlearn \\nspatiotemporal features. \\nARTNet \\nby \\nintegrating \\nthe \\nSMART block into \\nthe \\nC3D-ResNet18 \\narchitecture. Where, \\nSMART \\nblock \\narchitecture \\nis \\ncomposed \\nof \\nappearance \\nbranch \\nand relation branch. \\nTop-1 and Top-5 accuracy. \\nKinetics, \\nUCF101, \\nand \\nHMDB51. \\nMulti-\\nclass \\nConcatenation \\nand reduction \\noperation. \\n(i) In terms of spatiotemporal feature learning, SMART blocks outperform 3D \\nconvolutions (3D CNN). (ii) In the case of ARTNet, supplementing RGB input with \\noptical flow improves performance. (iii) The optical flow modality can give additional \\ninformation. (iv) Optical flow's high computing cost prevents it from being used in \\nreal-world systems. \\n[89] \\nSpatial, short-term motion and audio \\nclues using CNN. \\n(multimodal features). \\n \\n CNNs-LSTM model \\nwith \\nmulti-stream \\nmulti-class \\nfusion \\nprocess to adaptively \\ndetermine the optimal \\nfusion weights \\nfor \\ngenerating the final \\nscores of each class. \\nClassification accuracy. \\nUCF-101, \\nColumbia \\nConsumer \\nVideos.  \\n \\nMulti-\\nclass \\nMulti-Stream \\nMulti-Class \\nFusion. \\n \\n \\n \\nAverage fusion, Kernel average fusion, Weighted fusion, Logistic regression fusion, \\nand MKL fusion are all proven to be inferior to the proposed Multi-Stream Multi-Class \\nfusion technique. \\n[94] \\nTwo \\ndistinct \\nlayers: \\n1×1×1 \\nconventional \\nconvolutions \\nfor \\nchannel interaction (but no local \\ninteraction) and k×k×k depthwise \\nconvolutions \\nfor \\nlocal \\nspatiotemporal interactions (but not \\nchannel interaction). \\nChannel-Separated \\nConvolutional \\nNetwork (CSN). \\nTwo models: \\ninteraction-preserved \\nchannel-separated \\nnetwork (ip-CSN). \\ninteraction-reduced \\nchannel-separated \\nnetwork (ir-CSN). \\nClassification accuracy. \\nSports1M \\nand Kinetics. \\nMulti-\\nclass \\n- \\n(i) In 3D group convolutional networks, the number of channel interactions has a \\nsignificant impact on accuracy. (ii) Separating channel interactions from \\nspatiotemporal interactions in 3D convolutions improves accuracy and reduces \\ncomputing cost. (iii) Three-dimensional channel-separated convolutions offer \\nregularization and avoid overfitting. \\n[79] \\nThe 3D network is optimized with \\nthree loss functions: (i) cross-\\nentropy (CE) loss, (ii) Pseudo CE \\nLoss, and (iii) Soft CE Loss. \\nSemi-supervised \\nlearning (VideoSSL) \\nwith 3D ResNet-18. \\nTop-1 \\nUCF101, \\nHMDB51, \\nand Kinetics. \\nMulti-\\nclass \\n- \\n(i) For 3D video classification, a direct application of current semi-supervised \\nalgorithms (which were initially designed for 2D imagery) cannot yield adequate \\nresults. (ii) The accuracy of 3D-CNN models is much improved by a calibrated use of \\nobject appearance indicators for semi-supervised learning.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 6}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n7 \\nH. Geometric Deep Learning \\nGeometric deep learning deals with non-Euclidean graph \\nand manifold data. This type of data (irregularly arranged/ \\ndistributed randomly) is usually used to describe geometric \\nshapes. The purpose of geometric deep learning is to find the \\nunderlying patterns in geometric data where the traditional \\nEuclidean distance based deep learning approaches are not \\nsuitable. There are basically two methods available in literature \\nto apply deep learning on geometric data: (i) extrinsic methods, \\nand (ii) intrinsic methods. An example of these two methods is \\nillustrated in Fig. 8 [61]. The filters in intrinsic approaches are \\napplied on the 3D surfaces without being affected by the \\nstructural deformity. Rather than Euclidean realization, \\nintrinsic methods work on the manifold and are isometry-\\ninvariant by construction. Some of the works based on intrinsic \\ndeep learning include (i) Geodesic CNN [62], (ii) Anisotropic \\nCNN [63],  (iii) Mixture model network [64], (iv) Structured \\nPrediction Model [65], (v) Localized Spectral CNN [66], (vi) \\nPointNet [67], (vii) PointNet++ [68], and (viii) RGA-MLP [69]. \\nThe application of geometric deep learning (mostly intrinsic \\nmethods) in analyzing videos can help in better understanding \\nfrom the machine perspective, but it is still an open research \\nproblem and needs further investigation. \\n \\nFig. 8: Illustration of deep learning approaches on geometric data. (a) extrinsic \\nmethod and (b) intrinsic method.  \\nIV. BENCHMARK DATASETS, EVALUATION METRICS, AND \\nCOMPARISON OF EXISTING STATE-OF-ART FOR VIDEO \\nCLASSIFICATION \\nA. Benchmark Datasets For Video Classification \\nThere are several benchmark datasets being utilized for \\nclassification of videos, some of these notable datasets are \\nsummarized in Table III. The details related to these datasets \\nsuch as total number of videos contained in the dataset, number \\nof classes present in the dataset, the year of publication of \\ndataset, and the background of videos in the dataset are included \\nin the summary.  \\nB. Performance evaluation metrics for Video Classification \\nThe evaluation of video classification models is done using \\ndifferent performance measures. The most common measures \\nutilized to evaluate the models are Accuracy, Precision, Recall, \\nF1 score, Micro F1, and K-fold [10]. Some of the recent studies \\nusing these measures are listed in Table IV. \\nTABLE III \\nCOMMONLY USED EVALUATION METRICS FOR VIDEO CLASSIFICATION \\nEvaluation Metric \\nYear of Publication \\nReference \\nAccuracy \\n2020-2021 \\n[70]–[74] \\nPrecision \\n2020-2021 \\n[70], [72], [73] \\nRecall \\n2020-2021 \\n[70], [72], [73] \\nF1 Score \\n2020-2021 \\n[70], [72], [73] \\nMicro F1 \\n2020 \\n[75], [76] \\nK-Fold \\n2019 \\n[77] \\nTop-k  \\n2018,2021 \\n[78], [79] \\n \\nTABLE IV \\n BENCHMARK DATASETS \\nDataset \\n# of Videos  \\n# of Classes \\nYear \\nBackground \\nKTH \\n600 \\n6 \\n2004 \\nStatic \\nWeizmann \\n81 \\n9 \\n2005 \\nStatic \\nKodak \\n1358 \\n25 \\n2007 \\nDynamic \\nHollywood \\n430 \\n8 \\n2008 \\nDynamic \\nHollywood2 \\n1787 \\n12 \\n2009 \\nDynamic \\nMCG-WEBV \\n234414 \\n15 \\n2009 \\nDynamic \\nOlympic Sports \\n800 \\n16 \\n2010 \\nDynamic \\nHMDB51 \\n6766 \\n51 \\n2011 \\nDynamic \\nCCV \\n9317 \\n20 \\n2011 \\nDynamic \\nUCF-101 \\n13320 \\n101 \\n2012 \\nDynamic \\nTHUMOS-2014 \\n18394 \\n101 \\n2014 \\nDynamic \\nMED-2014 (Dev. set) \\n31000 \\n20 \\n2014 \\nDynamic \\nSports-1M \\n1133158 \\n487 \\n2014 \\nDynamic \\nActivityNet \\n27901 \\n203 \\n2015 \\nDynamic \\nEventNet \\n95321 \\n500 \\n2015 \\nDynamic \\nMPII Human Pose \\n20943 \\n410 \\n2014 \\nDynamic \\nFCVID \\n91223 \\n239 \\n2015 \\nDynamic \\nUCF11 \\n1600 \\n11 \\n2009 \\nDynamic \\nYoutube Celebrities Face \\n1910 \\n47 \\n2008 \\nDynamic \\nKinetics \\n300000 \\n400 \\n2017 \\nDynamic \\nYoutube-8M \\n6.1 M \\n3862 \\n2018 \\nDynamic \\nJHMDB \\n928 \\n21 \\n2011 \\nDynamic \\nSomething-something \\n110000 \\n174 \\n2017 \\nDynamic'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 7}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n8 \\nC. Comparison Of Some Existing Approaches On UCF-101 \\nDataset \\nUCF-101 is a benchmark action recognition dataset \\npublished by the researchers of University of Central Florida in \\nthe year 2012 [80], the videos in the dataset are collected from \\nthe YouTube. Total videos in the dataset are 13320 with 101 \\naction categories. The dataset is challenging because of the \\nuncontrolled environment in the captured videos and it is \\nwidely being used by researches working on video \\nclassification problem. Therefore, it is easy to compare most of \\nthe existing literature based on this dataset. The existing works \\nemploying UCF-101 are compared in Table V, where the \\nmethods are arranged in ascending order based on the \\nperformance. The results reported in Table V are taken from the \\nexisting studies in the literature. \\nTABLE V \\nCOMPARISON OF VIDEO CLASSIFICATION METHODS ON UCF-101 \\nMethod \\nAccuracy \\nLRCN [41] \\n82.9 \\nDT + MVSV [81] \\n83.5 \\nLSTM - Composite [42] \\n84.3 \\nFSTCN [82] \\n88.1 \\nC3D [83] \\n85.2 \\niDT + HSV [84] \\n87.9 \\nTwo-Stream [53] \\n88.0 \\nRNN-FV [85] \\n88.0 \\nLSTM [43] \\n88.6 \\nMultiSource CNN [86] \\n89.1 \\nImage-Based [48] \\n89.6 \\nTDD [27] \\n90.3 \\nMultilayer and Multimodal Fusion [87] \\n91.6 \\nTransformation CNN [88] \\n92.4 \\nMulti-Stream [89] \\n92.6 \\nKey Volume Mining [90] \\n92.7 \\nConvolutional Two-Stream [54] \\n93.5 \\nTemporal Segment Networks [31] \\n94.2 \\nV. CONCLUSIONS AND FUTURE RESEARCH DIRECTIONS \\nThis article reviews deep learning approaches for the task of \\nvideo classification. Some of the notable studies are \\nsummarized in detail and the key findings in these studies are \\nhighlighted. The key findings are reported as an effort to help \\nthe research community in developing new deep learning \\nmodels for video classification. From the analysis of the \\nexisting literature, following conclusions are drawn for video \\nclassification task: (i) The visual description works better than \\nthe text and the audio description and the combination of all \\nmodalities can contribute to better performance with an \\nincrease in computational cost. (ii) The architectures employing \\nCNN/RNN for feature extraction have the ability to perform \\nbetter than hand-crafted features provided that enough data is \\navailable for training. (iii) Tensor-Train Layer based RNN like \\nLSTM and GRU perform better than the plain RNN \\narchitectures for video classification. (iv) It is sometimes \\nnecessary to use optical flow for datasets like UCF-101. (v) It \\nis not always helpful to use optical flow, especially for the case \\nof videos taken from wild e-g Sports-1M. (vi) It is important to \\nuse a sophisticated sequence processing architecture like LSTM \\nto take advantage of optical flow. (vii) LSTMs when applied on \\nboth the optical flow and the image frames yield the highest \\nperformance measure for Sports-1M benchmark dataset. (viii) \\nAugmenting optical flow and RGB input helps in improving the \\nperformance. \\n(ix) \\nOptical \\nflow \\nmodality \\nprovides \\ncomplementary information. (x) A high computational \\nrequirement of optical flow limits its use in real-time systems. \\n(xi) Multi-Stream Multi-Class fusion can perform better than \\nAverage fusion, Weighted fusion, Kernel average fusion, MKL \\nfusion, and Logistic regression fusion on datasets like UCF-101 \\nand CCV. (xii) In 3D group convolutional networks, the volume \\nof channel interactions play a vital role in achieving a high \\naccuracy. (xiii) The Factorization of 3D convolutions by \\nseparating spatiotemporal interactions and channel interactions \\ncan lead to an improvement in accuracy and a decrease in the \\ncomputational cost. (xiv) 3D channel- separated convolutions \\nresults in a kind of regularization and prevents overfitting. (xv) \\nPopular \\nframeworks \\nof \\nconventional \\nsemi-supervised \\nalgorithms (which were originally developed for 2D images) \\nare unable to obtain good results for 3D video categorization. \\n(xvi) For semi-supervised learning, a calibrated employment of \\nthe object appearance cues keenly improves the accuracy of the \\n3D-CNN models. \\nAlthough, the latest developments in deep learning models \\nhave demonstrated the potential of these approaches for video \\nclassification task. However, most of the existing deep learning \\narchitectures for video classification are basically adopted from \\nthe favored deep learning architectures in image/speech \\ndomain. Therefore, most of the existing architectures remain \\ninsufficient to deal with the more complicated nature of video \\ndata that contain a rich information in the form of spatial, \\ntemporal, and acoustic clues. This calls an attention towards the \\nneed for a tailored network capable of effectively modeling the \\nspatial, temporal, and acoustic information. Moreover, training \\nCNN/RNN models require labeled datasets and acquiring those \\ndatasets are usually time-consuming and expensive, and hence \\na promising research direction is to utilize the considerable \\namount of unlabeled video data to derive better video \\nrepresentations.  \\nFurthermore, \\nthe \\ndeep \\nlearning \\napproaches \\nare \\noutperforming other state-of-the-art approaches for video \\nclassification. The deep learning google trend is still growing \\nand it is still above the trend for some other very well-known \\nmachine learning algorithms, as shown in Fig. 9 (a). However, \\nthe recent developments in deep learning approaches are still \\nunder evaluated and require further investigations for video \\nclassification task. One such example is geometric deep \\nlearning approaches, the worldwide research interest in this \\nspecific topic is shown in Figure 9 (b). Which describes that this \\ntopic is still confined to some states of US and has yet to be \\ndeveloped and investigated further. The use of geometric deep \\nlearning in extracting rich spatial information from the videos \\ncan also be a new research direction for better accuracy in video \\nclassification task.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 8}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n9 \\n \\nFigure 9: (a) Google trend on deep learning Vs some other state-of-the-art \\nmethods. (b) Worldwide research interest in geometric deep learning.  \\nREFERENCES  \\n[1] \\nW. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K. R. \\nMüller, “Explaining Deep Neural Networks and Beyond: A Review \\nof Methods and Applications,” Proc. IEEE, vol. 109, no. 3, pp. 247–\\n278, 2021. \\n[2] \\nS. Kiranyaz, O. Avci, O. Abdeljaber, T. Ince, M. Gabbouj, and D. J. \\nInman, “1D convolutional neural networks and applications: A \\nsurvey,” Mech. Syst. Signal Process., vol. 151, 2021. \\n[3] \\nN. Minallah, M. Tariq, N. Aziz, W. Khan, A. ur Rehman, and S. B. \\nBelhaouari, “On the performance of fusion based planet-scope and \\nSentinel-2 data for crop classification using inception inspired deep \\nconvolutional neural network,” PLoS One, vol. 15, no. 9 September, \\n2020. \\n[4] \\nA. U. Rehman and A. Bermak, “Averaging neural network ensembles \\nmodel for quantification of volatile organic compound,” 2019 15th \\nInt. Wirel. Commun. Mob. Comput. Conf. IWCMC 2019, pp. 848–\\n852, 2019. \\n[5] \\nZ. Wu, T. Yao, Y. Fu, and Y.-G. Jiang, “Deep learning for video \\nclassification and captioning,” Front. Multimed. Res., pp. 3–29, 2017. \\n[6] \\nQ. REN et al., “A Survey on Video Classification Methods Based on \\nDeep Learning,” DEStech Trans. Comput. Sci. Eng., no. cisnrc, 2019. \\n[7] \\nA. Anushya, “Video Tagging Using Deep Learning\\u2009: a Survey,” Int. \\nJ. Comput. Sci. Mob. Comput., vol. 9, no. 2, pp. 49–55, 2020. \\n[8] \\nP. Rani, J. Kaur, and S. Kaswan, “Automatic Video Classification: A \\nReview,” EAI Endorsed Trans. Creat. Technol., vol. 7, no. 24, p. \\n163996, 2020. \\n[9] \\nY. Li, C. Wang, and J. Liu, “A systematic review of literature on user \\nbehavior in video game live streaming,” Int. J. Environ. Res. Public \\nHealth, vol. 17, no. 9, 2020. \\n[10] \\nM. S. Islam, M. S. Sultana, U. K. Roy, and J. Al Mahmud, “A review \\non Video Classification with Methods, Findings, Performance, \\nChallenges, Limitations and Future Work,” J. Ilm. Tek. Elektro \\nKomput. dan Inform., vol. 6, no. 2, p. 47, 2021. \\n[11] \\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based \\nLearning Applied to Document Recognition,” Intell. Signal Process., \\npp. 306–351, 2001. \\n[12] \\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet \\nclassification with deep convolutional neural networks,” in Advances \\nin Neural Information Processing Systems, 2012, vol. 2, pp. 1097–\\n1105. \\n[13] \\nSimonyan.Karen and Zisserman.Andrew, “Very deep convolutional \\nnetworks for large-scale image recognition,” in 3rd International \\nConference on Learning Representations, ICLR 2015 - Conference \\nTrack Proceedings, 2015. \\n[14] \\nC. Szegedy et al., “Going deeper with convolutions,” in Proceedings \\nof the IEEE Computer Society Conference on Computer Vision and \\nPattern Recognition, 2015, vol. 07-12-June, pp. 1–9. \\n[15] \\nK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for \\nimage recognition,” in Proceedings of the IEEE Computer Society \\nConference on Computer Vision and Pattern Recognition, 2016, vol. \\n2016-Decem, pp. 770–778. \\n[16] \\nG. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \\n“Densely connected convolutional networks,” in Proceedings - 30th \\nIEEE Conference on Computer Vision and Pattern Recognition, \\nCVPR 2017, 2017, vol. 2017-Janua, pp. 2261–2269. \\n[17] \\nA. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi, “A survey of the \\nrecent architectures of deep convolutional neural networks,” Artif. \\nIntell. Rev., vol. 53, no. 8, pp. 5455–5516, 2020. \\n[18] \\nI. Laptev and T. Lindeberg, “Space-time interest points,” in \\nProceedings of the IEEE International Conference on Computer \\nVision, 2003, vol. 1, pp. 432–439. \\n[19] \\nH. Wang and C. Schmid, “Action recognition with improved \\ntrajectories,” Proc. IEEE Int. Conf. Comput. Vis., pp. 3551–3558, \\n2013. \\n[20] \\nP. Scovanner, S. Ali, and M. Shah, “A 3-dimensional sift descriptor \\nand its application to action recognition,” in Proceedings of the ACM \\nInternational Multimedia Conference and Exhibition, 2007, pp. 357–\\n360. \\n[21] \\nA. Kläser, M. Marszałek, and C. Schmid, “A spatio-temporal \\ndescriptor based on 3D-gradients,” BMVC 2008 - Proc. Br. Mach. \\nVis. Conf. 2008, 2008. \\n[22] \\nN. Dalal, B. Triggs, and C. Schmid, “Human detection using oriented \\nhistograms of flow and appearance,” Lect. Notes Comput. Sci. \\n(including \\nSubser. \\nLect. \\nNotes \\nArtif. \\nIntell. \\nLect. \\nNotes \\nBioinformatics), vol. 3952 LNCS, pp. 428–441, 2006. \\n[23] \\nS. Sadanand and J. J. Corso, “Action bank: A high-level \\nrepresentation of activity in video,” Proc. IEEE Comput. Soc. Conf. \\nComput. Vis. Pattern Recognit., pp. 1234–1241, 2012. \\n[24] \\nP. Dollár, V. Rabaud, G. Cottrell, and S. Belongie, “Behavior \\nrecognition via sparse spatio-temporal features,” in Proceedings - \\n2nd Joint IEEE International Workshop on Visual Surveillance and \\nPerformance Evaluation of Tracking and Surveillance, VS-PETS, \\n2005, vol. 2005, pp. 65–72. \\n[25] \\nG. Willems, T. Tuytelaars, and L. Van Gool, “An efficient dense and \\nscale-invariant spatio-temporal interest point detector,” Lect. Notes \\nComput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes \\nBioinformatics), vol. 5303 LNCS, no. PART 2, pp. 650–663, 2008. \\n[26] \\nL. Wang, Y. Qiao, and X. Tang, “Video action detection with \\nrelational dynamic-poselets,” in Lecture Notes in Computer Science \\n(including subseries Lecture Notes in Artificial Intelligence and \\nLecture Notes in Bioinformatics), 2014, vol. 8693 LNCS, no. PART \\n5, pp. 565–580. \\n[27] \\nL. Wang, Y. Qiao, and X. Tang, “Action recognition with trajectory-\\npooled deep-convolutional descriptors,” in Proceedings of the IEEE \\nComputer Society Conference on Computer Vision and Pattern \\nRecognition, 2015, vol. 07-12-June, pp. 4305–4314. \\n[28] \\nA. Kar, N. Rai, K. Sikka, and G. Sharma, “AdaScan: Adaptive scan \\npooling in deep convolutional neural networks for human action \\nrecognition in videos,” in Proceedings - 30th IEEE Conference on \\nComputer Vision and Pattern Recognition, CVPR 2017, 2017, vol. \\n2017-Janua, pp. 5699–5708. \\n[29] \\nC. Feichtenhofer, A. Pinz, and R. P. Wildes, “Spatiotemporal \\nmultiplier networks for video action recognition,” in Proceedings - \\n30th IEEE Conference on Computer Vision and Pattern Recognition, \\nCVPR 2017, 2017, vol. 2017-Janua, pp. 7445–7454. \\n[30] \\nZ. Qiu, T. Yao, and T. Mei, “Learning spatio-temporal representation'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 9}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n10 \\nwith pseudo-3D residual networks,” in proceedings of the IEEE \\nInternational Conference on Computer Vision, 2017, pp. 5533–5541. \\n[31] \\nL. Wang et al., “Temporal segment networks: Towards good \\npractices for deep action recognition,” in Lecture Notes in Computer \\nScience (including subseries Lecture Notes in Artificial Intelligence \\nand Lecture Notes in Bioinformatics), 2016, vol. 9912 LNCS, pp. 20–\\n36. \\n[32] \\nY. Wang, M. Long, J. Wang, and P. S. Yu, “Spatiotemporal pyramid \\nnetwork for video action recognition,” in Proc. IEEE Conf. Comput. \\nVis. Pattern Recognit., 2017, pp. 2097–2106. \\n[33] \\nZ. Lan, Y. Zhu, A. G. Hauptmann, and S. Newsam, “Deep Local \\nVideo Feature for Action Recognition,” in IEEE Computer Society \\nConference on Computer Vision and Pattern Recognition \\nWorkshops, 2017, vol. 2017-July, pp. 1219–1225. \\n[34] \\nI. C. Duta, B. Ionescu, K. Aizawa, and N. Sebe, “Spatio-temporal \\nvector of locally max pooled features for action recognition in \\nvideos,” in Proceedings - 30th IEEE Conference on Computer Vision \\nand Pattern Recognition, CVPR 2017, 2017, vol. 2017-Janua, pp. \\n3205–3214. \\n[35] \\nJ. Shen, Y. Huang, M. Wen, and C. Zhang, “Toward an Efficient \\nDeep Pipelined Template-Based Architecture for Accelerating the \\nEntire 2-D and 3-D CNNs on FPGA,” IEEE Trans. Comput. Des. \\nIntegr. Circuits Syst., vol. 39, no. 7, pp. 1442–1455, 2020. \\n[36] \\nI. C. Duta, T. A. Nguyen, K. Aizawa, B. Ionescu, and N. Sebe, \\n“Boosting VLAD with double assignment using deep features for \\naction recognition in videos,” Proc. - Int. Conf. Pattern Recognit., \\nvol. 0, pp. 2210–2215, 2016. \\n[37] \\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet \\nclassification with deep convolutional neural networks,” Adv. Neural \\nInf. Process. Syst., vol. 25, pp. 1097–1105, 2012. \\n[38] \\nZ. Xu, Y. Yang, and A. G. Hauptmann, “A discriminative CNN video \\nrepresentation for event detection,” Proc. IEEE Comput. Soc. Conf. \\nComput. Vis. Pattern Recognit., vol. 07-12-June, pp. 1798–1807, \\n2015. \\n[39] \\nR. Girdhar, D. Ramanan, A. Gupta, J. Sivic, and B. Russell, \\n“ActionVLAD: Learning spatio-temporal aggregation for action \\nclassification,” in Proceedings of the IEEE Conference on Computer \\nVision and Pattern Recognition, 2017, pp. 971–980. \\n[40] \\nN. Ballas, L. Yao, C. Pal, and A. Courville, “Delving deeper into \\nconvolutional networks for learning video representations,” 4th Int. \\nConf. Learn. Represent. ICLR 2016 - Conf. Track Proc., 2016. \\n[41] \\nJ. Donahue et al., “Long-term recurrent convolutional networks for \\nvisual recognition and description,” Proc. IEEE Comput. Soc. Conf. \\nComput. Vis. Pattern Recognit., vol. 07-12-June, pp. 2625–2634, \\n2015. \\n[42] \\nN. Srivastava, E. Mansimov, and R. Salakhutdinov, “Unsupervised \\nlearning of video representations using LSTMs,” 32nd Int. Conf. \\nMach. Learn. ICML 2015, vol. 1, pp. 843–852, 2015. \\n[43] \\nJ. Y. H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. \\nMonga, and G. Toderici, “Beyond short snippets: Deep networks for \\nvideo classification,” Proc. IEEE Comput. Soc. Conf. Comput. Vis. \\nPattern Recognit., vol. 07-12-June, pp. 4694–4702, 2015. \\n[44] \\nG. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler, “Convolutional \\nlearning of spatio-temporal features,” Lect. Notes Comput. Sci. \\n(including \\nSubser. \\nLect. \\nNotes \\nArtif. \\nIntell. \\nLect. \\nNotes \\nBioinformatics), vol. 6316 LNCS, no. PART 6, pp. 140–153, 2010. \\n[45] \\nQ. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng, “Learning \\nhierarchical invariant spatio-temporal features for action recognition \\nwith independent subspace analysis,” Proc. IEEE Comput. Soc. Conf. \\nComput. Vis. Pattern Recognit., pp. 3361–3368, 2011. \\n[46] \\nM. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt, \\n“Sequential deep learning for human action recognition,” Lect. Notes \\nComput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes \\nBioinformatics), vol. 7065 LNCS, pp. 29–39, 2011. \\n[47] \\nS. Ji, W. Xu, M. Yang, and K. Yu, “3D Convolutional neural \\nnetworks for human action recognition,” IEEE Trans. Pattern Anal. \\nMach. Intell., vol. 35, no. 1, pp. 221–231, 2013. \\n[48] \\nS. Zha, F. Luisier, W. Andrews, N. Srivastava, and R. Salakhutdinov, \\n“Exploiting Image-trained CNN Architectures for Unconstrained \\nVideo Classification,” in BMVC, 2015, pp. 60.1-60.13. \\n[49] \\nJ. Carreira and A. Zisserman, “Quo Vadis, action recognition? A new \\nmodel and the kinetics dataset,” Proc. - 30th IEEE Conf. Comput. Vis. \\nPattern Recognition, CVPR 2017, vol. 2017-Janua, pp. 4724–4733, \\n2017. \\n[50] \\nA. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and F. \\nF. Li, “Large-scale video classification with convolutional neural \\nnetworks,” in Proceedings of the IEEE Computer Society Conference \\non Computer Vision and Pattern Recognition, 2014, pp. 1725–1732. \\n[51] \\nD. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, \\n“Learning spatiotemporal features with 3D convolutional networks,” \\nin Proceedings of the IEEE International Conference on Computer \\nVision, 2015, vol. 2015 Inter, pp. 4489–4497. \\n[52] \\nM. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt, \\n“Sequential deep learning for human action recognition,” in Lecture \\nNotes in Computer Science (including subseries Lecture Notes in \\nArtificial Intelligence and Lecture Notes in Bioinformatics), 2011, \\nvol. 7065 LNCS, pp. 29–39. \\n[53] \\nK. Simonyan and A. Zisserman, “Two-stream convolutional \\nnetworks for action recognition in videos,” in Advances in Neural \\nInformation Processing Systems, 2014, vol. 1, no. January, pp. 568–\\n576. \\n[54] \\nC. Feichtenhofer, A. Pinz, and A. Zisserman, “Convolutional Two-\\nStream Network Fusion for Video Action Recognition,” in \\nProceedings of the IEEE Computer Society Conference on Computer \\nVision and Pattern Recognition, 2016, vol. 2016-Decem, pp. 1933–\\n1941. \\n[55] \\nZ. Wu, Y.-G. Jiang, X. Wang, H. Ye, X. Xue, and J. Wang, “Fusing \\nMulti-Stream Deep Networks for Video Classification,” in CoRR, \\n2015. \\n[56] \\nD. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, “A \\ncloser look at spatiotemporal convolutions for action recognition,” in \\nIn Proceedings of the IEEE conference on Computer Vision and \\nPattern Recognition, 2018, pp. 6450–6459. \\n[57] \\nK. Shan, Y. Wang, Z. Tang, Y. Chen, and Y. Li, “MixTConv: Mixed \\nTemporal Convolutional Kernels for Efficient Action Recognition,” \\nin 25th International Conference on Pattern Recognition (ICPR), \\n2021, pp. 1751–1756. \\n[58] \\nZ. Wu, X. Wang, Y. G. Jiang, H. Ye, and X. Xue, “Modeling spatial-\\nTemporal clues in a hybrid deep learning framework for video \\nclassification,” in MM 2015 - Proceedings of the 2015 ACM \\nMultimedia Conference, 2015, pp. 461–470. \\n[59] \\nS. Tanberk, Z. H. Kilimci, D. B. Tukel, M. Uysal, and S. Akyokus, \\n“A Hybrid Deep Model Using Deep Learning and Dense Optical \\nFlow Approaches for Human Activity Recognition,” IEEE Access, \\nvol. 8, pp. 19799–19809, 2020. \\n[60] \\nT. Alhersh, H. Stuckenschmidt, A. U. Rehman, and S. B. Belhaouari, \\n“Learning Human Activity From Visual Data Using Deep Learning,” \\nIEEE Access, vol. 9, pp. 106245–106253, 2021. \\n[61] \\nW. Cao, Z. Yan, Z. He, and Z. He, “A Comprehensive Survey on \\nGeometric Deep Learning,” IEEE Access, vol. 8, pp. 35929–35949, \\n2020. \\n[62] \\nJ. Masci, D. Boscaini, M. M. Bronstein, and P. Vandergheynst, \\n“Geodesic Convolutional \\nNeural Networks on \\nRiemannian \\nManifolds,” Proc. IEEE Int. Conf. Comput. Vis., vol. 2015-Febru, pp. \\n832–840, 2015. \\n[63] \\nD. Boscaini, J. Masci, E. Rodolà, and M. Bronstein, “Learning shape \\ncorrespondence with anisotropic convolutional neural networks,” \\nAdv. Neural Inf. Process. Syst., pp. 3197–3205, 2016. \\n[64] \\nF. Monti, D. Boscaini, J. Masci, E. Rodolà, J. Svoboda, and M. M. \\nBronstein, “Geometric deep learning on graphs and manifolds using'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 10}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n11 \\nmixture model CNNs,” Proc. - 30th IEEE Conf. Comput. Vis. Pattern \\nRecognition, CVPR 2017, vol. 2017-Janua, pp. 5425–5434, 2017. \\n[65] \\nO. Litany, T. Remez, E. Rodola, A. Bronstein, and M. Bronstein, \\n“Deep Functional Maps: Structured Prediction for Dense Shape \\nCorrespondence,” Proc. IEEE Int. Conf. Comput. Vis., vol. 2017-\\nOctob, pp. 5660–5668, 2017. \\n[66] \\nD. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castellani, and \\nP. \\nVandergheynst, \\n“Learning \\nclass-specific \\ndescriptors \\nfor \\ndeformable shapes using localized spectral convolutional networks,” \\nEurographics Symp. Geom. Process., vol. 34, no. 5, pp. 13–23, 2015. \\n[67] \\nC. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning \\non point sets for 3D classification and segmentation,” Proc. - 30th \\nIEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017, vol. \\n2017-Janua, pp. 77–85, 2017. \\n[68] \\nC. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deep \\nhierarchical feature learning on point sets in a metric space,” Adv. \\nNeural Inf. Process. Syst., vol. 2017-Decem, pp. 5100–5109, 2017. \\n[69] \\nY. Li and W. Cao, “An Extended Multilayer Perceptron Model Using \\nReduced Geometric Algebra,” IEEE Access, vol. 7, pp. 129815–\\n129823, 2019. \\n[70] \\nZ. Li, R. Li, and G. Jin, “Sentiment analysis of danmaku videos based \\non naïve bayes and sentiment dictionary,” IEEE Access, vol. 8, pp. \\n75073–75084, 2020. \\n[71] \\nM. Zhen et al., “Learning Discriminative Feature with CRF for \\nUnsupervised Video Object Segmentation,” Lect. Notes Comput. Sci. \\n(including \\nSubser. \\nLect. \\nNotes \\nArtif. \\nIntell. \\nLect. \\nNotes \\nBioinformatics), vol. 12372 LNCS, pp. 445–462, 2020. \\n[72] \\nG. A. Ruz, P. A. Henríquez, and A. Mascareño, “Sentiment analysis \\nof Twitter data during critical events through Bayesian networks \\nclassifiers,” Futur. Gener. Comput. Syst., vol. 106, pp. 92–104, 2020. \\n[73] \\nR. Fantinel, A. Cenedese, and G. Fadel, “Hybrid Learning Driven by \\nDynamic Descriptors for Video Classification of Reflective \\nSurfaces,” IEEE Trans. Ind. Informatics, 2021. \\n[74] \\nF. F. Costa, P. T. M. Saito, and P. H. Bugatti, “Video action \\nclassification through graph convolutional networks,” VISIGRAPP \\n2021 - Proc. 16th Int. Jt. Conf. Comput. Vision, Imaging Comput. \\nGraph. Theory Appl., vol. 4, pp. 490–497, 2021. \\n[75] \\nQ. Xu, L. Zhu, T. Dai, and C. Yan, “Aspect-based sentiment \\nclassification with multi-attention network,” Neurocomputing, vol. \\n388, pp. 135–143, 2020. \\n[76] \\nM. Bibi, W. Aziz, M. Almaraashi, I. H. Khan, M. S. A. Nadeem, and \\nN. Habib, “A Cooperative Binary-Clustering Framework Based on \\nMajority Voting for Twitter Sentiment Analysis,” IEEE Access, vol. \\n8, pp. 68580–68592, 2020. \\n[77] \\nK. Sailunaz and R. Alhajj, “Emotion and sentiment analysis from \\nTwitter text,” J. Comput. Sci., vol. 36, 2019. \\n[78] \\nL. Wang, W. Li, W. Li, and L. Van Gool, “Appearance-and-relation \\nnetworks for video classification,” Proc. IEEE Conf. Comput. Vis. \\npattern Recognit., pp. 1430–1439, 2018. \\n[79] \\nL. Jing, T. Parag, Z. Wu, Y. Tian, and H. Wang, “VideoSSL: Semi-\\nSupervised Learning for Video Classification,” in Proceedings of the \\nIEEE/CVF Winter Conference on Applications of Computer Vision, \\n2021, pp. 1110–1119. \\n[80] \\nK. Soomro, A. R. Zamir, and M. Shah, “UCF101: A Dataset of 101 \\nHuman Actions Classes From Videos in The Wild,” CRCV-TR-12-\\n01, 2012. \\n[81] \\nZ. Cai, L. Wang, X. Peng, and Y. Qiao, “Multi-view super vector for \\naction recognition,” in Proceedings of the IEEE Computer Society \\nConference on Computer Vision and Pattern Recognition, 2014, pp. \\n596–603. \\n[82] \\nL. Sun, K. Jia, D. Y. Yeung, and B. E. Shi, “Human action recognition \\nusing \\nfactorized \\nspatio-temporal \\nconvolutional \\nnetworks,” \\nProceedings of the IEEE International Conference on Computer \\nVision, vol. 2015 Inter. pp. 4597–4605, 2015. \\n[83] \\nD. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “C3D\\u2009: \\nGeneric Features for Video Analysis C3D,” in ICCV, 2015. \\n[84] \\nX. Peng, L. Wang, X. Wang, and Y. Qiao, “Bag of visual words and \\nfusion methods for action recognition: Comprehensive study and \\ngood practice,” in Computer Vision and Image Understanding, 2016, \\nvol. 150, pp. 109–125. \\n[85] \\nG. Lev, G. Sadeh, B. Klein, and L. Wolf, “RNN fisher vectors for \\naction recognition and image annotation,” in Lecture Notes in \\nComputer Science (including subseries Lecture Notes in Artificial \\nIntelligence and Lecture Notes in Bioinformatics), 2016, vol. 9910 \\nLNCS, pp. 833–850. \\n[86] \\nE. Park, X. Han, T. L. Berg, and A. C. Berg, “Combining multiple \\nsources of knowledge in deep CNNs for action recognition,” in 2016 \\nIEEE Winter Conference on Applications of Computer Vision, WACV \\n2016, 2016. \\n[87] \\nX. Yang, P. Molchanov, and J. Kautz, “Multilayer and multimodal \\nfusion of deep neural networks for video classification,” MM 2016 - \\nProc. 2016 ACM Multimed. Conf., pp. 978–987, 2016. \\n[88] \\nX. Wang, A. Farhadi, and A. Gupta, “Actions ~ Transformations,” in \\nProceedings of the IEEE Computer Society Conference on Computer \\nVision and Pattern Recognition, 2016, vol. 2016-Decem, pp. 2658–\\n2667. \\n[89] \\nZ. Wu, Y. G. Jiang, X. Wang, H. Ye, and X. Xue, “Multi-stream \\nmulti-class fusion of deep networks for video classification,” MM \\n2016 - Proc. 2016 ACM Multimed. Conf., pp. 791–800, 2016. \\n[90] \\nW. Zhu, J. Hu, G. Sun, X. Cao, and Y. Qiao, “A Key Volume Mining \\nDeep Framework for Action Recognition,” in Proceedings of the \\nIEEE Computer Society Conference on Computer Vision and Pattern \\nRecognition, 2016, vol. 2016-Decem, pp. 1991–1999. \\n[91] \\nA. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and F. \\nF. Li, “Large-scale video classification with convolutional neural \\nnetworks,” Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern \\nRecognit., pp. 1725–1732, 2014. \\n[92] \\nZ. Wu, Y. G. Jiang, J. Wang, J. Pu, and X. Xue, “Exploring inter-\\nfeature and inter-class relationships with deep neural networks for \\nvideo classification,” MM 2014 - Proc. 2014 ACM Conf. Multimed., \\npp. 167–176, 2014. \\n[93] \\nY. Yang, D. Krompass, and V. Tresp, “Tensor-train recurrent neural \\nnetworks for video classification,” 34th Int. Conf. Mach. Learn. \\nICML 2017, vol. 8, pp. 5929–5938, 2017. \\n[94] \\nD. Tran, H. Wang, L. Torresani, and M. Feiszli, “Video classification \\nwith channel-separated convolutional networks,” in In Proceedings \\nof the IEEE/CVF International Conference on Computer Vision, \\n2019, pp. 5552–5561. \\nAtiq Ur Rehman (S’17–M’19) received \\nthe bachelor’s degree in computer \\nengineering \\n(with \\ndistinction) \\nfrom \\nCOMSATS \\nUniversity \\nIslamabad, \\nPakistan, in 2010. He received the \\nmaster’s degree in computer engineering \\nfrom National University of Sciences and \\nTechnology (NUST), Pakistan, in 2013 and PhD degree in \\nComputer Science and Engineering from Hamad Bin \\nKhalifa University, Qatar in 2019. He is currently working \\nas a Post doc researcher with the College of Science and \\nEngineering, Hamad Bin Khalifa University, Qatar. His \\nresearch interests include the development of evolutionary \\ncomputation, pattern recognition and machine learning \\nalgorithms.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.7 (Build 19H1323) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20210816102413Z00'00'\", 'source': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'file_path': '../data/pdf_files/07.Deep Learning for Video Classification: A  Review.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': 'Microsoft Word - TAI.docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20210816102413Z00'00'\", 'trapped': '', 'modDate': \"D:20210816102413Z00'00'\", 'creationDate': \"D:20210816102413Z00'00'\", 'page': 11}, page_content='> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n \\n12 \\nSamir Brahim Belhaouri (SM’19) received \\nthe master’s degree in telecommunications \\nfrom the National Polytechnic Institute \\n(ENSEEIHT) of Toulouse, France, in 2000, \\nand the Ph.D. degree in Applied Mathematics \\nfrom the Federal Polytechnic School of \\nLausanne (EPFL), in 2006. He is currently an \\nassociate professor in the Division of Information and \\nComputing Technology, College of Science and Engineering, \\nHBKU. He also holds and leads several academic and \\nadministrator positions, Vice Dean for Academic & Student \\nAffairs at College of Science and General Studies and \\nUniversity Preparatory Program at ALFAISAL university \\n(KSA), University of Sharjah (UAE), Innopolis University \\n(Russia), Petronas University (Malaysia), and EPFL Federal \\nSwiss school (Switzerland). His main research interests include \\nStochastic Processes, Machine Learning, and Number Theory. \\nHe is now working actively on developing algorithms in \\nmachine learning applied to visual surveillance, sensing \\ntechnologies and biomedical data, with the support of several \\ninternational fund for research in Russia, Malaysia, and in \\nGCC.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 0}, page_content='sensors\\nArticle\\nScene Classiﬁcation for Sports Video Summarization\\nUsing Transfer Learning\\nMuhammad Raﬁq 1\\n, Ghazala Raﬁq 1\\n, Rockson Agyeman 1\\n, Gyu Sang Choi 1,*\\nand\\nSeong-Il Jin 2,*\\n1\\nDepartment of Information and Communication Engineering, Yeungnam University, Gyeongsan-si 38541,\\nKorea; raﬁq@ynu.ac.kr (M.R.); ghazala@ynu.ac.kr (G.R.); rocksyne@gmail.com (R.A.)\\n2\\nThe division of computer convergence, Chungnam National University, Daejeon 34134, Korea\\n*\\nCorrespondence: castchoi@ynu.ac.kr (G.S.C.); sijin@cnu.ac.kr (S.-I.J.)\\nReceived: 5 February 2020; Accepted: 15 March 2020; Published: 18 March 2020\\n\\x01\\x02\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\nAbstract: This paper proposes a novel method for sports video scene classiﬁcation with the particular\\nintention of video summarization. Creating and publishing a shorter version of the video is more\\ninteresting than a full version due to instant entertainment. Generating shorter summaries of the\\nvideos is a tedious task that requires signiﬁcant labor hours and unnecessary machine occupation.\\nDue to the growing demand for video summarization in marketing, advertising agencies, awareness\\nvideos, documentaries, and other interest groups, researchers are continuously proposing automation\\nframeworks and novel schemes. Since the scene classiﬁcation is a fundamental component of video\\nsummarization and video analysis, the quality of scene classiﬁcation is particularly important. This\\narticle focuses on various practical implementation gaps over the existing techniques and presents\\na method to achieve high-quality of scene classiﬁcation. We consider cricket as a case study and\\nclassify ﬁve scene categories, i.e., batting, bowling, boundary, crowd and close-up. We employ our\\nmodel using pre-trained AlexNet Convolutional Neural Network (CNN) for scene classiﬁcation.\\nThe proposed method employs new, fully connected layers in an encoder fashion. We employ\\ndata augmentation to achieve a high accuracy of 99.26% over a smaller dataset. We conduct a\\nperformance comparison against baseline approaches to prove the superiority of the method as\\nwell as state-of-the-art models. We evaluate our performance results on cricket videos and compare\\nvarious deep-learning models, i.e., Inception V3, Visual Geometry Group (VGGNet16, VGGNet19) ,\\nResidual Network (ResNet50), and AlexNet. Our experiments demonstrate that our method with\\nAlexNet CNN produces better results than existing proposals.\\nKeywords: deep learning; AlexNet CNN; small dataset; data augmentation\\n1. Introduction\\nThe rapid growth of devices and widespread internet connectivity and automation has generated\\ndata drastically like never before. In addition to textual data, video streaming contents are the main\\ncontributor in data generation, which demands a larger space and bandwidths to store and stream.\\nThe web pages with video content are more engaging and seem informative than that of only showing\\nthe text. Video processing, monitoring, summarizing, and broadcasting are snowballing with the\\nintroduction of live broadcast apps and the provision of live streaming on existing major video\\nhosting websites. Therefore, the video processing is gaining the attention of the computer vision and\\ndeep-learning researchers. Video content searching, video description, and video summarization are\\nthe hot topics of the times. They are high-level tasks in the ﬁeld of video processing. On the other\\nhand, shot classiﬁcation, scene classiﬁcation, shot-boundary detection, and action recognition are the\\nbuilding blocks for those high-level tasks.\\nSensors 2020, 20, 1702; doi:10.3390/s20061702\\nwww.mdpi.com/journal/sensors'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 1}, page_content='Sensors 2020, 20, 1702\\n2 of 18\\nSports videos are the most engaging and contributive content and have enormous commercial\\naspects. Sports video scenes are usually repetitive; because every sport has its predeﬁned camera\\npositions, zooming protocols, and action replay strategies. Sports videos are usually long in duration;\\ntherefore, it is not always possible for the ofﬂine audience to watch full contents of the recorded video\\nfor a speciﬁc event. Due to the longer duration of the recorded video and busier life of the audience,\\nresearchers are actively working on automatic video summarization, and scene classiﬁcation is the\\ncore component for the sports video summarization. Since sports videos have known rules for video\\ncapturing and have a particular scene protocol, a sport-speciﬁc scene classiﬁer can produce better\\nresults in comparison to generalized scene classiﬁer. Sports video covers all the events, including\\nexciting as well as boring events, and some of the events are less exciting, i.e., breaks, tedious parts of\\nthe game, and other activities where a viewer loses interest. It produces the opportunity to publish\\nthe videos consisting of only the best parts of the game. Therefore, accurate and high quality of scene\\nclassiﬁcation can help to produce a high-quality video summary.\\nThe objective of our proposed method is to present a high-performance method to achieve\\nhigh-quality summarization. The lower error rate in classiﬁcation ensures the high reliability of the\\nclassiﬁcation and hence high reliability of the dependent processes that consume scene classiﬁcation\\nbuilding block. We achieve this objective by introducing a three-step upgrade to the state-of-the-art\\nmethod proposed by [1], i.e., proposed a particular transfer learning architecture, introducing the\\npre-trained weights for training the model and applying data augmentation for training data. The\\narchitecture of the proposed model for scene classiﬁcation is shown in Figure 1. Our proposed model\\nfocuses on cricket as a case study, and we carry out our experiments on recorded videos taken from\\npublicly available YouTube source.\\nFigure 1. The proposed model architecture. Input video is extracted into frames and key frames are\\nclassiﬁed by a trained CNN. Output class identiﬁes the scene and is compared if continuous video.\\nThe sports audience is more interested in meaningful content, and analyzing and summarizing\\nthe video content is a substantial labor task. Computers are helping humans more promising than\\never before, and particularly with the evolution of machine learning and other methods of artiﬁcial\\nintelligence (AI) in almost every domain of the life sciences. Therefore, researchers are proposing novel\\ntechniques almost all the time. Since scene classiﬁcation if a fundamental building block, it requires\\nsigniﬁcant attention to improve the performance over the existing research state-of-the-art level.\\nPreviously, several researchers proposed generalized schemes for shot classiﬁcation, shot-boundary\\ndetection, and scene classiﬁcation; however, generalized proposals are less helpful for a speciﬁc\\ndomain. Proposing a specialized technique has its signiﬁcance and achieves superiority over the\\nexisting generalized proposals.\\nMany researchers proposed excellent methods recently with signiﬁcant improvements.\\nMinhas et al. [1] proposed a method based on deep learning techniques and presented a bunch'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 2}, page_content='Sensors 2020, 20, 1702\\n3 of 18\\nof improvements. However, a quality gap is still present, and a requirement exists to fulﬁll this gap in\\naddition to practical implementation limitations. Due to the non-availability of labeled sports datasets\\nin this domain and context, improvement is necessary to produce high-quality classiﬁcation on a\\nsmaller dataset in practically less time.\\nIn this paper, we propose a model for sports video scene classiﬁcation with the particular intention\\nof sports video summarization. The proposed method contributes a combination of three techniques\\nover the existing state-of-the-art technique [1]: the proposed architecture for the classiﬁcation model,\\nincorporating the pre-trained weights and training data augmentation. In the proposed architecture,\\nwe present a set of new layers for the transfer learning of AlexNet CNN. The proposed architecture is\\ndesigned to support ﬁve critical scene classes required in the sport of cricket. We introduced additional\\nfully connected layers with dropout and activation functions. We prepare the cricket sports dataset\\nfrom YouTube cricket sports recorded videos from the past events. We label the ﬁve most frequent and\\nessential scenes from the videos, i.e., batting, bowling, boundary, close-up, and the crowd, to evaluate\\nit to be the best ﬁt for video summarization of cricket sport. Figure 2 shows the training procedure. We\\nload the model with pre-trained weights on ImageNet challenge with 1000 classes and then conﬁned\\nthe feature map to our ﬁve classes by using the additional proposed layers. We split the training\\nimage data into the thee subsets for training, validation, and test. We augment the training data using\\ngeometric transformations. We employ the in-place data augmentation technique and introduce a\\nlarge image repository using batches of 32 images at a time. In-place transformations do not increase\\nthe size of the training data; hence control over training time is achieved, whereas increasing training\\ndataset size potentially increases the training time for the model. We iterate several learning rates\\nto ﬁnd the best learning rate to achieve peak accuracy. We perform our experiments using the best\\nlearning for the latest deep-learning models on the same dataset.\\nFigure 2. Proposed model training and evaluation block diagram.\\nWe compare the model with the existing state-of-the-art models, particularly deep-learning\\nmodels, e.g., Inception V3, VGGNET16, VGGNET19, ResNet50, and from non-deep-learning models\\nlike SVM to present a performance evaluation. We also compare the model with existing models in the\\nﬁeld of sports video summarization and scene classiﬁcation. We focus on scene classiﬁcation with a\\nprimary intention of video summarization using deep-learning methods, and we have selected the\\nsport of cricket as a case study.\\nOur proposed methodology demonstrates a 5% better performance over the existing method\\nproposed by [1] and supersedes the accuracy, precision, and recall with a signiﬁcant margin of 4–5% in\\nall parameters. We achieve a mean accuracy of 99.26% with a stable precision of 99.27% and a reliable\\nrecall of 99.26%, which yields the F1-measure of 99.26% to show the high quality of the classiﬁcation.\\nFurthermore, since we incorporate pre-trained CNN in the proposed method, consequently, it reduces\\nthe training time of the model drastically. On the other hand, pre-processing of the training data by'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 3}, page_content='Sensors 2020, 20, 1702\\n4 of 18\\nintroducing augmentation, eliminates the overﬁtting, and helps to achieve the high quality of the\\nmodel state. We introduce three fully connected layers appended to pre-trained CNN and applied the\\nbest learning rate during our extensive experiments. Our methodology shows superior performance\\nover existing methods during our experiments.\\nThe primary audience of the proposed method is the video summarization and video description\\nresearcher community.\\nHowever, classiﬁcation, in general, can also be employed in several\\nother domains, i.e., medical image analysis [2,3], agriculture classiﬁcation[4], biological organism\\nclassiﬁcation, self-driving vehicles[5], video captioning and other similar domains.\\nThe rest of the paper is organized into the following four sections, Section 2 covers the related\\nwork in the subject domain. The proposed method is presented in Section 3. The experimental results\\nand discussion is detailed in the Section 4 and ﬁnally the Section 5 conclude the work and suggest the\\nfuture aspects.\\n2. Related Work\\nExisting approaches presented signiﬁcant work on sports videos. Mainly shot classiﬁcation and\\nscene classiﬁcation methods classify the camera views in the ﬁeld, i.e., audience, players, and state\\nof the sports ﬁeld views specialized to each sport. Recent approaches employed various methods\\nand models, including conventional classiﬁcation models, i.e., Kth Nearest Neighbor (KNN), Support\\nVector Machines (SVM), as well as deep-learning models, i.e., AlexNet CNN, Recurrent Neural\\nNetwork (RNN), Long Short-Term Memory (LSTM), Visual Geometry Group Network (VGGNET)\\nand Gated Recurrent Units (GRUs). Approaches involving deep structures ensure an improved\\naccuracy but at the cost of increased computational complexity. The need for a dedicated dataset for\\nlarge-scale multi-label video classiﬁcation is being addressed by Abu-El-Haija et al. [6], with over eight\\nmillion videos summing to 500,000 h of playtime and 4800 visual attributes. The videos were labeled\\nusing vide annotation system from YouTubeTM. They classiﬁed videos into vehicles, sports, and\\nconcerts. We will review both classical and deep-learning approaches applied for scene classiﬁcation in\\nsports videos.\\n2.1. Scene Classiﬁcation via Conventional Approach\\nRahul et al. [7] presented a soccer video frame classiﬁcation using multi-class kernel SVM.\\nA window-based shot-detection scheme is adapted. For a decided window size, the difference\\nof a speciﬁc frame is calculated with every other frame. Depending upon a particular threshold,\\nthat speciﬁc frame is declared to be a shot-boundary. The shot can be classiﬁed as one of Bowler\\nRunup, Batsman Stroke, Player Close-Up, Umpire, Ground, Crowd, Animations, and Miscellaneous.\\nTemporal neighbor-hood information is also catered while performing the classiﬁcation. Dataset\\nused is collected from YouTube regarding the Indian Premier League (IPL) tournament consisting\\nof 16 matches with a total of 64 h of playtime. Zhou L. [8] proposed a moving sports video moving\\nobject detection and tracking using the hidden Markov model (HMM) to track the players in the\\nground using a top-view camera for the tracking of moving players. The integration of the hidden\\nMarkov model in physical education can reform physical education for students. The applied method\\nmaintains incredible accuracy and stability throughout the tracking process. Ling-Yu et al. [9]\\nproposed a uniﬁed framework for semantic scene classiﬁcation in sports videos. The proposed\\nmethod implements supervised learning for top-down video scene classiﬁcation. Low-level features\\nwere derived using color, texture, and DC images in the compressed domain. These low-level\\nfeatures were converted to mid-level features such as camera motion, action regions, and ﬁeld\\nshape properties. Mid-level representation was necessary to use the classiﬁers that do not map\\non low-level features. Finally, shots were classiﬁed using different supervised learning algorithms\\nlike decision trees, neural networks, support vectors machine (SVM). Camera shots were classiﬁed\\ninto mid shots, long shots, and close-up shots. The proposed framework achieved a classiﬁcation\\naccuracy of 85% to 95% on ﬁve different sports, i.e., volleyball, basketball, tennis, soccer, and table'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 4}, page_content='Sensors 2020, 20, 1702\\n5 of 18\\ntennis. Ashok et al. [10], proposed an optical ﬂow approach for different sorts of batsman stroke\\ndetection in cricket sports videos. A histogram-based technique is being adopted to detect the\\nshot-boundary. Low-level features (i.e., grass pixel ratio) and mid-level features (i.e., camera view,\\ndistance, etc.) were used to train a hybrid classiﬁer to classify seven classes, i.e., ground view,\\nbatsman view, crowd view, long shot, pitch view, ﬁelder view, and others. The proposed approach\\ncomprises Naïve bias, KNN, and multi-class SVM for shot classiﬁcation into ﬁeld-view and non-ﬁeld\\nviews. The method used 29,000 frames from cricket match video played between England and\\nAustralia for training and 14,000 frames for testing purposes. The authors claimed for 90.9% of\\nrecall with 50% precision. Overall accuracy following the optical ﬂow mechanism is around 80%.\\nSigari et al.\\n[11] exploited the heuristic/ rule-based technique as well as employed SVM for\\ncounter-attack detection based on camera motion estimation.\\nThe ﬁrst step in the process of\\ncounter-attack detection detects the shot boundaries and makes segments of the video to the detected\\nshots. An SVM-based classiﬁer with RBF kernel produced the best results.\\n2.2. Scene Classiﬁcation via Deep-Learning Approach\\nMinhas et al. [1] proposed a deep-learning-based shot classiﬁcation model and presented good\\naccuracy over different shot classes consisting of long, medium, close-ups, and crowd/out-of-ﬁeld\\nshots. A well-known deep-learning model, AlexNet CNN is used to classify shots or frames extracted\\nfrom the sports videos and employed four classes to train the network from scratch on Nvidia GTX\\n580 Graphic Processing Units (GPU). Overall training and validation performance are boosted using\\nresponse normalization and dropout layers. The adopted methodology demonstrated good accuracy\\nof 94.07% against baseline methods on the subjected dataset. Russo et al. [12] proposed a model by\\ncombining deep learning and transfer learning, i.e., combining the functionality of VGGNET16, RNN,\\nand GRU with transfer learning for ﬁnal classiﬁcation. The authors hand-drafted the sports dataset to\\nfocus on sports action-based classiﬁcation. These mixed sports videos were used to classify 10 and 15\\ntarget classes using CNN as feature extractor and then combining with temporal information from\\nRNN for model formulation. The model demonstrated a good accuracy of 94% for 10 and 92% for 15\\nsports classes. Ng et al. [13] is capable of handling full-length videos using the AlexNet and GoogLeNet\\nmodels. Sports-IM consisting of around 1.2M YouTube sports videos with 487 classes and UCF-101\\ncontaining 13,320 videos with 101 action classes are used while evaluating the proposed architecture.\\nVideo frames are augmented for diverse training of the model. For LSTMs, the optical ﬂow mechanism\\nshowed improvement in the results, whereas the fusion of convolutional pooling networks with optical\\nﬂow did not show any improvement. Jungheon et al. [14] proposed a multi-Modality classiﬁcation\\nsystem for video events using visual and audio relationships extracted from the video. Inception-V3\\nalgorithms are employed for image feature vector extraction. However, Mel Frequency Cepstral\\nCoefﬁcients (MFCCs) were used to extract the audio feature vector. Finally, SVM and SoftMax classify\\nvideo events. YLI-MED dataset with 1823 videos and a self-collected YouTube dataset comprising\\nof 1369 videos were used for evaluation of the proposed system. Normalization of audio and visual\\nfeature vectors, as well as correlation integration, achieved better performance results. Hong et al. [15]\\nproposed an end to end Soccer video scene and event classiﬁcation using CNN with Deep transfer\\nlearning and achieved above 89% of accuracy. Separable features extracted using the CNN model\\ncan be visualized in 3D space. Transfer learning was applied to avoid overﬁtting of the model. A\\nnew Soccer Video Scene and Event dataset (SVSED) is introduced, comprising of 600 video clips\\nclassifying six classes. Agyeman et al. [16] proposed sports video summarization by considering\\nspatiotemporal features learning using 3D CNN and LSTM by employing ﬁne-tuning. CNN based on\\nResidual Network (ResNet) is used for feature extraction from video clips. Summarization of 10 soccer\\nvideos evaluated by 48 participants from 8 countries was performed. The self-annotated dataset, as\\nwell as UCF101 dataset comprising of 13,320 video clips with 101 human action classes, is used to\\nevaluate the performance of the system on ﬁve classes, i.e., centerline, corner-kick, free-kick, goal,\\nand throw-in. Video summarized using the proposed system received a good score of 4 out of 5 from'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 5}, page_content='Sensors 2020, 20, 1702\\n6 of 18\\nthe participants. Sozykin K et al. [17] proposed a 3D CNN-based multi-label deep Human Action\\nRecognition (HAR) system for sports video summarization for the sport of Hockey and presented\\nmore than ten classes. Data pre-processing techniques like resizing, normalization, windowing, and\\nsequence labeling were used. The dataset used for evaluation contains 36 grayscale videos recorded\\nwith a static camera position. The authors obtained a low mean F1-score of 67% on the single multi-label\\nk-output model (SMKO). Bhalla et al. [18] proposed a multimodal approach for automatic cricket video\\nsummarization. They employed Optical Character Recognition (OCR), sound detection, and replay\\ndetection techniques for the extraction of essential events in the cricket match. The proposed system\\nachieved an accuracy of 89.45% for event detection. Highlights generated by the system were compared\\nwith the highlights telecasted by the ofﬁcial broadcaster of the match. Tejero-De-Prablos [19] presented\\na scheme for user-generated sports video summarization and employed the LSTM model to identify\\nvarious user actions in the video. Videos were classiﬁed into interesting or non-interesting categories.\\nThe performance of the system compared with several combinations of different features. Evaluation\\nresults outperform existing summarization techniques. Shih [20] covered various content-aware\\nsports video analysis and summarization techniques on a broader scope of games, techniques, issues,\\nbenchmarks, and datasets. Karmaker et al. [21] employed an optical ﬂow tracing method for shot\\nclassiﬁcation of the cricket sport. A trained 3D MACH ﬁlter recognized the actions. A ﬁlter trained\\nover six videos to recognize the actions was employed. Finally, Decision trees and ELMAN neural\\nnetwork was used to classify the shots.\\n3. Proposed Method\\nThis section provides a detailed methodology of the proposed method. The core of the proposed\\nmethod is AlexNet CNN, which is pre-trained on ImageNet [22] weights.\\nFigure 1 shows the\\narchitecture of the proposed model. We employed AlexNet CNN as a foundation with pre-trained\\nweights and employed transfer learning by adding three fully connected layers and trained our model\\non the labeled sports dataset. The proposed method consists of ﬁve steps. In step 1 we prepared\\nthe dataset from publicly available sports videos, then in step 2, we employed AlexNet CNN and\\nadded three additional fully connected layers. First two layers appended with dropout and last layer\\nactivated with SoftMax, as shown in Figure 1, and after the layer conﬁguration, we compiled the model\\nwith extended layers. In step 3, we loaded pre-trained weights to the model. In step 4, we split the\\ndataset for training, validation, and testing, and applied augmentation on training data. Finally, in step\\n5, we trained the model. Figure 2 shows the block diagram of the training and evaluation procedure.\\n3.1. Proposed Model Architecture\\nIn the proposed method, we employed AlexNet CNN deep-learning architecture for scene\\nclassiﬁcation and evaluated using Keras application 2.0 and Tensor Flow backend in Python. Keras\\nprovides a variety of deep-learning application layers. Table 1 enlists the complete layer conﬁguration\\nof the proposed model.\\n3.1.1. Application Layers\\nWe employed Keras layers to construct AlexNet and extended the codebase from the ConvNet\\nlibrary [23]. AlexNet CNN then loaded pre-trained weights from [24]. The proposed layer architecture\\nconsists of Keras ConvNet AlexNet model from layers 1 to 32 and the transfer learning from layers\\n33 to 38. Layer 37 consists of ﬁve neurons to match our classiﬁcation task. The dense layer with ﬁve\\noutput classes conﬁnes the probabilities received from the dropout-5 layer, and then output is activated\\nwith SoftMax activation. Final classiﬁcation based on probabilities obtained from SoftMax activation is\\ndone to ﬁnd the exact association from the output heat-map. The AlexNet CNN model requires input\\nas a 3D tensor with a size 227 × 227 × 3 for color images to be introduced to the model for training and\\nclassiﬁcation.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 6}, page_content='Sensors 2020, 20, 1702\\n7 of 18\\nTable 1. Proposed model architecture (Transfer Learning).\\nS#\\nLayer (Type)\\nOutput\\nParameters\\n1.\\ninput_1 (InputLayer)\\n(None, 3, 227, 227)\\n-\\n2.\\nconv_1 (Conv2D)\\n(None, 96, 55, 55)\\n34,944\\n3.\\nmax_pooling2d_1 (MaxPooling2D)\\n(None, 96, 27, 27)\\n-\\n4.\\nzero_padding2d_1 (ZeroPadding2D)\\n(None, 96, 31, 31)\\n-\\n5.\\nlambda_1 (Lambda)\\n(None, 48, 31, 31)\\n-\\n6.\\nlambda_2 (Lambda)\\n(None, 48, 31, 31)\\n-\\n7.\\nconv_2_1 (Conv2D)\\n(None, 128, 27, 27)\\n153,728\\n8.\\nconv_2_2 (Conv2D)\\n(None, 128, 27, 27)\\n153,728\\n9.\\nconv_2 (Concatenate)\\n(None, 256, 27, 27)\\n-\\n10.\\nmax_pooling2d_2 (MaxPooling2D)\\n(None, 256, 13, 13)\\n-\\n11.\\nzero_padding2d_2 (ZeroPadding2D)\\n(None, 256, 15, 15)\\n-\\n12.\\nconv_3 (Conv2D)\\n(None, 384, 13, 13)\\n885,120\\n13.\\nzero_padding2d_3 (ZeroPadding2D)\\n(None, 384, 15, 15)\\n-\\n14.\\nlambda_3 (Lambda)\\n(None, 192, 15, 15)\\n-\\n15.\\nlambda_4 (Lambda)\\n(None, 192, 15, 15)\\n-\\n16.\\nconv_4_1 (Conv2D)\\n(None, 192, 13, 13)\\n331,968\\n17.\\nconv_4_2 (Conv2D)\\n(None, 192, 13, 13)\\n331,968\\n18.\\nconv_4 (Concatenate)\\n(None, 384, 13, 13)\\n-\\n19.\\nzero_padding2d_4 (ZeroPadding2D)\\n(None, 384, 15, 15)\\n-\\n20.\\nlambda_5 (Lambda)\\n(None, 192, 15, 15)\\n-\\n21.\\nlambda_6 (Lambda)\\n(None, 192, 15, 15)\\n-\\n22.\\nconv_5_1 (Conv2D)\\n(None, 128, 13, 13)\\n221,312\\n23.\\nconv_5_2 (Conv2D)\\n(None, 128, 13, 13)\\n221,312\\n24.\\nconv_5 (Concatenate)\\n(None, 256, 13, 13)\\n-\\n25.\\nconvpool_5 (MaxPooling2D)\\n(None, 256, 6, 6)\\n-\\n26.\\nﬂatten (Flatten)\\n(None, 9216)\\n-\\n27.\\ndense_1 (Dense)\\n(None, 4096)\\n37,752,832\\n28.\\ndropout_1 (Dropout)\\n(None, 4096)\\n-\\n29.\\ndense_2 (Dense)\\n(None, 4096)\\n16,781,312\\n30.\\ndropout_2 (Dropout)\\n(None, 4096)\\n-\\n31.\\ndense_3 (Dense)\\n(None, 1000)\\n4,097,000\\n32.\\ndropout_3 (Dropout)\\n(None, 1000)\\n-\\n33.\\ndense_4 (Dense)\\n(None, 512)\\n512,512\\n34.\\ndropout_4 (Dropout)\\n(None, 512)\\n-\\n35.\\ndense_5 (Dense)\\n(None, 128)\\n65,664\\n36.\\ndropout_5 (Dropout)\\n(None, 128)\\n-\\n37.\\ndense_6 (Dense)\\n(None, 5)\\n645\\n38.\\nsoftmax (Activation)\\n(None, 5)\\n-\\nModel layer architecture shows the layers as produced from the Python program. The layers from\\n1 to 32 represent the AlexNet CNN base model and is compatible with ImageNet weights available\\nat [24]. The layers from 33 to 38 are proposed transfer learning layers. Fully connected layers(33, 35)\\nare activated with Rectiﬁed Linear Unit (ReLU) function and last layer at 37 is activated with SoftMax\\nfor classiﬁcation output.\\n3.1.2. Convolutional Layers\\nConvolution is the primary process of any neural network model required for learning and\\nclassiﬁcation using feature maps. In this layer, a kernel (a small matrix with specialized values to'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 7}, page_content='Sensors 2020, 20, 1702\\n8 of 18\\nextract features) is convoluted over layer input tensor to obtain feature maps and forwarded to output\\ntensor. We obtain an output feature map T from the Equation (1) and (2).\\nTr\\nh = Tr−1\\nh\\n−Kr\\nh + 2P\\nSr\\nh\\n+ 1\\n(1)\\nTr\\nw = Tr−1\\nw\\n−Kr\\nw + 2P\\nSrw\\n+ 1\\n(2)\\nWhere T is the output tensor with Tw width and Th height of the feature map of the previous\\nlayer. K denotes the kernel with size Kw and Kh and ﬁlter slides with stride S to Sw and Sh horizontal\\nand vertical directions with padding P. Similarly, r indicates the layer of operation. Output feature\\nmap tensor is obtained by applying a convolution process on the input layer X by kernel K such that\\nY(x, y) = (X ∗K)(x, y) where Y(x, y) is the output feature map tensor and product of the input tensor\\nX and the kernel K with x and y spatial indices. The convolution is represented here with standard\\nnotation by symbol ∗. Equation (3) shows the detail expression of convolution in Equation(3)\\nY(x, y) =\\nKw\\n2\\n∑\\ni=−Kw\\n2\\nKh\\n2\\n∑\\nj=−Kh\\n2\\nX(x −i, y −j)K(i, j)\\n(3)\\nThe proposed framework consists of ﬁve convolutional layers with three split convolutional layers to\\nload balance on multiple GPUs. We have eliminated batch normalization after convolutional layers\\nsince we have not observed any accuracy improvements during our experiments. Convolutional\\nLayers’ output was activated using Rectiﬁed Linear Unit (ReLU)\\n3.1.3. Transfer Learning Layers\\nA fully connected layer cross-connects every neuron from the input tensor to every neuron in the\\noutput tensor, with an activation function at the output layer. The fully connected layer ﬂattens the\\ninput tensor if the input shape rank is higher than 2. Hence a fully connected layer is a dot product of\\nthe input tensor by the applied kernel.\\nFully connected layers generate probabilities in a wide range; therefore, to reduce the noise level, it\\nis essential to eliminate the weaker predictions. To perform this operation, a dropout process is carried\\nout with a certain threshold of usually 0.5. This less densiﬁes the neurons after removing the value of\\nlesser probabilities. The dropout layer helps to avoid a model overﬁtting and signiﬁcantly improves\\nthe validation accuracy. A dropout layer reduces the functional size of the process by removing\\nunproductive neuron outputs. It is a regularization technique and ﬁlters out complex co-adaptation\\nduring the training phase.\\n3.1.4. Activation Function\\nRectiﬁed Linear Unit (ReLU) is a tensor output activation function and makes the model training\\nprocess non-linear. During the convolution process output, tensor may contain positive and negative\\nvalues; therefore, before forwarding the output to the next layer, an activation function is applied.\\nA ReLU function converts the values lower than zero to a zero value, and positive values leave\\nunchanged. This process is called rectiﬁcation. A non-saturating function f (x) = max(0, x) returns\\nzero or positive values from a range of negative and positive values. The output feature map is cleaned\\nout of negative values. It increases the non-linearity of the model without compromising the quality\\nof classiﬁcation in receptive ﬁelds during the convolution process. We can express ReLU function\\nmathematically, as shown in Equation (4).\\nY(x, y) =\\n(\\n0,\\nX(x, y) ≤0\\nX(x, y),\\notherwise\\n(4)'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 8}, page_content='Sensors 2020, 20, 1702\\n9 of 18\\nWhere X(x, y) is the input of ReLU, and Y(x, y) is computed output from ReLU activation for a\\nspeciﬁc neuron at x and y position.\\n3.2. Data Augmentation\\nIn the proposed method, data augmentation plays a signiﬁcant role in improving the performance\\nof the model. Data augmentation relates to modifying the image data and perform a series of operations,\\nsuch that the modiﬁed image remains in the same class. Data augmentation helps to generalize the\\ninput data, which, as a result, reﬂects a better test accuracy. There are three main techniques while\\naugmenting training data: expanding dataset, in-place or on the ﬂy augmentation, combining dataset\\nwith in-place augmentation. In our experiments, we employed in-place data augmentation.\\nIn-place augmentation relates to modifying the original data by applying a series of transforms\\nand returns the transformed data only. It exceptionally means that the size of the data is not changed,\\nand source data is not permanently modiﬁed, hence, it is not an additive process. Therefore, in\\nrepeating experiments, the training dataset is always different from the previous training since the\\ndata is transformed in-memory batch by batch with random transformations before inputting for the\\ntraining of the model.\\nWe employed Keras ImageDataGenerator to augment our training data. It provides various\\ntransforms to augment the image data such that scale, rotate, shear, brightness, zoom, channel shift,\\nwidth and height shift, and horizontal and vertical shift. In the proposed method, we applied geometric\\ntransforms like rescale, rotation, width shift, height shift, and horizontal shift to augment our source\\ndata for our experiments. In our experimental setup, on each input batch, we applied the rotation\\nangle of 30 degrees, width, and height shift each 20% and horizontal ﬂip randomly.\\nThe augmentation block in ﬁg-training accepts training source data from the data generator as a\\nbatch. It applies a series of random transformations as described above on each image in the batch.\\nThe transformed image batch replaces the original image batch and is presented to input for the\\ntraining generator.\\n3.3. Implementation Details\\nIn the proposed model, we introduced additional fully connected layers for the ﬁne-tuning of\\nthe model. Conﬁguration and implementation of the complete AlexNet CNN as a foundation model\\nhelped to load pre-trained weights, and appending additional layers does not disturb the carefully\\ntrained ImageNet weights. We employed three fully connected layers, i.e., dense-4 and dense-5\\nlayers followed by dropout of 50% and 30% respectively, and the last, i.e., dense-6 layer, followed\\nby a SoftMax activation layer. The fully connected layers were employed in an encoder fashion,\\nwith layer shapes 512, 128, and ﬁve, respectively. Training time was observed to be quite low with\\nrespect to the time for training the network from scratch. Table 2 shows the transfer learning layers\\nconﬁguration. We employed additional layers to tackle more classes with inter-class similarities and\\nintra-class differences.\\nTable 2. Transfer Learning - Layer conﬁguration.\\nS#\\nLayer (Type)\\nOutput\\nParameters\\n33.\\ndense_4 (Dense)\\n(None, 512)\\n512,512\\n34.\\ndropout_4 (Dropout)\\n(None, 512)\\n-\\n35.\\ndense_5 (Dense)\\n(None, 128)\\n65,664\\n36.\\ndropout_5 (Dropout)\\n(None, 128)\\n-\\n37.\\ndense_6 (Dense)\\n(None, 5)\\n645\\n38.\\nsoftmax (Activation)\\n(None, 5)\\n-\\nDeep learning has rapidly taken over the complex classiﬁcation tasks for intelligent\\napplications [25]. In the image classiﬁcation domain [26], it requires enormous time to train a network'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 9}, page_content='Sensors 2020, 20, 1702\\n10 of 18\\nup-to its optimal weights; however, transfer learning [27] has shown promising improvements in\\ntraining time and contextual classiﬁcation. Transfer learning can be adapted in three ways. 1: Fixed\\nfeature extractor, in which last fully connected layer is replaced to classify frames based on trained\\nnetwork, 2: Fine-Tuning of the CNN relates to adjusting the weights to ﬁt the model in the custom\\nscenario; this is done using backpropagation, which can be applied on almost any layer of the AlexNet\\nCNN, 3: Partial Pre-trained models are shared by the community. Checkpoints during the model\\ntraining are obtained, which are shared with other researchers to save time. Transfer learning reduces\\nthe training time, in most of the cases. In the proposed scenario, all the layers of AlexNet pre-trained\\nnetwork remained unchanged and employed the complete model. We employed the model adapted\\nfrom AlexNet, loaded the pre-trained weights, and appended new fully connected layers.\\nTraining a model from scratch requires a large dataset to obtain reasonable accuracy, which,\\nhowever, requires a signiﬁcant time and processing cost, whereas using a small dataset causes the\\nmodel overﬁtting. Introducing data augmentation strategy boosts the model validation accuracy by\\napproximately 10% [28], and combining data augmentation with pre-trained weights, trains a model\\nnear-perfect even if the dataset is not very large. It reduces the training time and achieves higher\\nevaluation scores.\\nVideo Source is an MPEG-4 digital video of the full-length recorded match. It contains all the\\nfeatures that a live stream represents. A video is a composition or group of frames, and a frame is\\nan image with the same size as the source video display size. The frame extraction process takes one\\nframe at a time for further processing. We extracted frames at six frames per second from the video,\\nsuch that we picked every ﬁfth frame from a video with a frame rate of 30 frames per second. A large\\nnumber of frames are obtained from a video of a full event. The frames obtained from the video are\\nhigh resolution and landscape. So before feeding a frame to the classiﬁcation system, frames were\\nresized to match the input shape of the model, i.e., 227 × 227 × 3. The pre-conditioned frames with\\nRGB color space were fed into the input layer of the classiﬁer model.\\nPre-conditioned frames are introduced to input in the proposed CNN model, which classiﬁes the\\ninput frame to a target class. Labeling the classes correctly for event detection is particularly important\\nsince the classiﬁcation results highly depend on the model training accuracy. Scene classiﬁcation is\\nbased on frame classiﬁcation and is performed over a range of frames detected for a certain predeﬁned\\nclass. A temporal similarity between consecutive frames results in the same class until there is a\\nsigniﬁcant change in the scene.\\n4. Results and Discussion\\nThe result section represents the experimental evaluations of the performance of the proposed\\nmethodology. In the later part, we will discuss the results in detail. We evaluated our results using\\nprecision, recall, accuracy, error rate, and F1-score for all available categories on the dataset of videos.\\nWe employed various small and large datasets sourced from publicly available datasets for image\\nclassiﬁcation and manually labeled dataset of sports videos. However, we have presented results for\\nthe sports video dataset only.\\n4.1. Dataset\\nFor performance evaluations, we have selected various sports videos from YouTubeTM,\\nconsidering different series and different lighting conditions. Our dataset consists of the following\\nvideos. Cricket 2014 New Zealand v South Africa 1st ODI Highlights - YouTube [29], World Record 438\\nMatch-South Africa vs. Australia- Part 2 - YouTube [30], Pakistan vs. Australia 3rd ODI Full Highlights\\n- YouTube [31], 1st ODI: Pakistan vs. Australia HD Full Highlights - YouTube [32].\\nA collection of 6800 frames was extracted from the source videos and labeled for ﬁve required\\nclasses. Figure 3 depicts the various classes employed in our experiments. The proposed method can\\nclassify different sports; however, to validate the method performance, we considered cricket as a case\\nstudy. Frame groups from key-events of the cricket sport were chosen as classes to identify the shots'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 10}, page_content='Sensors 2020, 20, 1702\\n11 of 18\\nand scenes expertly. The extracted frames were purposely classiﬁed to support video summarization\\ntasks for the sport of cricket. Due to the nature of video data, there are several features where overlaps\\nare present in multiple classes, e.g., almost all the classes have people (audience, empire, and players)\\nin view; furthermore, bowling and batting have the pitch as a standard feature. Boundary and crowd\\nhave the same size of the audience in both classes, and batting and close-up classes also portray\\ninter-class similarities. On the other hand, due to the various colors of shirts in a single scene, there\\nare intra-class differences, which makes the dataset a problematic task for classiﬁers, especially when\\nthere is a smaller dataset. Our proposed framework handles it effectively using augmentation and\\ntaking advantage of pre-trained weights to achieve robust performance over a shorter time for training\\nin addition to a smaller dataset.\\nFigure 3. Scene classes, (a) Bowling, (b) Batting, (c) Boundary, (d) Close-up, and (e) Crowd.\\n4.2. Training Procedure\\nFigure 2 explains in detail, the complete process of the model training and evaluation of the\\nproposed method. We employed TensorFlowTM backend with Keras applications for training purposes\\nand incorporated dataset generators to handle the large dataset of images with optimal memory usage.\\nImage data was fed to the training classiﬁer using a batch of 32 images augmented with the Keras\\nImageDataGenerator class [33]. We parameterized the generator to transform input images to apply a\\nset of transformations such that ﬂip, rotate, resize, and shift at both dimensions. We partitioned dataset\\ninto 70%, 30% for training and testing respectively, and separated 15% from training for validation\\ndataset. Training achieved a robust accuracy of above 99% in very initial epoch; however, to observe\\nthe overﬁtting and stability, we executed training until 50 epochs. We applied the Adam optimizer, a\\nlearning rate of 0.00001, and a loss model of ’categorical cross-entropy’ with accuracy as the training\\nperformance metric. Figure 4 shows the training progress over 50 epochs and reﬂects the stability and\\nhigh achievements of our proposed model, whereas Figure 5 depicts the training history of the model\\nproposed by Minhas et al. [1].\\nFigure 4. Proposed model training history. (a) Model accuracy, (b) Model loss.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 11}, page_content='Sensors 2020, 20, 1702\\n12 of 18\\nFigure 5. Minhas et al. training history. (a) Model accuracy, (b) Model loss.\\n4.3. Simulation Parameters\\nDuring our simulations, we applied optimistic and realistic parameters, considering the video\\nsummarization. We selected Python as a primary programming language throughout the simulation\\nprocess. We uploaded pre-trained weights from ImageNet with 1000 classes and used Adam optimizer\\nwith an optimal learning rate for training optimization. Table 3 lists the simulation parameters used in\\nour experiments.\\nTable 3. Simulation parameters.\\nModel\\nProposed method, ResNet50,\\nInception v3, VGG16, VGG19\\nMinhas et al.\\nSVM\\nLanguage\\nPython, Keras\\nPython, Keras\\nPython, Keras\\nPre-trained\\nImageNet, 1000 classes\\nNo\\nNo\\nOptimizer\\nAdam Optimizer\\nAdam Optimizer\\nRBF Kernel\\nLearning rate\\n0.00001\\n0.0000001\\ngamma [0.001, 0.0001]\\nLoss function\\nCategorical Cross-Entropy\\nCategorical Cross-Entropy\\nPerformance metric\\nAccuracy\\nAccuracy\\nAccuracy\\nTotal classes\\n05\\n05\\n05\\nVideo ﬁle format\\nMPEG-4\\nMPEG-4\\nMPEG-4\\nAugmentation\\nScale, Rotate, Shift, Flip\\nNone\\nNone\\nBatch size\\n32\\n32\\nNA\\n4.4. Learning Rate Selection\\nDuring the experiments, to train and evaluate the model, various learning rates were tried.\\nThe best learning rate under Adam optimizer worked with 0.00001. To keep the parameters comparable,\\nwe applied the same learning rate and the same optimizer for all the models under evaluation.\\nFigure 6 depicts a logarithmic line graph to show the accuracy response obtained from the model at\\ndifferent learning rates.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 12}, page_content='Sensors 2020, 20, 1702\\n13 of 18\\nFigure 6. Accuracy obtained using different learning rates.\\n4.5. Evaluation Metrics\\nWe evaluated our experiments using standard model evaluating metrics. We measure correct\\ndecisions and erroneous predictions in terms of precision, recall, and F1-score. F1-Score is the harmonic\\nmean of precision and recall. It reaches its best value at 1 when precision and recall both are perfect,\\nwhereas the worst F1 score is zero. It gives a better measure of incorrectly classiﬁed cases. Equation (5)\\nexpresses the F1-score as a measure of model performance.\\nF1Score = 2 ∗(Precision ∗Recall)\\n(Precision + Recall)\\n(5)\\n4.6. Performance Evaluation\\nThe proposed model presented improvements in accuracy, precision, recall, and F1-score.\\nThe model achieved a robust performance in comparison to other models. We compared our model\\nwith other deep CNN models like Inception v3, ResNet50, VGGNET16, VGGNET19 as well as Support\\nVector Machine (SVM) model and with the work presented by Minhas et al. The performance\\ndemonstrates the robustness of the model. The proposed model achieved a mean accuracy of 99.26%,\\nprecision of 99.27%, recall of 99.26%, which computes to 99.26% of F1-score, and an error rate of 0.74%.\\nResults indicated the effectiveness of the proposed model in the domain of sports video summarization.\\nFigure 7 demonstrates the predicted classiﬁcation of the scenes compared with ground truth.\\nProposed model evaluations reveal the inter-class similarities and intra-class differences.\\nTable 4 presents the confusion matrix of incorporated classes for shot classiﬁcation. Inter-class\\nsimilarities are evident in bowling and batting; however, even though there are signiﬁcant similarities\\nbetween boundaries and crowd due to the audience shown in the long-distance camera view,\\nthe proposed model classiﬁed it precisely. However, Table 5 reﬂects the confusion matrix presented\\nby [1] and is a comparatively an under-optimized matrix even after 1000 epochs in contrast to 50 epochs\\nfor our proposed model. We evaluated various models shown in Table 6 to compare model performance\\nagainst each class. Performance indicator F1-score shows that the proposed model outperformed\\namong other deep CNN models, as well as SVM.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 13}, page_content='Sensors 2020, 20, 1702\\n14 of 18\\nFigure 7. Video scene classiﬁcation predicted vs. ground truth.\\nTable 4. Confusion matrix (proposed model).\\nPredicted\\nActual\\nBatting Boundary Bowling Close-Ups Crowd\\nBatting\\n184\\n0\\n4\\n0\\n0\\nBoundary\\n0\\n459\\n0\\n0\\n0\\nBowling\\n10\\n0\\n351\\n0\\n0\\nClose-ups\\n0\\n0\\n0\\n791\\n0\\nCrowd\\n0\\n0\\n0\\n0\\n90\\nThe confusion matrix for the proposed model shows a high-quality classiﬁcation over ﬁve classes.\\nTable 5. Confusion matrix Minhas et al. [1].\\nPredicted\\nActual\\nBatting Boundary Bowling Close-Ups Crowd\\nBatting\\n169\\n4\\n2\\n13\\n0\\nBoundary\\n0\\n451\\n0\\n8\\n0\\nBowling\\n25\\n3\\n300\\n33\\n0\\nClose-ups\\n1\\n3\\n0\\n787\\n0\\nCrowd\\n0\\n17\\n0\\n2\\n71\\nThe confusion matrix shows a high rate of frame misclassiﬁcation.\\nTable 6. Classiﬁcation F1-score comparison with state-of-the-art models.\\nModel Names\\nEpoch Batting Boundary Bowling Close-Ups Crowd Mean\\nMinhas et al.\\n1000\\n88%\\n96%\\n90%\\n96%\\n88%\\n92%\\nInception v3\\n50\\n96%\\n99%\\n98%\\n100%\\n97%\\n98%\\nResNet50\\n50\\n94%\\n100%\\n97%\\n100%\\n99%\\n98%\\nVGGNET16\\n50\\n90%\\n98%\\n95%\\n98%\\n96%\\n95%\\nVGGNET19\\n50\\n91%\\n98%\\n96%\\n98%\\n95%\\n96%\\nSVM (64x64)\\nNA\\n84%\\n90%\\n95%\\n92%\\n88%\\n90%\\nProposed Method 50\\n96%\\n100%\\n98%\\n100%\\n100%\\n99%\\nClass wise F1-score in comparison with state-of-the-art models shows a robust performance.\\n4.7. Performance Comparison with Existing Sports Video Shot/Scene Classiﬁcation Methods\\nWe compared the performance of the proposed model with state-of-the-art models\\nand\\nexisting\\nmethods\\nproposed\\nin\\nrecent\\nresearch\\nin\\nsports\\nvideo\\nshot\\nclassiﬁcation.\\nTable 7 shows the results summary and quality score.\\nTo compare our model, we selected the most relevant and recent work in the same domain.\\nMinhas et al. [1] proposed a deep CNN model with signiﬁcant performance improvement against'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 14}, page_content='Sensors 2020, 20, 1702\\n15 of 18\\nstate-of-the-art methods like SVM, KNN, and ELM. They employed cricket and soccer mixed images\\nfor a dataset of four classes (close, medium, long, crowd). They prepared dataset from YouTube\\nvideos by frame extraction, labeled it manually and trained the network from scratch. The last fully\\nconnected layer was replaced with the required number of categories and trained the network over\\na set of categories from a collection of frames extracted from the publicly available video sources.\\nThe authors introduced frames of multiple sports like baseball, soccer, and cricket. It improved the\\noverall classiﬁcation capability of CNN Spectacularly lower inter-class similarities. To fairly compare\\nour proposed model with the reference model [1], we have implemented the reference model to test\\nthe performance of both methods using the same dataset. Table 7 shows performance comparison and\\nsuperiority of the proposed model is evident.\\nTable 7. Model performance evaluation and comparison-table with the state-of-the-art models\\nMethod name\\nepoch\\nAccuracy%\\nError%\\nPrecision%\\nRecall%\\nF1-score%\\nMinhas et al. [1]\\n1000\\n94.34\\n5.66\\n94.50\\n94.34\\n94.14\\nInception v3\\n50\\n98.41\\n1.59\\n98.50\\n98.41\\n98.38\\nResNet50\\n50\\n98.99\\n1.01\\n99.00\\n98.99\\n98.99\\nVGGNET16\\n50\\n97.46\\n2.54\\n97.52\\n97.46\\n97.46\\nVGGNET19\\n50\\n98.09\\n1.91\\n98.20\\n98.09\\n98.07\\nSVM\\nNA\\n86.71\\n13.29\\n87.28\\n86.71\\n86.80\\nProposed method\\n50\\n99.26\\n0.74\\n99.27\\n99.26\\n99.26\\nThe method proposed in Minhas et al. [1] is implemented and evaluated using the same dataset as of the proposed\\nmodel. Results show a performance gain over the existing state-of-the-art methods. Furthermore, the proposed\\nmethod shows a performance superiority over competitive models.\\nVarious studies proposed models, frameworks, and methodologies in the recent past for shot\\nclassiﬁcation exclusively or part of their scheme, conclusively, deep-learning approaches presented\\nhigh quality of the classiﬁcation. Deep learning proved its importance in image classiﬁcation, and\\nincorporating the pre-trained model and applying augmentation [28] improved results drastically\\nand reduced training time. Our experiments over sports videos scene classiﬁcation using the above\\ntechniques in addition to appending three fully connected layers in a pyramid shape to converge\\noutput to ﬁve classes exhibited the improved classiﬁcation results. This combination of multiple\\ntechniques together made this model superior in terms of training time and the size of the dataset\\nrequired for training and the performance. We employed a high-power GPU for our simulations,\\nwhich helped us run various simulations to ﬁnd the best learning rates, evaluate several deep-learning\\nmodels, and process video data efﬁciently.\\nWe selected the best learning rate to achieve the peak performance of the model. We tested our\\nmodel on various datasets publicly available on the Kaggle [34] to make sure it maintains superiority\\nover reference-models. We performed our experiments on a handpicked dataset extracted from the\\nvideo streams. The results are improved over the existing techniques. However, there are certain\\nlimitations to the method, i.e., the frames that do not belong to any category get easily miss-classiﬁed\\nto the closest target class. Such frames are not necessary for summarization, especially if they were\\nignored during the training process by the authors, but such frames cause a noise. To use this\\ntechnique in video summarization, we suggest training the model by including all possible available\\nscene classes. We suggest using this method in conjunction with shot-boundary detection methods to\\nproduce excellent video summarization performance.\\n5. Conclusions and Future Work\\nIn this paper, we proposed a model for sports video scene classiﬁcation with the particular\\nintention of sports video summarization. Our ﬁndings demonstrated that our proposed model\\nperformed best against state-of-the-art models and existing recent research level. We evaluated our\\nproposed model on cricket as a case study and obtained videos from publicly available YouTube source.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 15}, page_content='Sensors 2020, 20, 1702\\n16 of 18\\nWe labeled the ﬁve most frequent scenes from the videos, i.e., batting, bowling, boundary, close-up,\\nand the crowd to evaluate it to be the best ﬁt for video summarization. We compared our proposed\\nmodel with the existing state-of-the-art models, particularly deep-learning models, e.g., Inception\\nV3, VGGNET16, VGGNET19, ResNet50, and from non-deep-learning models like SVM to present a\\nperformance evaluation. We also compared our proposed model with recently proposed models in\\nthe ﬁeld of sports video summarization, and scene classiﬁcation; the performance results showed a\\nremarkable improvement.\\nAlthough the proposed methodology is evaluated over sports video data to provide a building\\nblock for video summarization; however, it can also be used for medical image analysis, agriculture\\nclassiﬁcation, biological organism classiﬁcation, self-driving vehicles and various other ﬁelds where\\nclassiﬁcation requires respectively higher accuracy and the error rate is a critical constraint. The\\nproposed model produced excellent performance indicators over previously proposed models, which\\nmakes it a strong candidate to be a reusable building block. Scene classiﬁcation can be further extended\\nto research on video description and medical video analysis and event description. Similarly, sports\\nvideo to text summarization is a hot topic of the times and requires researchers to improve the quality\\nof the task.\\nAuthor Contributions: Conceptualization, M.R.; Data curation, G.R. and R.A.; Formal analysis, M.R. and G.R.;\\nFunding acquisition, G.S.C.; Methodology, M.R.; Project administration, G.S.C.; Resources, G.S.C.; Software,\\nM.R. and G.R.; Supervision, S.-I.J. and G.S.C.; Validation, R.A. and G.S.C.; Writing—original draft, M.R.;\\nWriting—review and editing, S.-I.J. and G.S.C.\\nFunding: This research was supported by the Ministry of Trade, Industry & Energy (MOTIE, Korea) under\\nIndustrial Technology Innovation Program. No.10063130, MSIT(Ministry of Science and ICT), Korea, under the\\nITRC(Information Technology Research Center) support program (IITP-2019-2016-0-00313) supervised by the\\nIITP(Institute for Information & communications Technology Promotion), and the Brain Korea 21 Plus Program(No.\\n22A20130012814) funded by the National Research Foundation of Korea (NRF).\\nConﬂicts of Interest: The authors declare no conﬂict of interest.\\nReferences\\n1.\\nMinhas, R.A.; Javed, A.; Irtaza, A.; Mahmood, M.T.; Joo, Y.B. Shot classiﬁcation of ﬁeld sports videos using\\nAlexNet Convolutional Neural Network. Appl. Sci. 2019, 9(3), 483, doi:10.3390/app9030483. [CrossRef]\\n2.\\nAhn, E.; Kumar, A.; Feng, D.; Fulham, M.; Kim, J. Unsupervised deep transfer feature learning for medical\\nimage classiﬁcation. In Proceedings of the International Symposium on Biomedical Imaging, Venice, Italy,\\n8–11 April 2019; pp. 1915–1918, doi:10.1109/ISBI.2019.8759275. [CrossRef]\\n3.\\nKer, J.; Wang, L.; Rao, J.; Lim, T. Deep Learning Applications in Medical Image Analysis. IEEE Access 2017,\\n6, 9375–9379, doi:10.1109/ACCESS.2017.2788044. [CrossRef]\\n4.\\nPeng, Y.; Liao, M.; Song, Y.; Deng, H.; Wang, Y. FB-CNN : Feature Fusion-Based Bilinear CNN for\\nClassiﬁcation of Fruit Fly Image. IEEE Access 2020, 8, 3987–3995, doi:10.1109/ACCESS.2019.2961767.\\n[CrossRef]\\n5.\\nBehrendt, K.; Mangin, O.; Bhakta, N.; Lefevre, S. Is this car going to move? parked car classiﬁcation for\\nautomated vehicles. In Proceedings of the IEEE Intelligent Vehicles Symposium, Paris, France, 9–12 June\\n2019; pp. 541–548, doi:10.1109/IVS.2019.8813810. [CrossRef]\\n6.\\nAbu-El-Haija, S.; Kothari, N.; Lee, J.; Natsev, P.; Toderici, G.; Varadarajan, B.; Vijayanarasimhan, S.\\nYouTube-8M: A Large-Scale Video Classiﬁcation Benchmark. arXiv 2016, arXiv:1609.08675.\\n7.\\nSharma, R.A.; Pramod Sankar, K.; Jawahar, C.V. Fine-grain annotation of cricket videos. In Proceedings\\nof the 3rd IAPR Asian Conference on Pattern Recognition (ACPR 2015), Kuala Lumpur, Malaysia, 3–6\\nNovember 2015; pp. 421–425, doi:10.1109/ACPR.2015.7486538. [CrossRef]\\n8.\\nZhou, L.Sports video motion target detection and tracking based on hidden markov model. In Proceedings\\nof the 17th International Conference on Scientometrics and Informetrics, Qiqihar, China, 28–29 April 2019;\\npp. 825–829, doi:10.1109/ICMTMA.2019.00186. [CrossRef]\\n9.\\nDuan, Ling Yu; Xu, Min; Tian, Qi; Xu, Chang Sheng; Jin, Jesse S. A uniﬁed framework for semantic shot\\nclassiﬁcation in sports video. IEEE Trans. Multimedia 2005, 7, 1066–1083. [CrossRef]'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 16}, page_content='Sensors 2020, 20, 1702\\n17 of 18\\n10.\\nA. Kumar, J. Garg and A. Mukerjee Cricket activity detection. In Proceedings of the International Image\\nProcessing, Applications and Systems Conference, Sfax, Tunisia, 5–7 November 2014, pp. 1–6.. [CrossRef]\\n11.\\nSigari, M.H.; Soltanian-Zadeh, H.; Kiani, V.; Pourreza, A.R.\\nCounterattack detection in broadcast\\nsoccer videos using camera motion estimation.\\nIn Proceedings of the International Symposium\\non Artiﬁcial Intelligence and Signal Processing, Mashhad, Iran, 3–5 March 2015; pp.\\n101–106,\\ndoi:10.1109/AISP.2015.7123487. [CrossRef]\\n12.\\nRusso, M.A.; Kurnianggoro, L.; Jo, K.H.\\nClassiﬁcation of sports videos with combination of deep\\nlearning models and transfer learning.\\nIn Proceedings of the 2nd International Conference on\\nElectrical, Computer and Communication Engineering, Cox’sBazar, Bangladesh, 7–9 February 2019,\\ndoi:10.1109/ECACE.2019.8679371. [CrossRef]\\n13.\\nNg, Y.H.J.; Hausknecht, M.; Vijayanarasimhan, S.; Vinyals, O.; Monga, R.; Toderici, G.\\nBeyond short\\nsnippets: Deep networks for video classiﬁcation. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, Boston, MA, USA, 7–12 June 2015.\\n14.\\nLee, J.; Koh, Y.; Yang, J. A deep learning based video classiﬁcation system using multimodality correlation\\napproach. In Proceedings of the International Conference on Control, Automation and Systems, Jeju, South\\nKorea, 18–21 October 2017; pp. 2021–2025, doi:10.23919/ICCAS.2017.8204286. [CrossRef]\\n15.\\nHong, Y.; Ling, C.; Ye, Z. End-to-end soccer video scene and event classiﬁcation with deep transfer learning.\\nIn Proceedings of the 2018 International Conference on Intelligent Systems and Computer Vision, Fez,\\nMorocco, 2–4 April 2018; pp. 1–4, doi:10.1109/ISACV.2018.8369043. [CrossRef]\\n16.\\nAgyeman, R.; Muhammad, R.; Choi, G.S.\\nSoccer Video Summarization Using Deep Learning.\\nIn\\nProceedings of the 2nd International Conference on Multimedia Information Processing and Retrieval, San\\nJose, CA, USA, 28–30 March 2019, doi:10.1109/MIPR.2019.00055. [CrossRef]\\n17.\\nSozykin, K.; Protasov, S.; Khan, A.; Hussain, R.; Lee, J.\\nMulti-label class-imbalanced action\\nrecognition in hockey videos via 3D convolutional neural networks.\\nIn Proceedings of the\\n2018 IEEE/ACIS 19th International Conference on Software Engineering, Artiﬁcial Intelligence,\\nNetworking and Parallel/Distributed Computing, Busan, South Korea, 27–29 June 2018; pp. 146–151,\\ndoi:10.1109/SNPD.2018.8441034. [CrossRef]\\n18.\\nBhalla, A.; Ahuja, A.; Pant, P.; Mittal, A.\\nA Multimodal Approach for Automatic Cricket Video\\nSummarization.\\nIn Proceedings of the 2019 6th International Conference on Signal Processing and\\nIntegrated Networks, Noida, India, 7–8 March 2019; pp.\\n146–150, doi:10.1109/SPIN.2019.8711625.\\n[CrossRef]\\n19.\\nTejero-De-Pablos, A.; Nakashima, Y.; Sato, T.; Yokoya, N.; Linna, M.; Rahtu, E.\\nSummarization of\\nUser-Generated Sports Video by Using Deep Action Recognition Features. IEEE Trans. Multimedia 2018,\\n20, 2000–2011, doi:10.1109/TMM.2018.2794265. [CrossRef]\\n20.\\nShih, Huang-ChiaA Survey of Content-Aware Video Analysis for Sports. IEEE Trans. Circuits Syst. Video\\nTechnol. 2018, 28, 1212–1231, doi:10.1109/TCSVT.2017.2655624. [CrossRef]\\n21.\\nKarmaker, D.; Chowdhury, A.Z.; Miah, M.S.; Imran, M.A.; Rahman, M.H. Cricket shot classiﬁcation\\nusing motion vector.\\nIn Proceedings of the 2015 2nd International Conference on Computing\\nTechnology and Information Management, Johor, Malaysia, 21–23 April 2015;\\npp.\\n125–129,\\ndoi:10.1109/ICCTIM.2015.7224605. [CrossRef]\\n22.\\nJia, D.; Wei, D.; Socher, R.; Li-Jia, L.; Kai, L.; Fei-Fei, L. ImageNet: A large-scale hierarchical image database.\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA,\\n20–25 June 2009; pp. 248–255, doi:10.1109/cvprw.2009.5206848. [CrossRef]\\n23.\\nKarpathy, A. CS231n Convolutional Neural Networks for Visual Recognition. Available online: http:\\n//cs231n.stanford.edu/ (accessed on 26 December 2019).\\n24.\\nGitHub - heuritech/convnets-keras. Available online: http://cs231n.github.io/convolutional-networks/\\n(accessed on 26 December 2019)\\n25.\\nLecun, Y.; Bengio, Y.; Hinton, G. Deep learning, Nature 2015, 521, 436–444, doi:10.1038/nature14539.\\n[CrossRef] [PubMed]\\n26.\\nRawat, W.; Wang, Z. Deep convolutional neural networks for image classiﬁcation: A comprehensive\\nreview. Neural Comput. 2017, 29, 2353–2449.doi:10.1162/NECO_a_00990. [CrossRef] [PubMed]'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2020-03-20T17:18:59+08:00', 'source': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'file_path': '../data/pdf_files/05.Scene Classification for Sports Video Summarization Using Transfer Learning.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-03-20T17:18:59+08:00', 'trapped': '', 'modDate': \"D:20200320171859+08'00'\", 'creationDate': \"D:20200320171859+08'00'\", 'page': 17}, page_content='Sensors 2020, 20, 1702\\n18 of 18\\n27.\\nShen, W.; Jia, Y.; Wu, Y. 3D Shape Reconstruction from Images in the Frequency Domain. In Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 18 June\\n2019; pp. 4471–4479.\\n28.\\nChollet, F. M A N N I N G. In Deep Learning with Python; Manning Publications Co.: Shelter Island, NY,\\nUSA, 2018.\\n29.\\nCricket 2014 New Zealand v South Africa 1st ODI Highlights. Available online: https://www.youtube.\\ncom/watch?v=vZaRcTC0wFc (accessed on 26 December 2019).\\n30.\\nWorld Record 438 Match-South Africa vs Australia- part 2. Available online: https://www.youtube.com/\\nwatch?v=ZGUfdMukaZc (accessed on 26 December 2019).\\n31.\\nPakistan vs Australia 3rd ODI Full Highlights. Available online: https://www.youtube.com/watch?v=\\ncibvi2w6FDA (accessed on 26 December 2019).\\n32.\\n1st ODI: Pakistan VS Australia HD full Highlights. Available online: https://www.youtube.com/watch?\\nv=1-Zgn6SnQdM (accessed on 26 December 2019).\\n33.\\nApplications - Keras Documentation, 2015. Available online: https://keras.io/applications/ (accessed on\\n9 December 2019).\\n34.\\nKaggle: Your Home for Data Science. Available online: https://www.kaggle.com/ (accessed on 26\\nDecember 2019).\\nc⃝2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access\\narticle distributed under the terms and conditions of the Creative Commons Attribution\\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2022-03-08T12:03:42+00:00', 'source': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'file_path': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'ICT Express, 8 (2022) 65-71. doi:10.1016/j.icte.2021.03.004', 'keywords': '', 'moddate': '2022-03-08T12:03:42+00:00', 'trapped': '', 'modDate': \"D:20220308120342+00'00'\", 'creationDate': \"D:20220308120342+00'00'\", 'page': 0}, page_content='Available online at www.sciencedirect.com\\nScienceDirect\\nICT Express 8 (2022) 65–71\\nwww.elsevier.com/locate/icte\\nCategorization of actions in soccer videos using a combination of transfer\\nlearning and Gated Recurrent Unit\\nAnik Sena,b, Kaushik Deba,∗\\na Department of Computer Science & Engineering, Chittagong University of Engineering & Technology (CUET), Chattogram 4349, Bangladesh\\nb Department of Computer Science & Engineering, Premier University, Chattogram 4000, Bangladesh\\nReceived 27 July 2020; received in revised form 9 March 2021; accepted 10 March 2021\\nAvailable online 26 March 2021\\nAbstract\\nExtraction of knowledge from soccer videos has enormous applications like context-based advertisement, content-based video retrieval,\\nmatch summarization, and highlight extraction. Overlapping soccer actions and uncontrolled video capturing conditions make it challenging\\nto detect action accurately. For overcoming these problems, Convolutional Neural Network and Recurrent Neural Network are used jointly to\\nclassify different lengths of soccer actions. Initially, transfer learning from pre-trained VGG network extracts characteristic spatial features.\\nAfterwards, Gated Recurrent Unit deals with temporal dependency and solves the vanishing gradient problem. Finally, softmax layer assigns\\ndecimal probabilities to each class. Experimental results demystify the significance of the proposed architecture.\\nc⃝2021 The Korean Institute of Communications and Information Sciences (KICS). Publishing services by Elsevier B.V. This is an open access\\narticle under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\\nKeywords: Convolutional Neural Network (CNN); Gated Recurrent Unit (GRU); RecurrenT Neural Network (RNN); Transfer learning\\n1. Introduction\\nThough abounding research has been conducted with the\\ngoal of video classification, this field still poses multifold\\nnovel challenges. In recent years, sports-based work is becom-\\ning popular among researchers. Of all types of sports, soccer is\\none of the most popular and viewed across the globe. However,\\nin our knowledge, there are only a few works dedicated to\\nsoccer, which can have a wide range of applications in the field\\nof computer vision. For example, our work can be combined\\nwith some Natural Language Processing (NLP) models to\\ngenerate Artificial Intelligence (AI) based automatic, unbiased,\\nand blind commentary system. Moreover, to eliminate the\\nmanual, tedious task like automatic soccer highlight generation\\nand automated match summarizing, it is required to detect the\\nvital soccer actions and categorize them accordingly.\\nIn the upcoming sections, we expound on video clas-\\nsification by taking soccer-based sports actions. Moreover,\\naccording to [1], it can be seen that more than 84% of\\nsoccer fans watch past world cups or other soccer games on\\n∗Corresponding author.\\nE-mail address:\\ndebkaushik99@cuet.ac.bd (K. Deb).\\nPeer review under responsibility of The Korean Institute of Communica-\\ntions and Information Sciences (KICS).\\nYouTube. Due to the overlapping scenes between different\\nsoccer actions, classifying the proper actions is a challeng-\\ning task. Currently, there exist only a few works dedicated\\nto soccer. The dataset is not yet to that scale among the\\ncurrent works, and many of the vital actions are ignored. How-\\never, automatically categorizing various soccer actions has a\\nhigh commercial impact on the broadcasters. The above facts\\nand the tremendous achievement of deep learning algorithms\\nmotivated us for this work.\\nTo outline the central handouts of our work, we have\\nconsidered 10 soccer actions corner, foul, free-kick, goal-kick,\\nlong-pass, penalty, short-pass, shot-on-goal, substitution, and\\nthrow-in and developed a new dataset termed as SoccerAct10.\\nTo incorporate both the spatial-features and temporal-features,\\nwe seize the benefit of CNN and RNN. In the ongoing trend,\\nresearchers have intensive use of deep neural networks as it\\npushes the boundaries of many hand-engineered processes.\\nResearches related to soccer can be further categorized.\\nVideo retrieval from soccer to generate the highlights is per-\\nformed in [2]. Moving object detection is accomplished by\\nmotion intensity. For audio information extraction, short time\\nenergy and speech components are used. In [3], a framework\\nfor summarizing soccer videos is introduced which allows\\nreal-time event detection by cinematic features.\\nhttps://doi.org/10.1016/j.icte.2021.03.004\\n2405-9595/ c⃝2021 The Korean Institute of Communications and Information Sciences (KICS). Publishing services by Elsevier B.V. This is an open access\\narticle under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2022-03-08T12:03:42+00:00', 'source': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'file_path': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'ICT Express, 8 (2022) 65-71. doi:10.1016/j.icte.2021.03.004', 'keywords': '', 'moddate': '2022-03-08T12:03:42+00:00', 'trapped': '', 'modDate': \"D:20220308120342+00'00'\", 'creationDate': \"D:20220308120342+00'00'\", 'page': 1}, page_content='A. Sen and K. Deb\\nICT Express 8 (2022) 65–71\\nEvent detection from tiny set of soccer videos is pro-\\nposed in [4]. Mean-gradient features was compounded to shot\\nboundary detection. Furthermore, low and mid level feature\\ndescriptors were fed into the MKL based SVM classifier.\\nHowever, a study in [5] exemplifies that the softmax classifier\\ncan achieve better performance than the SVM classifier.\\nIn [6], 4 different actions of soccer, particularly shot-on-\\ngoal, placed-kick, throw-in and goal-kick, are classified. 3-fold\\ncross validation was performed with a tiny dataset containing\\nonly 20 videos as training and 5 as testing. The proposed\\nSVM string kernel based model attains a mean accuracy of\\n73%. In [7], a method for detecting goals in soccer video\\nis proposed which also combines audio clues. However, this\\nwork paves the way for more improvement. In [8], 3 group\\nactions during soccer, namely left side attacking (LA), left\\nside defending (LD), and stalemate (ST) are classified. This\\nmethod commits comparatively low accuracy of 83.87% for\\nthe LD and ST. In [9], 4 soccer actions: goal-kick, placed-\\nkick, shot-on-goal, and throw-in are classified. Using Bag\\nof Words (BoW), dominant motion, and Long Short-Term\\nMemory (LSTM), 92% accuracy is achieved. However, in a\\nstudy comparing BoW and CNN performance, it is shown that\\nCNN based method attains higher accuracy than BoW [10].\\nThe contributions of this work are depicted below:\\n(i) Developed a custom dataset having a variable length of\\nvideos\\n(ii) Proposed an algorithm for sampling a fixed number of\\nframes from a variable length of videos\\n(iii) Implemented deep learning algorithms, namely CNN,\\nGRU to overcome the limitations of the conventional\\nfeature extraction approach. Moreover, to overcome the\\nlimitation of a popular linear classifier SVM, we utilized\\nsoftmax classifier\\n(iv) Rigorous experiments are conducted to find the best\\narchitecture, and eventually designed a hybrid CNN–\\nGRU architecture which can categorize 10 distinct soc-\\ncer actions considering diverse capturing conditions\\n(e.g., abrupt change of camera angle while taking a cor-\\nner) and overlapping sub-patterns (e.g., players moving\\ntoward goal post can be similar for both free-kick and\\ncorner)\\n2. Proposed soccer actions categorization (SAC)\\narchitecture\\nSequence data are generated from variable length of videos.\\nThen, sequence data are fed into CNN (VGG), and extracted\\nfeatures are passed into GRU. The output from GRU is passed\\ninto softmax, and the class level is predicted. The steps to\\ndevelop the architectures for SAC are depicted in Fig. 1.\\n2.1. Network architecture\\nFrom various lengths of videos, features are extracted using\\nVGG. These extracted features are passed to a fully connected\\nlayer with ReLU activation. Then, flatten layer is stacked.\\nTime distributed wrapper encloses the layers mentioned above\\nFig. 1. Workflow of the proposed architecture for SAC.\\nFig. 2. VGG–GRU network architecture for SAC.\\nto apply the same layer to each frame. GRU layer is stacked\\nafterwards. Dropout is added with GRU to prevent overfitting.\\nVarious configurations of hidden units for Dense and GRU\\nlayers are depicted in Table 1. Finally, softmax activation\\nfunction as given in Eq. (1) is used, which normalizes the\\nprobabilities of different classes that sums up to 1.\\nσ(z) j =\\nez j\\n∑k\\nk=1 ezk f or j = 1, . . . , k\\n(1)\\nStochastic gradient descent (SGD) optimizer having 0.0001\\nlearning rate and 0.9 momentum is used throughout all the\\nexperiments. These hyperparameters are chosen using grid\\nsearch. ModelCheckpoint and EarlyStopping callbacks are\\nused here. The former saves the best weight, and the latter\\nstops training if there is no improvement in validation loss for\\n5 consecutive epochs. We used categorical crossentropy loss\\nfunction as defined in Eq. (2).\\nL( ˆyi, y) = −\\nK\\n∑\\ni=1\\nyi log ˆ\\n(yi)\\n(2)\\nHere, K is the total no of classes, yi is the desired one hot\\nencoded vector, and ˆyi is the model predicted output vector.\\nDuring backpropagation, the gradient starts to backpropagate\\nthrough the derivative of the loss function with respect to the\\noutput of the softmax layer.\\nThe initial batch size was 5 and varied this value rang-\\ning from 1 to 8 while resuming the training. The combined\\nVGG–GRU architecture is highlighted in Fig. 2.\\n2.2. Variable length videos to sequence generation\\nIncorporating an uneven number of frames is a challenging\\ntask while dealing with videos. The main action can be at any\\nvideo part; it can be at the starting part, middle part, or end\\npart. Therefore, when considering the frames, it is to be done\\nso that the frames of the whole video come into consideration.\\n66'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2022-03-08T12:03:42+00:00', 'source': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'file_path': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'ICT Express, 8 (2022) 65-71. doi:10.1016/j.icte.2021.03.004', 'keywords': '', 'moddate': '2022-03-08T12:03:42+00:00', 'trapped': '', 'modDate': \"D:20220308120342+00'00'\", 'creationDate': \"D:20220308120342+00'00'\", 'page': 2}, page_content='A. Sen and K. Deb\\nICT Express 8 (2022) 65–71\\nTable 1\\nNetwork configuration and accuracy of our proposed networks.\\nModel\\nPre-trained\\nmodel\\nTrainable layers\\n(excluding\\nfully-connected)\\nHidden units in\\nTimeDistributed\\nDense layer\\nHidden units\\nin GRU layer\\n(dropout)\\nTotal\\ntrainable\\nparameters\\nTraining\\naccuracy\\n(%)\\nF1-Score\\n(%)\\nModel-1\\nVGG16\\nNone\\n256\\n32 (0.4)\\n5,28,042\\n97.11\\n88.00\\nModel-2\\nVGG16\\nAll\\n2048\\n84 (0.6)\\n2,40,45,118\\n99.97\\n93.00\\nModel-3\\nVGG16\\nFinal 4\\n512\\n64 (0.4)\\n89,28,074\\n100\\n92.00\\nModel-4\\nVGG16\\nFinal 8\\n512\\n64 (0.5)\\n1,48,27,850\\n99.98\\n94.00\\nModel-5\\nVGG19\\nNone\\n1024\\n64 (0.4)\\n36,84,170\\n97.20\\n85.00\\nModel-6\\nVGG19\\nAll\\n512\\n64 (0.4)\\n2,18,73,034\\n99.90\\n94.00\\nModel-7\\nVGG19\\nFinal 4\\n2048\\n96 (0.5)\\n1,75,96,138\\n99.74\\n92.00\\nModel-8\\nVGG19\\nFinal 8\\n2048\\n72 (0.5)\\n2,23,038,58\\n99.97\\n93.00\\nModel-9\\nXception\\nFinal 4\\n512\\n32 (0.25)\\n16,492,122\\n96.67\\n77.00\\nModel-10\\nInceptionV3\\nFinal 9\\n512\\n32 (0.25)\\n8,553,866\\n98.97\\n84.00\\nIn our work, we accepted this challenge and proposed an\\nalgorithm that generates a fixed sequence length from uneven\\nnumber of video frames. The steps are outlined in Algorithm\\n1. The outer loop executes for all the files in a directory. Let\\nus consider the total number of files to be n. Until the loop of\\nline 4, the algorithm will execute the instructions in constant\\ntime. The inner loop considers each 2D RGB frame of a video\\nfile. Considering the highest number of frames to be m, the\\nproposed algorithm’s overall complexity is O(mn).\\nAlgorithm 1: Variable length videos to sequence genera-\\ntion.\\nInput: Integer seqlen, where seqlen < total frames of a\\nvideo.\\nOutput: List of data and labels\\n1 data ←[], labels ←[];\\n2 for each video file do\\n3\\nf rame count ←total frames in the file; label ←\\nGet Label of the video file;\\nx ←round( f rame count ÷ seqlen); count ←0 ,\\ni ←0, f rames ←[];\\n/* initialize count and i with 0. For each frame,\\ncount will be incremented. If a frame is selected,\\ni will be incremented.\\n*/\\n4\\nfor each frame in a file do\\n5\\nf rame ←read f rame values;\\n/* below condition checks if current frame is to\\nbe selected or not.\\n*/\\n6\\nif count mod x = 0 and i < seqlen then\\n7\\nAppend f rame to f rames; i + +;\\n8\\nend\\n9\\ncount + +;\\n// increment for each frame\\n10\\nend\\n11\\ndata.append(frames), labels.append(labels);\\n12 end\\n2.3. CNN and transfer learning\\nCNN is a version of multilayer perceptrons that is designed\\nto automate the spatial feature extraction process from image.\\nIt has already proven its tremendous success in the field of\\ncomputer vision. This feature extraction process is performed\\nusing some stack of layers, namely Convolution Layer, Pool-\\ning Layer, Flatten Layer, and Fully-connected Layer. The\\nconvolution operation is performed with some filters, thus\\nproducing the convolved feature maps. Pooling reduces the\\nparameters by reducing the spatial size of the image. The ini-\\ntial filters generally expose the low-level features like colors,\\ntexture, and lines, while the downstream layers detect more\\ncomplex patterns, which evidently describe an image. Weights\\nare updated in a similar fashion as multilayer perceptron\\n(MLP).\\nA deep neural network must have to be trained with a\\nmassive amount of data to attain high accuracy. However, for a\\ndomain-specific problem, it sometimes becomes challenging to\\nmanage this well-annotated vast dataset [11]. Transfer learning\\nalludes to the process of picking up knowledge from one\\nproblem and applying to another problem. Recently, a lot of\\nresearchers are using transfer learning instead of designing\\nthe whole network from scratch. It also provides the offer\\nof adding layers accordingly or re-train all the weights. By\\nlayer freezing technique, weight updating of specific layers\\ncan be stopped. Pre-trained models, already trained on massive\\ndatasets like ImageNet, provide an excellent starting point for\\nanother problem where the dataset size is not yet well enough.\\nSeveral pre-trained CNN models are the most widely used.\\nVGG, ResNet, Inception are worth mentioning. With some\\ninitial experiments, it is observed that VGG yields compar-\\natively high accuracy than other pre-trained models, namely\\nXception and InceptionV3, as shown in Table 1. Thereupon,\\ntransfer learning from deep VGG models are used [12] for our\\nspecific task of SAC. VGG uses very small (3×3) convolution\\nfilters with stride of 1. Max-pooling is performed over a 2 × 2\\npixel window, with stride 2. We used 2 versions of VGG,\\nVGG16 with ≈138M parameters, and VGG19 with ≈143M\\nparameters.\\n2.4. Gated recurrent unit (GRU)\\nFor video classification, it requires temporal features along\\nwith spatial features. For temporal features, various RNN has\\nbeen proposed. However, due to the gradient’s long prop-\\nagation through all the timesteps, the vanilla RNN suffers\\na lot from vanishing gradient and exploding gradient prob-\\nlem [13]. Hence, LSTM [14] was proposed later, which proves\\n67'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2022-03-08T12:03:42+00:00', 'source': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'file_path': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'ICT Express, 8 (2022) 65-71. doi:10.1016/j.icte.2021.03.004', 'keywords': '', 'moddate': '2022-03-08T12:03:42+00:00', 'trapped': '', 'modDate': \"D:20220308120342+00'00'\", 'creationDate': \"D:20220308120342+00'00'\", 'page': 3}, page_content='A. Sen and K. Deb\\nICT Express 8 (2022) 65–71\\nits effectiveness in maintaining the long temporal dependency.\\nGRU [15] is used in our work because this network is even a\\nsimplified version of LSTM, which can achieve similar results\\nas per LSTM, using fewer parameters. Update gate, Reset gate,\\nand Current memory gate are used. Update gate determines\\nthe amount of previous knowledge that is to be passed along\\ninto the future. Reset gate determines the amount of prior\\nknowledge that is to be forgotten. Using the current memory\\ngate, it calculates the present state value by combining the\\nprevious hidden state with the current input. The equations\\nfor the update gate, reset gate, current memory state, and final\\nmemory are given in Eq. (3), Eq. (4), Eq. (5), and Eq. (6)\\nrespectively:\\nzt = σ(Wz · [ht−1, xt] + bz)\\n(3)\\nrt = σ(Wr · [ht−1, xt] + br)\\n(4)\\n˜ht = tanh(W · [rt ∗ht−1, xt] + b)\\n(5)\\nht = (1 −zt) ∗ht−1 + zt ∗˜ht\\n(6)\\nHere, Wz, Wr, and W are learnable weight matrices, ht−1 is\\nthe previous hidden state, xt is the input vector, σ and tanh\\nare the sigmoid and tanh activation function, ∗represents the\\nhadamard product, and bz, br, and b are biases.\\n3. Experimental results and discussions\\nThe experiment was conducted on the configuration of\\nAMD Ryzen 7 2700X Eight-core 3.7 GHz Processor with\\nUbuntu 18.04 OS, 32 GB RAM, NVIDIA GEFORCE RTX\\n2060 SUPER of 8 GB GPU Memory. Keras using the Ten-\\nsorFlow backend was used. We extracted the dataset video\\nclips from different events of World Cup, AFC Asian Cup,\\nUEFA, Copa America, English Premier League, Italian Serie\\nA, German Bundesliga, and Spanish La Liga. Our custom\\ndataset, termed SoccerAct10, consists of a total of 3110 train-\\ning videos, the lowest 250 videos of foul, and the highest\\n375 videos of goal-kick. Additional 500 videos from different\\ngames, apart from the training set, were generated for the\\ntest set to remove the biasness. Videos were ranging from\\na minimum of 1.0 s to a maximum of 7.8 s Augmentation\\ntechniques, namely scaling, rotation, and horizontal flip are\\napplied to the training set. The average no. of frames for\\nthe whole dataset is 74.95. Meanwhile, the standard deviation\\nis 26. In Figs. 3(a)–3(j) sequential frames of 10 actions are\\nshown.\\nWith 100 samples of each class, we conducted an experi-\\nment using the VGG16 network to choose the best sequence\\nlength. Furthermore, almost all the sequence lengths from 5 to\\n15 provide more than 90% validation accuracy. Moreover, for\\nthe sequence of length 10, the model works out its best with\\n96.57% validation accuracy. Hence, the sequence length of 10\\nis considered for further investigation. 150 × 150 pixels frame\\nsize is considered through all the experiments. We conducted\\nvarious experiments to determine the best model. At first, for\\neach model, all the dense layers of VGG16 and VGG19 are\\nFig. 3. Sequential frames of (a) corner, (b) foul, (c) free-kick, (d) goal-kick,\\n(e) long-pass, (f) penalty, (g) short-pass, (h) shot-on-goal, (i) substitution,\\nand (j) throw-in.\\ndropped. Then a variety of configurations were tested to deter-\\nmine the best one. Some of the best configurations are shown\\nin Table 1. In Model-1 and Model-5, layer freezing is applied\\nto all the layers of VGG16 and VGG19, respectively. Model-2\\nand Model-6 is derived by making all the VGG16 and VGG19\\nlayers trainable. Model-3 and Model-7 keep the final 4 VGG16\\nand VGG19 trainable. Model-4 and Model-8 were executed by\\nfreezing all the layers except the final 8 layers of VGG16 and\\nVGG19, respectively. Model-9 takes output from an intermedi-\\nate layer (block4 sepconv2 act) of pre-trained Xception model\\nand makes final 4 layers trainable. Model-10 takes output from\\nan intermediate layer (average pooling2d 4) of pre-trained\\nInceptionV3 model and makes final 9 layers trainable.\\nAnalyzing the data, we see that transfer learning from\\nXception, and InceptionV3 models provide lower accuracy\\nthan VGG-based models. So, we were inspired to derive the\\nbest architectures for our SAC by fine-tuning the VGG-based\\nmodels in more detail.\\n68'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2022-03-08T12:03:42+00:00', 'source': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'file_path': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'ICT Express, 8 (2022) 65-71. doi:10.1016/j.icte.2021.03.004', 'keywords': '', 'moddate': '2022-03-08T12:03:42+00:00', 'trapped': '', 'modDate': \"D:20220308120342+00'00'\", 'creationDate': \"D:20220308120342+00'00'\", 'page': 4}, page_content='A. Sen and K. Deb\\nICT Express 8 (2022) 65–71\\nWe used grid search to determine the best optimizer and\\nlearning rate. From SGD, Adam, and RMSprop optimizers,\\nSGD with 0.0001 learning rate gave the best result. 0.9 mo-\\nmentum was used with the SGD optimizer. ReLU activa-\\ntion function is used in the TimeDistributed Dense layer as\\nit produced better results than tanh and sigmoid. The F1-\\nScore analysis is considered to evaluate model performance\\nas it considers both false positive and false negative samples.\\nModel-4 and Model-6 both provide an accuracy of 94% and\\namong these two models, Model-4 is the best as it yields\\ncomparatively less trainable parameters.\\nFor further exploration, the VGG configuration for both\\nthe best models is kept fixed. Afterwards, two types of ex-\\nperiments were conducted: one by keeping the hidden unit of\\nTimeDistributed dense layer as 512 and changing the GRU\\nhidden units as shown in Fig. 4(a), and another by keeping\\nthe GRU hidden unit as 64 while changing the hidden units in\\nTimeDistributed dense layer as depicted in Fig. 4(b). In either\\nway, we can conclude that our Model-4 and Model-6 gives us\\nthe best architecture for SAC. The confusion matrices for the\\nbest 2 models, Model-4 and Model-6 are depicted in Figs. 5(a)\\nand 5(b).\\nOur neural network is made up of CNN and RNN. Instead\\nof building CNN from scratch, we leverage the pre-trained\\nVGG model, which extracts the dominant spatial features\\nfrom video frames. Being trained on a massive dataset like\\nImageNet, transfer learning models give us the ability to work\\nwith a comparatively smaller dataset. However, to detect the\\nactions from videos, spatial features are not well enough.\\nIt also necessitates keeping track of the changes that occur\\nthroughout the video. Considering this fact, we adapted GRU,\\na simpler version of RNN, which tracks video frames’ changes\\nacross the timesteps. Moreover, it solves the vanishing gradient\\nproblem of our architecture and has fewer trainable parameters\\ncompared to LSTM, another version of RNN.\\nThe best model performance is shown in Fig. 6. From\\nthis figure, up to first 5 epochs, test accuracy is greater than\\ntraining accuracy, which signifies that initially, the test set\\ncontains comparatively easier data to predict. Gradually, test\\naccuracy is increasing with the training accuracy. After 25th\\nepoch, the test accuracy is increasing very slowly, which\\nindicates that the remaining test samples are hard to pre-\\ndict. EarlyStopping callback, monitoring no improvement of\\ntest loss for 5 consecutive epochs, stops the training at 50th\\nepoch. We resume the process again with the best model\\nsaved to check if accuracy can be further increased. Despite\\nhaving better accuracy, some actions are misclassified due to\\nthe actions being obscured by the surrounding players. Our\\nbest-tuned Model-4 architecture, getting the best weights for\\nSAC under complex scene conditions and overlapping sub-\\npatterns, attains better accuracy of 94% compared with [9]\\n(i.e. BoW, Dominant Motion, LSTM based approach) and [16]\\n(i.e. CNN–GRU based architecture), as shown in Table 2. The\\narchitecture used in [16] is not so deep as VGG and giving\\na very low accuracy of only 32% for our dataset. So, we can\\nconclude that, for SAC, our proposed VGG–GRU based deep\\narchitecture attains higher accuracy.\\nFig. 4. F1-Score graph against (a) hidden units in GRU layer, (b) hidden\\nunits in TimeDistributed dense layer.\\nTable 2\\nAccuracy comparison with other works.\\nReference\\nDataset size\\nAccuracy\\n[9]\\n100\\n92%\\n[16]\\n300\\n32%\\nOur proposed architecture\\n3610\\n94%\\nThe model proposed in [9] deals with only 4 soccer actions,\\nwhile our work has considered 10 soccer actions. If the number\\nof actions or videos is increased in [9], there might be a\\nfall in accuracy value due to overlapping similar actions.\\nAlternatively, there might be a modification of architecture\\nto incorporate new videos. The architecture proposed in [16]\\ncommits a very low accuracy for our dataset. The architecture\\nof [16] is shallow, which might be causing this low accuracy\\nvalue. By increasing the depth of the model, we can expect an\\nincrease in accuracy for our dataset.\\n4. Conclusion\\nIn this paper, we proposed hybrid VGG–GRU architectures\\nto classify 10 different soccer actions. To the best of our\\nknowledge, this is the highest number of distinct soccer actions\\nto be classified in a single paper. A new dataset termed as\\nSoccerAct10 is composed of various illumination conditions,\\nvarious camera angles, continuous moving cameras, and over-\\nlapping patterns. Videos of varying lengths are considered, and\\nthe sequence length is determined empirically. We leverage\\npre-trained VGG models for feature extraction. GRU incorpo-\\nrates temporal features of videos and eliminates the vanishing\\n69'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2022-03-08T12:03:42+00:00', 'source': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'file_path': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'ICT Express, 8 (2022) 65-71. doi:10.1016/j.icte.2021.03.004', 'keywords': '', 'moddate': '2022-03-08T12:03:42+00:00', 'trapped': '', 'modDate': \"D:20220308120342+00'00'\", 'creationDate': \"D:20220308120342+00'00'\", 'page': 5}, page_content='A. Sen and K. Deb\\nICT Express 8 (2022) 65–71\\nFig. 5. Confusion matrix for (a) Model-4, (b) Model-6.\\nFig. 6. Performance of Model-4 after 1st early stopping.\\ngradient problem. Various experiments were conducted to de-\\ntermine the layers and hidden units. Our experimental results\\nshow that VGG–GRU based proposed method outperforms\\nthan the other existing models by achieving highest of 94%\\naccuracy. However, this paper has made a few extensions for\\nadditional examination. By scaling the dataset, including more\\ncomplex scene actions, this false detection rate can be mini-\\nmized further. We commit to including more soccer actions.\\nMoreover, another upcoming challenge will be considering the\\nsudden appearance of spectators in between the main action.\\nWe also plan to explore other transfer learning models like\\nInception, Resnet, Mobilenet, and Xception in more detail and\\nlook over if accuracy can be further increased. By combining\\ndeep learning algorithms with the conventional feature extrac-\\ntion approach, some experiments will be executed. However,\\nGPU memory is a first rate impediment to cooperate larger\\nmodel.\\nCRediT authorship contribution statement\\nAnik Sen: Conceptualization, Methodology, Software, Data\\ncuration, Writing - original draft, Visualization, Investiga-\\ntion, Writing - review & editing. Kaushik Deb: Supervision,\\nWriting - review & editing.\\nDeclaration of competing interest\\nThe authors declare that they have no known competing\\nfinancial interests or personal relationships that could have\\nappeared to influence the work reported in this paper.\\nReferences\\n[1] Soccer fan youtube behavior statistics - think with google, google.\\n(n.d.), 2020, Accessed: 2020-08-07, https://www.thinkwithgoogle.com\\n/data/soccer-fan-youtube-behavior-statistics.\\n[2] H. Liu, Highlight extraction in soccer videos by using multimodal\\nanalysis, in: 2017 13th International Conference on Natural Computa-\\ntion, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD), IEEE,\\n2017, pp. 2169–2173.\\n[3] A. Ekin, A.M. Tekalp, R. Mehrotra, Automatic soccer video anal-\\nysis and summarization, IEEE Trans. Image Process. 12 (7) (2003)\\n796–807.\\n[4] W. Zhao, Y. Lu, H. Jiang, W. Huang, Event detection in soccer videos\\nusing shot focus identification, in: 2015 3rd IAPR Asian Conference\\non Pattern Recognition (ACPR), IEEE, 2015, pp. 341–345.\\n[5] X. Qi, T. Wang, J. Liu, Comparison of support vector machine\\nand softmax classifiers in computer vision, in: 2017 Second Interna-\\ntional Conference on Mechanical, Control and Computer Engineering\\n(ICMCCE), IEEE, 2017, pp. 151–155.\\n[6] L. Ballan, M. Bertini, A. Del Bimbo, G. Serra, Action categorization\\nin soccer videos using string kernels, in: 2009 Seventh International\\nWorkshop on Content-Based Multimedia Indexing, IEEE, 2009, pp.\\n13–18.\\n[7] P. Shi, X.-q. Yu, Goal event detection in soccer videos using\\nmulti-clues detection rules, in: 2009 International Conference on\\nManagement and Service Science, IEEE, 2009, pp. 1–4.\\n[8] Y. Kong, X. Zhang, Q. Wei, W. Hu, Y. Jia, Group action recognition\\nin soccer videos, in: 2008 19th International Conference on Pattern\\nRecognition, IEEE, 2008, pp. 1–4.\\n[9] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, A. Baskurt, Action\\nclassification in soccer videos with long short-term memory recurrent\\nneural networks, in: International Conference on Artificial Neural\\nNetworks, Springer, 2010, pp. 154–159.\\n[10] E. Okafor, P. Pawara, F. Karaaba, O. Surinta, V. Codreanu, L.\\nSchomaker, M. Wiering, Comparative study between deep learning\\nand bag of visual words for wild-animal recognition, in: 2016 IEEE\\nSymposium Series on Computational Intelligence (SSCI), IEEE, 2016,\\npp. 1–8.\\n70'),\n",
       " Document(metadata={'producer': 'pdfTeX', 'creator': 'Elsevier', 'creationdate': '2022-03-08T12:03:42+00:00', 'source': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'file_path': '../data/pdf_files/01.Categorization of actions in soccer videos using a combination of transfer  learning and Gated Recurrent Unit.pdf', 'total_pages': 7, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'ICT Express, 8 (2022) 65-71. doi:10.1016/j.icte.2021.03.004', 'keywords': '', 'moddate': '2022-03-08T12:03:42+00:00', 'trapped': '', 'modDate': \"D:20220308120342+00'00'\", 'creationDate': \"D:20220308120342+00'00'\", 'page': 6}, page_content='A. Sen and K. Deb\\nICT Express 8 (2022) 65–71\\n[11] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, C. Liu, A survey on\\ndeep transfer learning, in: International Conference on Artificial Neural\\nNetworks, Springer, 2018, pp. 270–279.\\n[12] K. Simonyan, A. Zisserman, Very deep convolutional networks for\\nlarge-scale image recognition, 2015, CoRR, abs/1409.1556.\\n[13] R. Pascanu, T. Mikolov, Y. Bengio, On the difficulty of training\\nrecurrent neural networks, in: International Conference on Machine\\nLearning, 2013, pp. 1310–1318.\\n[14] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural\\nComput. 9 (8) (1997) 1735–1780.\\n[15] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,\\nH.\\nSchwenk,\\nY.\\nBengio,\\nLearning\\nphrase\\nrepresentations\\nusing\\nRNN encoder-decoder for statistical machine translation, 2014, arXiv\\npreprint arXiv:1406.1078.\\n[16] M.A. Russo, A. Filonenko, K.-H. Jo, Sports classification in sequential\\nframes using CNN and RNN, in: 2018 International Conference on\\nInformation and Communication Technology Robotics (ICT-ROBOT),\\nIEEE, 2018, pp. 1–3.\\n71'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 0}, page_content='machine learning &\\nknowledge extraction\\nArticle\\nAI-Based Video Clipping of Soccer Events\\nJoakim Olav Valand 1,2,†, Haris Kadragic 1,2,†, Steven Alexander Hicks 1,3\\n, Vajira Lasantha Thambawita 1,3\\n,\\nCise Midoglu 1,*\\n, Tomas Kupka 4, Dag Johansen 5\\n, Michael Alexander Riegler 1,5\\nand Pål Halvorsen 1,3,4,*\\n\\x01\\x02\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\nCitation: Valand, J.O.; Kadragic, H.;\\nHicks, S.A.; Thambawita, V.L.;\\nMidoglu, C.; Kupka, T.; Johansen, D.;\\nRiegler, M.A.; Halvorsen, P. AI-Based\\nVideo Clipping of Soccer Events.\\nMach. Learn. Knowl. Extr. 2021, 3,\\n990–1008. https://doi.org/10.3390/\\nmake3040049\\nAcademic Editor:\\nRamón Alberto Mollineda Cárdenas\\nReceived: 5 November 2021\\nAccepted: 4 December 2021\\nPublished: 8 December 2021\\nPublisher’s Note: MDPI stays neutral\\nwith regard to jurisdictional claims in\\npublished maps and institutional afﬁl-\\niations.\\nCopyright: © 2021 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed\\nunder\\nthe\\nterms\\nand\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\n1\\nSimulaMet, 0167 Oslo, Norway; joakim.valand@gmail.com (J.O.V.); harisk@iﬁ.uio.no (H.K.);\\nsteven@simula.no (S.A.H.); vajira@simula.no (V.L.T.); michael@simula.no (M.A.R.)\\n2\\nDepartment of Informatics, University of Oslo, 0373 Oslo, Norway\\n3\\nDepartment of Computer Science, Oslo Metropolitan University, 0167 Oslo, Norway\\n4\\nForzasys AS, 0167 Oslo, Norway; tomas@forzasys.com\\n5\\nDepartment of Computer Science, UIT The Arctic University of Norway, 9037 Tromsø, Norway;\\ndag.johansen@uit.no\\n*\\nCorrespondence: cise@simula.no (C.M.); paalh@simula.no (P.H.)\\n†\\nThese authors contributed equally to this work.\\nAbstract: The current gold standard for extracting highlight clips from soccer games is the use of\\nmanual annotations and clippings, where human operators deﬁne the start and end of an event\\nand trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the\\nextent of being rendered infeasible for use in lower league games. In this paper, we aim to automate\\nthe process of highlight generation using logo transition detection, scene boundary detection, and\\noptional scene removal. We experiment with various approaches, using different neural network\\narchitectures on different datasets, and present two models that automatically ﬁnd the appropriate\\ntime interval for extracting goal events. These models are evaluated both quantitatively and qualita-\\ntively, and the results show that we can detect logo and scene transitions with high accuracy and\\ngenerate highlight clips that are highly acceptable for viewers. We conclude that there is considerable\\npotential in automating the overall soccer video clipping process.\\nKeywords: event clipping; deep learning; logo transition; scene boundary detection; soccer; sports\\nanalysis; video\\n1. Introduction\\nSports broadcasting and streaming are becoming increasingly popular, and the interest\\nfor viewing videos from sports events grows daily. For example, 3.572 billion viewers\\ntuned in to watch the 2018 FIFA World Cup [1], and as of 2020, soccer had a global\\nmarket share of about 43% of the 250 billion USD spectator sports industry [2]. The\\namount of content worldwide, such as footage, event highlights, goal and player statistics,\\nscores, and rankings, is enormous, not to mention rapidly growing, and there is a huge\\ninterest from numerous actors to consume this available content. In this respect, it is\\nimportant to provide game summaries, as has been done for decades, and more recently,\\nto dedicate streams for particular categories of events, such as goals, cards, saves, and\\npenalties. However, generating such summaries and event highlights requires the tedious\\nand expensive task of manually detecting and clipping events. The process of generating\\nsummaries, event highlights, and tags is often performed redundantly by different actors\\nfor different purposes.\\nA typical tagging center in live operation for both broadcast and streaming services is\\nshown in Figure 1, where a ﬁrst-level operator can follow one or more games concurrently,\\nand by the push of a button at the time of an event, publish the event to users. Note that\\nnot all tagging centers neccessarily use the same kind of pipeline (two levels of operation\\nintroduced to reduce the publishing latency, by annotating ﬁrst and reﬁning the event later);\\nhowever, the same type of costly operations are still required, regardless of the complexity\\nMach. Learn. Knowl. Extr. 2021, 3, 990–1008. https://doi.org/10.3390/make3040049\\nhttps://www.mdpi.com/journal/make'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 1}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n991\\nof the labeling procedure, i.e., splitting the task into two levels, or undertaking it as a\\nsingle combined procedure. The published event is then often automatically clipped using\\nstatic “−A” frames and “+B” frames from the annotated event position. Depending on the\\navailable resources, a second-level manual and more ﬁne-granular annotation operation\\ncan be performed. This is a relatively time-consuming operation, but it is of signiﬁcant\\nimportance to improve the user experience of viewers.\\n(a)\\n(b)\\nFigure 1. Tagging center in live operation. Several persons involved and a large number of buttons to press in a cumbersome,\\nerror-prone, and tedious manual process, affecting costs and quality. (a) One person can follow multiple games. (b) Adding\\nmetadata and ﬁne-granular clipping.\\nIn this context, automating the entire pipeline is considered to be the “holy grail” in\\nsports video production, since it would allow for the faster generation of game highlights\\nat a much lower cost and with lower latency. Here, recent developments in artiﬁcial\\nintelligence (AI) technology have shown great potential, but state-of-the-art results [3–7] are\\nfar from good enough for practical scenarios that have demanding real-time requirements,\\nwhere the detection of certain events such as goals and cards must have 100% accuracy.\\nEven though the detection operation has by far received the most attention, it is probably\\nthe easiest initial operation in the overall tagging pipeline, which potentially includes\\nvarious other operations after event detection.\\nIn this paper, we focus on improving the second level of operations—namely, the more\\ntime-consuming and expensive, ﬁne-grained annotation stage, where manually detected\\nevents are enhanced and reﬁned. Automating this process has the potential to both save\\nresources and improve quality, as this last production step is often not performed due to\\ntime limitations and costs. We aim to develop an AI-based solution to identify appropriate\\ntime intervals to highlight clips on a video’s timeline by deﬁning the optimal start and\\nstop point for each event. Thus, instead of a human operator manually searching frame-\\nby-frame back and forth, we use AI technology to ﬁnd scene changes, game turnovers,\\nlogo transitions, cheering, and replays to detect an event’s time interval automatically. In\\nparticular, after experimenting with a variety of Machine Learning (ML) models, we present\\na system that can ﬁnd logo transitions using models based on ResNet [8] and VGG [9]\\nand undertake scene boundary detection using a TransNet V2 [10] model pre-trained on\\nthe ClipShots [11] and IACC.3 [12] datasets. Cheering by players and fans can optionally\\nbe removed by detecting scenes between the event and replays. These sub-systems are\\nthen combined into an automated clipping system. Our experimental results show that\\nboth scene changes and logo transitions are detected with high accuracy, and a subjective\\nassessment from 61 participants clearly indicates that an automated system can provide\\nevent highlights of high quality.\\nThis work is a continuation of [13], where an initial event detection framework based\\non logo transition and scene boundary detection was proposed. We extend this work by\\nmaking the following contributions:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 2}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n992\\n•\\nWe elaborate further on the logo detection component, providing more details on the\\ntechnical implementation, and additionally present a complexity analysis in terms of\\nexecution time;\\n•\\nWe elaborate further on the scene boundary detection component, providing more\\ndetails on the technical implementation, and additionally present an analysis of\\nmisclassiﬁed transitions (false positives and false negatives);\\n•\\nWe add a new component to our pipeline, whereby cheering and celebration scenes\\ncan optionally be trimmed;\\n•\\nWe provide a more detailed description of the datasets and performance metrics we\\nhave used;\\n•\\nWe run a subjective evaluation (user study) involving 61 participants in order to\\nevaluate the quality of our overall pipeline and present the results in the form of an\\nA/B study across various participant classes;\\n•\\nWe add a discussion of the potential pitfalls of our pipeline and the generalizability of\\nour approach.\\nIn summary, we have developed an algorithmic video clipping system for soccer,\\nwhich is able to perform in real-time, automating the most labor-intensive part of the\\nhighlight generation process and potentially reducing production costs. Such a system is\\napplicable to various other sports broadcasts such as skiing, handball, or ice hockey, and\\npresents a viable potential to affect future sports productions.\\nThe rest of this paper is organized as follows. In Section 2, we provide a brief overview\\nof related work in the ﬁelds of action detection and event clipping. We elaborate on our\\nproposed framework in Section 3, followed by our implementation and experiments in\\nSection 4. We present the results from our subjective evaluation campaign in Section 5. We\\nraise a number of discussion points in Section 6 and conclude the paper in Section 7.\\n2. Related Work\\nRecent research has successfully used ML to solve video-related problems and to\\npresent viewers with video segments of desired events in an efﬁcient manner. Here, we\\npresent selected works in the area of event detection and event clipping.\\nEvent detection, also called action detection or action spotting, has lately received\\na great deal of attention. For example, multiple variants of two-stream convolutional\\nneural networks (CNNs) have been applied to the problem [14,15] and extended to in-\\nclude 3D convolution and pooling [16,17]. Wang et al. [18,19] proposed temporal segment\\nnetworks (TSN), and C3D [20] explored 3D convolution learning spatio-temporal fea-\\ntures. Tran et al. [21] used (2+1)D convolutions to allow the network to learn spatial and\\ntemporal features separately. Further approaches aim at ﬁnding temporal points in the\\ntimeline [3,7,22–26], but even though many of these works present interesting approaches\\nand promising results, such technologies are not yet ready to be used in real-life deploy-\\nments. The reason is that most of the proposed models are computationally expensive\\nand relatively inaccurate. In deployments where action/event annotation results are used\\nin an ofﬁcial context (e.g., live sports broadcasts), these must be 100% accurate—i.e., no\\nfalse alarms or missed events are allowed—so manual operations are still needed. In this\\npaper, our goal is to automate the manual process of event tagging for soccer videos, while\\nmaintaining accuracy and efﬁciency.\\nIn the area of event clipping, the amount of existing work is limited. Koumaras et al. [27]\\npresented a shot detection algorithm, and Zawbaa et al. [28] implemented a more tailored\\nalgorithm to handle cuts that transitioned gradually over several frames. Zawbaa et al. [29]\\nclassiﬁed soccer video scenes as long, medium, close-up, and audience/out of ﬁeld, and\\nseveral papers presented good results regarding scene classiﬁcation [29–31]. Video clips\\ncan also contain replays after an event, and replay detection can help to ﬁlter out irrel-\\nevant replays. Ren et al. [32] introduced the class labels play, focus, replay, and breaks.\\nDetecting replays in soccer games using a logo-based approach was shown to be effective\\nusing a support vector machine (SVM) algorithm, but not as effective using an artiﬁcial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 3}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n993\\nneural network (ANN) [28,29]. Furthermore, it was shown that audio may be an important\\nmodality for ﬁnding good clipping points. Raventos et al. [33] used audio features to give\\nan importance score to video highlights, and Tjondronegoro et al. [34] used audio for a\\nsummarization method, detecting whistle sounds based on the frequency and pitch of\\nthe audio. Finally, some work focused on learning spatio-temporal features using various\\nML approaches [15,17,21], and Chen et al. [35] used an entropy-based motion approach to\\naddress the problem of video segmentation in sports events.\\nThese works indicate a potential for the AI-supported production of sports videos.\\nFor example, extracting temporal information can be very useful for generating highlight\\nclips. However, the presented results are still limited, and most importantly, the actual\\nevent clipping operation is not addressed. Computing should be possible to undertake\\nwith very low latency, as the production of highlight clips needs to be done in real-time, for\\nthe majority of use cases.\\n3. Proposed Framework\\nWe propose an AI-based framework for the automatic clipping of soccer videos. Our\\nframework is based on observations from real production environments in Norway and\\nSweden, such as the environment shown in Figure 1. In this section, we give a brief\\noverview of our proposed solution.\\n3.1. Motivation\\nClipping a video event is a tedious and expensive task. For example, consider the ﬁrst\\nrow in Figure 2, depicting the video frames around an event occurring at the red frame.\\nThe produced video has scene changes, where different cameras capture the event from\\ndifferent angles and cover different parts of the soccer ﬁeld. A goal typically consists of a\\nsmall part of the attack leading to the goal; then, players are celebrating, the audience is\\ncheering, and ﬁnally, several replays of the goal are shown. The viewers often want a clip\\nto start a little before the goal event and to include the goal itself, possibly some cheering,\\nand a clean-cut after some replays.\\nFigure 2. Example of original frame sequence and the clipping solutions.\\nTo save time and promote certain events with low latency, elite soccer leagues in Nor-\\nway and Sweden use a multi-level annotation scheme. The ﬁrst level annotator (Figure 1a)\\nmarks the event on the timeline, tags the event with the scoring team, and publishes the\\nevent with a static clipping at the start (e.g., −A frames from the event frame) and stop\\n(e.g., +B frames) timestamps, as shown in the second row in Figure 2. This means that the\\nclips often start far too early or in the middle of the event of interest, and they often end\\nabruptly in the middle of a replay. Then, if time and resources are available, a second-level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 4}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n994\\nannotator (Figure 1b) searches for a better clipping position in the frame sequence, which\\nis a time-consuming and costly process. Our basic idea is to automate this second-level\\nannotation (clipping), as shown in the third row in Figure 2, and further optimize by\\nremoving some of the celebrations and cheering to reduce the length of the entire clip\\n(fourth row in Figure 2).\\nIn the following, we describe a framework for automating this task through ML, using\\nstate-of-the-art deep learning models for event clipping. Our proposed framework for\\nautomated highlight generation uses a combination of logo detection, scene boundary\\ndetection, and optional trimming.\\n3.2. Logo Detection\\nA typical goal event has logo transitions between the cheering, the replays, and when\\nthe event is ﬁnished (see Figure 3 for examples from the Norwegian Eliteserien). These\\ncan be used as temporal anchors for clipping by an automated framework, and thus, it is\\nimportant to accurately detect logo transitions. We propose the use of a sliding window\\napproach, searching the video for logos and extracting relevant features, such as shapes,\\nedges, motions, and complex patterns. The challenge here is to reduce the computational\\ncost and memory usage of extracting these features while still keeping all the relevant\\ninformation for our model. For this task, we propose the use of relatively complex state-of-\\nthe-art models that have performed well on similar tasks; i.e., we build our system based\\non models such as VGG16 [9] and ResNet [8]. As a comparison, a smaller CNN and a\\nlightweight VGG, which are computationally cheaper, can be used to see how well they\\nperform compared to the state-of-the-art models. Moreover, Zambaa et al. [29] presented\\ngood results using SVM models for logo detection, so we also test SVM together with state-\\nof-the-art CNN feature extractors. Through this comparison, our goal is to ﬁnd a model\\nthat provides sufﬁciently good predictions while still remaining computationally cheap.\\nFigure 3. Typical logo transitions from the Norwegian Eliteserien dataset.\\n3.3. Scene Boundary Detection\\nAnother well-suited place to clip a video sequence is at a scene change (or a “shot\\nboundary”) where, for example, the camera angle changes. For this task, we propose to\\nuse TransNet V2 [10]: a state-of-the-art scalable architecture for scene boundary detection\\nthat has achieved superb performance on shot-boundary datasets, such as ClipShots [11],\\nRAI [36], and BBC [37]. The network takes a sequence of consecutive video frames and\\nuses a series of convolutions together with handcrafted image features. The features are\\nconcatenated, and then the system returns a prediction for every frame in the input [10,38].\\nThe TransNet model is pre-trained on transitions extracted from ClipShots [11] and the\\nTRECVid IACC.3 [12] datasets. We propose to compare this pre-trained model with a model\\ntrained from scratch on different soccer video clips and identify the inﬂuencing factors on\\nperformance so that an optimal scene boundary detection model can be integrated into our\\noverall pipeline.\\n3.4. Trimming of Cheering Scenes\\nAnother observation we make regarding typical goal events is that the event might\\nbe followed by a variable duration of cheering and celebration scenes. The importance of\\nthese scenes depends heavily on game context, as well as the potential limit on the desired'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 5}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n995\\nduration of each highlight clip. Therefore, we propose that the component for trimming\\nthe cheering scenes is optional, depending on whether there is a need to reduce the length\\nof a highlight clip or not. For this task, we derive an empirical rule-based solution, taking\\ninto account the following observations: (1) scene changes between the main goal event\\nand the replays often start with a logo transition; (2) the ﬁrst goal scene is almost always\\nfrom a single camera, meaning that all camera/scene changes before the logo transition are\\nmost likely spectators cheering and players celebrating; and (3) celebration scenes provide\\na useful context for highlight clips—therefore, a nonzero duration of cheering should be\\nincluded after the goal and before the replay, albeit short if necessary.\\n4. Experiments and Results\\nIn the following, we explain how we combined the individual components mentioned\\nin Section 3 into a full-ﬂedged automated event clipping system. We experimented with\\nvarious approaches for each component of our pipeline, and we ﬁnally present two models\\nthat automatically ﬁnd the appropriate time interval for goal event extraction.\\n4.1. Datasets\\nTo develop our automated clipping system, we used two different soccer datasets. We\\nused the open SoccerNet v2 [39] dataset and an in-house collected dataset of goals from the\\nNorwegian “Eliteserien”. SoccerNet contains a large number of manually annotated, com-\\nplete games with more than 100,000 different annotations across various event classes, with\\n1643 goals in particular, and around 150,000 scene change timestamps. Our “Eliteserien”\\ndataset is smaller, with 300 clips of goals. These clips start 25 s before the annotated goal\\nand end 50 s after. Furthermore, for scene boundary detection, we extracted 100 frames\\naround each of the scene change timestamps of SoccerNet. We analyzed several sequences\\nto get a better idea about how a goal event is built up and concluded that these events are\\ntypically structured as shown in the ﬁrst row in Figure 2.\\n4.2. Implementation Details\\nOur tested models were implemented using Python version 3.7.10 (gcc version 7.3.0)\\nwith Numpy 1.19.2, Keras 2.4.3, Sklearn 0.24.1, and Tensorﬂow 2.4.1, and the experiments\\nwere run on a DGX-2 server. Both datasets were split into train, validation, and test sets.\\nFor Eliteserien, we split each of the 50 clips into 60% train, 20% validation, and 20% test sets.\\nThe SoccerNet dataset is split according to the game that the frames are extracted from,\\nand we used the split recommended by the SoccerNet team. This resulted in 29 games for\\ntraining, 6 for validation, and 5 for testing. To avoid large weight values and to prevent\\nthe exploding gradient problem, we normalized our pixel values to be centered around\\nzero—i.e., between −1 and 1—similar to the pre-processing function of ResNet50 V2 in\\nKeras [40]. Furthermore, when training our models, we augmented each image in the\\ntraining set with a random degree of shear between 0 and 0.2. Shear distorts the image\\nalong an axis to rectify the perception angles and can represent looking at an object from\\ndifferent angles. The images also were subjected to a random value between 0–20% of\\nzoom for each axis independently. The lost pixels were ﬁlled with the nearest pixel’s value.\\nThere was also a 50% chance of horizontal ﬂip. These augmentations happened on the ﬂy\\nwith the use of Keras ImageDataGenerator [41].\\n4.3. Metrics\\nTo evaluate the performance of different ML models, we have used precision, recall,\\nand F1-score. Given that the true positive (TP) is the number of samples correctly identiﬁed\\nas positive, true negative (TN) is the number of samples correctly identiﬁed as negative,\\nfalse positive (FP) is the number of samples wrongly identiﬁed as positive, and false\\nnegative (FN) is the number of samples wrongly identiﬁed as negative, these metrics are\\ndeﬁned as follows:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 6}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n996\\nRecall is a measure of sensitivity and is deﬁned as the ratio of samples that are correctly\\nidentiﬁed as positive over all positive samples:\\nrecall =\\nTP\\n# of all positives =\\nTP\\nTP + FN\\nPrecision is also called Positive Predictive Value (PPV) and is deﬁned as the ratio of\\nsamples that are correctly identiﬁed as positive over all samples that are identiﬁed as\\npositive (i.e., the fraction of retrieved samples that are actually relevant):\\nprecision =\\nTP\\n# of all returned samples =\\nTP\\nTP + FP\\nF1 score is deﬁned as the harmonic mean of the precision and recall:\\nF1 score = 2 × precision × recall\\nprecision + recall =\\n2TP\\n2TP + FP + FN\\nIn addition to the above metrics, system performance metrics such as complexity,\\nprocessing speed, and resource consumption are of interest. In our work, we have used the\\nachieved frame rate (FPS) as a metric representing the ability of the system for performing\\nin real-time.\\n4.4. Logo Detection Performance\\nAs mentioned in Section 3, we used VGG16 [9], ResNet [8], a simple CNN, and a\\nlightweight VGG for the task of logo detection. To train the CNN models, we initialized\\nthe weights using the Glorot uniform initialization [42] with the Adam optimizer [43] to\\nimprove the weights. We used binary cross-entropy as the loss function. Because ResNet50\\nV2 comes with pre-trained weights on ImageNet [44], we used these, and all data were\\npre-processed as described above. In general, we used a learning rate of 0.001 and a batch\\nsize of 32. We ran training with early stopping using a patience of 10 epochs, meaning that\\nwe stopped if the loss of the validation set did not improve for 10 epochs. We also reduced\\nthe learning rate by a factor of 10, with a patience of 7 epochs on plateau, meaning that\\nthere was no improvement for validation loss. This was to ﬁne-tune the model in the last\\nepochs. We ran training for a maximum of 40 epochs. For both Eliteserien and Premier\\nLeague data, we used several input resolutions and switched between RGB (3 channels)\\nand grayscale (1 channel) images as a trade-off between computation time and accuracy. To\\ntrain the SVM models, we started by extracting features either using the VGG16 network or\\nthe simple CNN. Then, we used a grid search for hyper-parameter tuning (across different\\nregularization parameter and learning rate values), which returned the best estimator. We\\ndeﬁned a max iteration of 100 epochs.\\nTable 1 shows the top 10 models for the Eliteserien dataset, as tested on 50 different\\nclips. It can be observed from these results that there is a good performance on the test set,\\nwith the VGG-inspired model using a grayscale 54 × 96 × 1 input reaching 100% recall and\\nprecision. The SVM-based model with different inputs shows similar scores, albeit slightly\\nworse. We noticed that some models had a signiﬁcant decrease in performance, especially\\non the recall, between the training and testing. This may be due to overﬁtting, which is\\ndiscussed in Section 6. The models correctly identify more than 15 frames, meaning they\\nwill still perform well as part of the logo detection component. As the test set is quite small\\nand includes some team logos that are not present in the training set, even one misclassiﬁed\\nlogo frame can have a signiﬁcant impact on the recall. While investigating the performance\\nof the models on the full clips from Eliteserien, we deduced that all models perform well\\nenough to ﬁnd all logos we tested on without any false positives.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 7}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n997\\nTable 1. Best logo detection results on the Eliteserien test set, after optimizing the model conﬁgura-\\ntions on the validation set.\\nModel\\nInput\\nRecall\\nPrecision\\nF1-Score\\nResNet\\n54 × 96 × 3\\n0.9686\\n0.9954\\n0.9818\\nVGG-inspired\\n108 × 192 × 3\\n0.9686\\n0.9954\\n0.9818\\nVGG-inspired\\n72 × 72 × 3\\n0.991\\n1.0000\\n0.9955\\nVGG-inspired\\n54 × 96 × 1\\n1.0000\\n1.0000\\n1.0000\\nVGG-inspired\\n27 × 48 × 3\\n0.9731\\n1.0000\\n0.9864\\nSimple CNN\\n72 × 72 × 3\\n0.9731\\n0.9954\\n0.9841\\nSVM (simple CNN)\\n108 × 192 × 1\\n0.9955\\n1.0000\\n0.9978\\nSVM (simple CNN)\\n72 × 72 × 3\\n0.9731\\n1.0000\\n0.9864\\nSVM (simple CNN)\\n72 × 72 × 1\\n0.9865\\n0.9865\\n0.9865\\nSVM (simple CNN)\\n27 × 48 × 3\\n0.9776\\n1.0000\\n0.9887\\nTable 2 shows the top 10 models for the larger Premier League dataset (extracted from\\nSoccerNet), using the best configurations tuned on the validation set. The results for the\\nCNNs are overall good, where the models are capable of finding most logos with few false\\nnegatives. If we consider the two-level tagging scenario mentioned earlier, where the first\\nlevel marks the event on the timeline and publishes the event with a static clipping at the\\nstart (−A frames from the event frame) and stop (+B frames) times (see Figure 1a), our\\nmodule will have significantly fewer background frames, and the false negatives are a much\\nsmaller problem. We find that, even with more complex logos, our strategy works well.\\nTable 2. Best logo detection results on the Premier League test set, after optimizing the model\\nconﬁgurations on the validation set.\\nClassiﬁer\\nInput\\nRecall\\nPrecision\\nF1-Score\\nResNet\\n144 × 256 × 3\\n0.986\\n0.997\\n0.993\\nResNet\\n108 × 192 × 3\\n0.995\\n1.000\\n0.997\\nResNet\\n54 × 96 × 3\\n0.946\\n0.989\\n0.967\\nVGG-inspired\\n144 × 256 × 3\\n0.992\\n0.992\\n0.992\\nVGG-inspired\\n144 × 256 × 1\\n0.978\\n1.000\\n0.989\\nVGG-inspired\\n108 × 192 × 3\\n0.943\\n0.994\\n0.968\\nVGG-inspired\\n72 × 72 × 1\\n0.965\\n0.983\\n0.974\\nVGG-inspired\\n54 × 96 × 3\\n0.995\\n0.753\\n0.857\\nSimple CNN\\n144 × 256 × 3\\n0.978\\n0.997\\n0.988\\nSimple CNN\\n108 × 192 × 3\\n0.981\\n0.978\\n0.980\\nFinally, to determine whether logos can be detected in real-time, we present the\\ncomputational efﬁciency of various models and inputs in Table 3. Regardless of the tested\\nconﬁgurations, we observe that the processed frame rate is far beyond a 30 or 50 FPS real-\\ntime threshold. Even though the execution cost for the different CNN models is relatively\\nhigh, we do not need to compromise in order to get acceptable performance in practice.\\nFor use in our ﬁnal system, we select the best-performing models on both datasets.\\nThe best model for Eliteserien is VGG-inspired with an input resolution of 54 × 96 × 1\\n(100% F1-score), and the best model for SoccerNet is ResNet with an input resolution of\\n108 × 192 × 3 (99.7% F1-score).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 8}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n998\\nTable 3. Logo detection execution times.\\nModel\\nInput\\nFPS\\nResNet\\n144 × 256 × 3\\n1798\\nResNet\\n108 × 192 × 3\\n3117\\nResNet\\n54 × 96 × 3\\n12,295\\nVGG-inspired\\n144 × 256 × 3\\n94,169\\nVGG-inspired\\n144 × 256 × 1\\n96,428\\nVGG-inspired\\n108 × 192 × 3\\n65,281\\nVGG-inspired\\n72 × 72 × 1\\n85,722\\nVGG-inspired\\n54 × 96 × 3\\n86,594\\nSimple CNN\\n144 × 256 × 3\\n340,936\\nSimple CNN\\n108 × 192 × 3\\n341,897\\nSimple CNN\\n72 × 72 × 3\\n339,099\\nSVM (VGG16)\\n108 × 192 × 3\\n442\\nSVM (VGG16)\\n72 × 72 × 3\\n3172\\nSVM (simple CNN)\\n108 × 192 × 3\\n179\\nSVM (simple CNN)\\n72 × 72 × 3\\n1103\\nSVM (simple CNN)\\n72 × 72 × 1\\n883\\nSVM (simple CNN)\\n27 × 48 × 3\\n14,023\\n4.5. Scene Boundary Detection Performance\\nAs mentioned in Section 3, we used the TransNet V2 [10] model for the task of scene\\nboundary detection, which comes with a pre-trained model. We ﬁrst compared the pre-\\ntrained model with the model trained on our soccer clips exclusively.\\nThe SoccerNet scene boundary detection (SBD) dataset is a set of clips made from\\nall shot boundary transitions in SoccerNet [39], classiﬁed into abrupt, smooth, and logo\\ntransitions. TransNet V2 takes 100 frames as input in 48 × 27 resolution, and we therefore\\nextracted 100 frames from each shot boundary. To make it more robust towards the variable\\nplacement of the boundary frame, we randomly selected a frame between frames 30 and 60,\\nwhich were considered the shot boundary. We also made sure that close shot boundaries\\nwere also annotated for each of our clips. The dataset contains over 150,000 shot boundaries,\\nwith 43,000 logo transitions, 85,000 abrupt transitions, 28,000 smooth transitions, and 153\\nlabeled “other”. The dataset uses the same train, validation, and test split as provided\\nby [3] in order to evaluate full-length matches as well. For the presented results, we used a\\ntolerance of a predicted value of δ = 24 frames, meaning the prediction must be within a\\ndistance of 24 frames of the actual scene change to be considered a true positive (correct\\nprediction).\\nWe start by testing on the SoccerNet SBD dataset to identify the performance on\\nextracted scene changes. The results are presented in Table 4. There are only small\\ndifferences, but we can observe that the pre-trained model performs better than the model\\ntrained from scratch on all classes except for the abrupt class. On the abrupt class with the\\ntrained model, we observe a higher recall than that of the pre-trained model. We experience\\nthe same when we combine abrupt and gradual transitions in one metric. In the context of\\nthe function the component has in our system, we prioritize precision over recall, as the\\nrecall is high for both. The consequences of a false positive are more severe than those of a\\nfalse negative, as it may result in a clip in the middle of a scene sequence. A false negative\\nwill lead to the system making a default cut, but false positives will potentially fool the\\nsystem into including/excluding scenes that it is not supposed to.\\nTo determine how accurately the model can predict scene changes on the entire videos,\\nnot only the single boundary clips, we repeated the experiment on the full SoccerNet\\ntest set, containing 100 full-length games. The results are shown in Table 5. The recall\\nperformance is in line with the previous test, but we observe lower scores in general.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 9}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n999\\nTable 4. Performance of scene detection for each transition type (abrupt scene transition, gradual\\nscene transition, logo transition) on our SoccerNet SBD test set.\\nMetric\\nWeights\\nAll\\nAbrupt\\nGrad\\nLogo\\nPrecision\\nSoccerNet\\n95.66%\\n98.37%\\n97.66%\\n76.80%\\nPre-trained\\n95.96%\\n98.73%\\n98.69%\\n77.26%\\nRecall\\nSoccerNet\\n80.35%\\n96.63%\\n87.51%\\n33.24%\\nPre-trained\\n80.67%\\n95.58%\\n89.19%\\n36.06%\\nF1-score\\nSoccerNet\\n87.34%\\n97.49%\\n92.31%\\n46.40%\\nPre-trained\\n87.65%\\n97.13%\\n93.70%\\n49.17%\\nTable 5. Boundary detection results for full-length video test set.\\nWeights\\nPrecision\\nRecall\\nF1-Score\\nSoccerNet\\n45.63%\\n78.08%\\n57.60%\\nPre-trained\\n46.88%\\n78.85%\\n58.80%\\nWhen analyzing the results per game, we observed an almost perfect score of above\\n90% for some, while models predicted hundreds of false positives for others. This may be\\ndue to various match properties, such as team jersey and advertising board colors, lighting\\nconditions, production style, etc., which caused a model to detect false positives. It is\\nalso possible that the quality of some of the annotations was poor, as false positives were\\nclustered in the same matches. However, to investigate further, we manually reviewed\\nsome of the false positives and false negatives predicted by the pre-trained model.\\nWe started with the matches from the test set from Premier League season 2016–2017.\\nMore carefully checking a large number of “false predictions”, we observed that several of\\nthem were wrongly classiﬁed as false; i.e., the annotators seem to have missed or omitted\\nseveral scene changes in the annotation process. To identify more details, we analyzed\\n10 random soccer periods from 10 different games. After thoroughly considering 1378 false\\npositives, we surprisingly learned that only 2 of them were actual errors. Almost all cases\\nof the false negatives we observed were abrupt or smooth transitions. Moreover, looking\\nat the false positives in Figure 4, it is possible to observe the change of the background,\\nwhich may easily be misclassiﬁed as an abrupt or fade transition. For false negatives (scene\\nboundaries that the model misses to identify), we observed that these were mostly logos\\nand smooth transitions. As illustrated in Figure 5, these are very challenging cases.\\nFigure 4. The predicted false positive scene changes, where there are close similarities to abrupt and fade transitions.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 10}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1000\\nFigure 5. Some of the transitions the model misses.\\nOverall, we conclude that our scene boundary detection component has a good\\nperformance, and due to the slightly higher values for the pre-trained model, we use this\\nmodel in our ﬁnal pipeline. It should be noted here that in order to remain consistent\\nand comparable with existing studies using the same well-established and benchmarked\\ndataset, we chose to use our results based on the SoccerNet dataset as it is (without\\nmodiﬁcation, e.g., with respect to potentially mislabeled samples) in our ﬁnal pipeline.\\n4.6. Optional Trimming Performance\\nOur system can optionally remove cheering and celebration scenes to further reduce\\nthe length of a highlight clip. This is done by identifying scene changes between the main\\ngoal event and the replays, which often start with a logo transition. The ﬁrst goal scene is\\nalmost always from a single camera, meaning that all camera/scene changes before the\\nlogo transition are most likely spectators cheering and players celebrating.\\nAfter the goal, we include X seconds of celebration. Empirically derived, we search for\\na scene change between 5–10 s after the goal, but if not found, we clip after 8 s by default.\\nIn order for the replay clips not to come in abruptly, we also include a part of the last scene\\nbefore the replay begins; i.e., up to the last Y seconds of the last scene before the replay\\nis included, with a maximum of 5 s. Our assumption is that a replay always comes with\\na logo transition before or after. If two logo transitions are detected, we assume that the\\nreplay begins after the ﬁrst.\\n4.7. Final Pipeline\\nIn our ﬁnal pipeline, we combine the three components described above. In Section 4.4,\\nwe have shown that the logo transition detection component performs well on both logo\\nframe datasets. The best model for Eliteserien is VGG-inspired with an input resolution\\nof 54 × 96 × 1 (100% F1-score), and the best model for SoccerNet is ResNet with an input\\nresolution of 108 × 192 × 3 (99.7% F1-score). We use these models in our ﬁnal pipeline. In\\nSection 4.5, we achieved good results from training TransNet V2 [10] on SoccerNet; however,\\nit did not outperform the pre-trained model trained on ClipShots [11] and IACC.3 [12]. We\\ntherefore use the pre-trained model in our ﬁnal pipeline. In Section 4.6, we described our\\ninitial tests to ﬁnd suitable thresholds for trimming the video clip further by removing\\nframes between the goal event and the replay. This component was designed by examining\\nthe production patterns in real soccer broadcasts. Clipping at scene changes can improve\\nthe quality by avoiding the scenario where a clip starts a few frames before a scene change.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 11}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1001\\nThis utilizes the broadcast production as well, as a scene changes often happen when\\nsomething exciting happens.\\nOur clipping system is depicted in Figure 6. The input to the system is a frame\\nsequence containing an annotated event from the ﬁrst-level tagging operation, with enough\\nvideo available on both sides of the event’s point in time. The system ﬁrst identiﬁes logo\\nand scene boundary transitions, which are marked with timestamps. These timestamps are\\nthen used by a video processing module, which converts the input frame sequence into a\\nﬁnal highlight clip based on the clipping protocol described in Algorithm 1.\\nFigure 6. An overview of our ﬁnal pipeline, over an example frame sequence where the time of the event is given as input.\\nFrames where a cutting operation would be appropriate are found using SBD and logo transition detection, and the overall\\nclipping protocol is described in Algorithm 1.\\nAs shown in Figure 6, the SBD component is run from the start of the frame sequence\\nand identiﬁes a set of temporal anchor points (timestamps) that are potential clipping\\npoints. The logo transition detection component is run between the event itself and the\\nend of the frame sequence and returns temporal anchor points that are potential clipping\\npoints for the end of the highlight clip. Together with the temporal anchor point of the\\nannotated event itself (manually tagged by the ﬁrst-level operator), these anchor points are\\ninputs to the clipping procedure described in Algorithm 1 as scene change transitions,\\nlogo transitions, event, and the actual clipping is undertaken by the module shown\\nas “video processing” in Figure 6.\\nThe clipping protocol works as follows (Algorithm 1): it determines the start point\\nbased on the scene that starts 12 to 5 s (startInterval) before the goal event. It chooses\\nthe scene furthest from the event. If there is no scene change, a default value of 10 s\\n(defaultStart) is used. The end point is chosen to be in the middle of the last logo\\ntransition, or if no logo transition is found, as the last scene between 8 s (defaultCut) and\\n25 s (defaultEnd) after the event. The defaultEnd value is used if no scene change or\\nlogo transition is found. If the option to trim celebration scenes is enabled, the ﬁrst 5–10 s\\n(cutInterval) after the event are searched for a scene change to identify the start of the\\ncut. If no scene change is found, then the defaultCut value is used as the start of the cut.\\nAll frames between this point and the ﬁrst logo transition are cut.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 12}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1002\\nAlgorithm 1: Clipping protocol for goal events.\\nInput\\n:list of temporal anchors (scene change transitions, logo transitions, event),\\nboolean for optional trimming of cheering scenes (cutCrowd),\\ntemporal thresholds (startInterval, cutInterval),\\ntemporal defaults (de f aultStart, de f aultEnd, de f aultCut)\\nOutput:temporal anchors (start, end, cuts)\\n1 start = defaultStart;\\n2 end = defaultEnd;\\n3 cuts = None;\\n4 for each scene change before event do\\n5\\nif scene change in startInterval then\\n6\\nstart = scene change;\\n7\\nbreak;\\n8\\nend\\n9 end\\n10 if logo trainsitions are found then\\n11\\nend = last logo transition ;\\n12\\nif cutCrowd is true then\\n13\\ncuts = frames between [scene change in cutInterval OR else at de f aultCut] AND ﬁrst logo\\ntransition;\\n14\\nend\\n15 else\\n16\\nfor each scene change after event do\\n17\\nif scene change is between de f aultCut and de f aultEnd then\\n18\\nend = scene change;\\n19\\nbreak;\\n20\\nend\\n21\\nend\\n22 end\\n23 return start, end, cuts;\\n5. Subjective Evaluation\\nThe quality of a highlight clip is strongly subjective. Therefore, we also evaluated the\\nperformance of our pipeline in terms of end-user perception, in comparison to the static\\nclipping method used by the industry today. We ran a subjective evaluation campaign via\\nan online survey, where the participants were asked to visually compare and score different\\nclipping methods. In particular, our goal was to benchmark the static clipping method with\\nour pipeline (automatically generated highlight clips, with and without cheering scenes).\\nIn total, 64 people participated in a user study, giving their consent for the collection\\nof their metadata and answers. Participant metadata consisted of information about the\\nparticipants such as age, gender, whether they have an interest in sports/soccer, their\\nviewing habits, and whether they have video editing experience. In order to limit the\\nlength of our user study to 10–12 min, we randomly selected ﬁve events representing\\ndifferent goal situations as listed in Table 6. We ran the following algorithms in various\\ncombinations on these ﬁve events:\\n•\\nOriginal: The default static clipping of −A and +B seconds around the identiﬁed\\npoint in time where the event happened;\\n•\\nOur model—full: Automatic clipping using logo transition and scene boundary\\ndetection;\\n•\\nOur model—short: Automatic clipping using logo transition and scene boundary\\ndetection, where we also shortened the clip by removing cheering and celebration\\nscenes.\\nIn each question corresponding to one event, a brief description of the event was\\nprovided, and the participants were asked to rate two alternative clips on a scale between 1\\n(very poor) to 10 (broadcast ready) and optionally give comments. Overall, all participants\\nassessed the same 10 clips, in the same pairwise fashion, to provide trustful results and\\ndirect comparisons. After removing 3 outliers (participants who responded with a max-\\nimum rating to all clips), we ended up with 61 participants, male and female, in an age\\nrange between 19 and 59 years old.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 13}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1003\\nTable 6. Model comparisons in the subjective evaluation.\\nComparison ID\\nModel\\nvs.\\nModel\\n1 (corner goal)\\nOriginal\\nOur model—short\\n2 (counter-attack goal)\\nOur model—short\\nOur model—full\\n3 (cross-ball goal)\\nOriginal\\nOur model—full\\n4 (penalty goal)\\nOriginal\\nOur model—full\\n5 (corner goal)\\nOriginal\\nOur model—short\\nFigure 7 presents the results from our study, in the form of an A/B test. The overall\\nscores over all participants are shown in Figure 7a. As we can observe, our models received\\nbetter scores over all comparisons, and the additional clipping that removed cheering\\nscenes produced the most preferred highlight clips. Our “short” model received an average\\nscore of 7.40, our “full” model an average of 6.84, and the original model an average of\\n5.89. Details about the scores for each participant class can be found in Table 7. Among the\\nparticipant class (b), those with video editing experience, two participants were professional\\neditors. Both conﬁrmed that our models result in a signiﬁcant improvement over the\\noriginal; i.e., giving our models 1.58 and 3.08 higher average scores on the Likert scale,\\nrespectively. However, the results also show that there is still some room for improvement.\\nConsidering the additional features often employed by traditional broadcast productions,\\nsuch as scene fading, custom transitions, new audio tracks targeted for speciﬁc clips, etc.,\\nto improve viewer experience, it can be said that automated pipelines have some room to\\nimprove.\\nTable 7. Detailed scores per A/B test in the subjective evaluation.\\nParticipant Class\\nModel Name\\nAverage Score\\nStandard Deviation\\nMedian\\n(a) Overall\\nOur model—Short\\n7.40\\n1.98\\n8\\nOur model—Full\\n6.84\\n2.10\\n7\\nOriginal\\n5.89\\n2.12\\n6\\n(b) Video editing experience\\nOur model—Short\\n7.59\\n2.33\\n8\\nOur model—Full\\n6.67\\n2.42\\n7\\nOriginal\\n5.47\\n2.42\\n5\\n(c) Soccer fans\\nOur model—Short\\n7.24\\n2.00\\n8\\nOur model—Full\\n6.72\\n1.88\\n7\\nOriginal\\n5.82\\n1.98\\n6\\n(d) Not soccer fans\\nOur model—Short\\n7.55\\n1.96\\n8\\nOur model—Full\\n6.95\\n2.29\\n7\\nOriginal\\n5.96\\n2.24\\n6\\n(e) Sport fans\\nOur model—Short\\n7.20\\n1.93\\n8\\nOur model—Full\\n6.57\\n1.89\\n7\\nOriginal\\n5.71\\n1.95\\n5\\n(f) Not sport fans\\nOur model—Short\\n8.07\\n2.03\\n9\\nOur model—Full\\n7.71\\n2.48\\n9\\nOriginal\\n6.50\\n2.53\\n6\\n(g) Females\\nOur model—Short\\n7.42\\n1.85\\n8\\nOur model—Full\\n7.18\\n1.90\\n7\\nOriginal\\n5.98\\n2.09\\n6\\n(h) Males\\nOur model—Short\\n7.39\\n2.03\\n8\\nOur model—Full\\n6.72\\n2.15\\n7\\nOriginal\\n5.86\\n2.13\\n6\\n(i) Age 29 and below\\nOur model—Short\\n7.31\\n2.01\\n8\\nOur model—Full\\n6.79\\n2.06\\n7\\nOriginal\\n5.86\\n2.18\\n6\\n(j) Age 30 and above\\nOur model—Short\\n7.93\\n1.75\\n9\\nOur model—Full\\n7.11\\n2.31\\n8\\nOriginal\\n6.06\\n1.72\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 14}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1004\\n1\\n2\\n3\\n4\\n5\\n0\\n20\\n40\\n4\\n17\\n11\\n21\\n53\\n30\\n16\\n25\\n36\\n4\\n16\\n19\\n14\\n10\\n29\\nVotes\\n(a)\\n1\\n2\\n3\\n4\\n5\\n0\\n5\\n10\\n15\\n0\\n5\\n3\\n6\\n16\\n9\\n6\\n7\\n12\\n1\\n4\\n5\\n2\\n2\\n7\\nVotes\\n(b)\\n1\\n2\\n3\\n4\\n5\\n0\\n10\\n20\\n3\\n6\\n6\\n12\\n25\\n13\\n10\\n12\\n17\\n2\\n10\\n12\\n7\\n5\\n14\\nVotes\\n(c)\\n1\\n2\\n3\\n4\\n5\\n0\\n10\\n20\\n30\\n1\\n11\\n5\\n9\\n28\\n17\\n10\\n13\\n19\\n2\\n6\\n7\\n7\\n5\\n15\\nVotes\\n(d)\\n1\\n2\\n3\\n4\\n5\\n0\\n20\\n40\\n3\\n15\\n8\\n18\\n41\\n22\\n11\\n18\\n26\\n3\\n11\\n14\\n13\\n7\\n25\\nVotes\\n(e)\\n1\\n2\\n3\\n4\\n5\\n0\\n5\\n10\\n1\\n2\\n3\\n3\\n12\\n8\\n5\\n7\\n10\\n1\\n5\\n5\\n1\\n3\\n4\\nVotes\\n(f)\\n1\\n2\\n3\\n4\\n5\\n0\\n10\\n20\\n30\\n1\\n4\\n2\\n5\\n14\\n8\\n5\\n8\\n12\\n0\\n1\\n3\\n1\\n2\\n29\\nVotes\\n(g)\\n1\\n2\\n3\\n4\\n5\\n0\\n10\\n20\\n30\\n40\\n3\\n13\\n9\\n16\\n39\\n22\\n11\\n17\\n24\\n4\\n15\\n16\\n13\\n8\\n29\\nVotes\\n(h)\\n1\\n2\\n3\\n4\\n5\\n0\\n20\\n40\\n4\\n16\\n10\\n20\\n45\\n25\\n15\\n22\\n30\\n3\\n11\\n14\\n12\\n7\\n29\\nVotes\\n(i)\\n1\\n2\\n3\\n4\\n5\\n0\\n2\\n4\\n6\\n8\\n10\\n0\\n1\\n1\\n1\\n8\\n5\\n1\\n3\\n6\\n1\\n5\\n5\\n2\\n3\\n3\\nVotes\\nOriginal\\nShort (Ours)\\nFull (Ours)\\nNo Preference\\n(j)\\nFigure 7. Subjective evaluation: preferred model per question for different participant classes. Note that two clips are\\ncompared per question. The bar legend is shown in the bottom right plot. (a) Overall (61 persons). (b) Video editing\\nexperience (17 persons). (c) Soccer fans (30 persons). (d) Not soccer fans (31 persons). (e) Sport fans (47 persons). (f) Not\\nsport fans (14 persons). (g) Females (15 persons). (h) Males (46 persons). (i) Age 29 and below (52 persons). (j) Age 30 and\\nabove (9 persons).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 15}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1005\\n6. Discussion\\nOur proposed pipeline generates highlight clips that are accurate in terms of capturing\\nlogo transition and scene change boundaries, as well as being better perceived by viewers\\ncompared to traditional clipping methods. However, there was an open issue that we\\nwould like to address in this section.\\nTaking into account the results from Section 4.4 for Eliteserien and comparing the\\nperformance of various models on the validation and test sets, we identiﬁed a possible\\noverﬁtting problem. Figure 8 presents the learning curves for Simple CNN and ResNet,\\nwhere ResNet training ran for 12 epochs, and the simple CNN ran for 14 epochs. For the\\nResNet model (Figure 8c,d), the validation loss stops improving already after two epochs,\\nwhile the training loss keeps improving. This suggests overﬁtting. The simple CNN model\\nhas a steadier decrease of loss before it stops improving afterseven epochs. This can be the\\nresult of a small dataset and low complexity.\\n(a)\\n(b)\\n(c)\\n(d)\\nFigure 8. Overﬁtting analysis: comparing training and validation loss (low is better) and accuracy (high is better).\\n(a) Accuracy: Simple CNN. (b) Loss: Simple CNN. (c) Accuracy: ResNet. (d) Loss: ResNet.\\nFocusing in detail on the logos that are missed, we observed that most are within the\\nﬁrst ﬁve frames of a team logo transition, and we noticed that these team logos are not\\npresent in the training set. To tackle this problem, we generated a synthetic dataset to train\\nthe models to recognize all team logos. We created a logo collection by extracting images\\nfrom both Eliteserien and Premier League using the FFmpeg tool and then the surrounding\\nframes to obtain the logo transition. Each lasted for 20 frames in total, 10 of which were\\nfade-in, 5 fully covered the logo, and 5 were fade-outs. For a number of selected models,\\nwe checked the validation results from training on the dataset supplemented with synthetic\\nlogo frames. The results of this experiment demonstrated that training with the original\\ntraining set together with our synthetic logo images led to an increase of recall and helped\\nthe models to recognize team logos that were not encountered during training. However,\\nwe also observed a decrease in precision for all models, as more backgrounds were now\\nmisclassiﬁed. Although overﬁtting can be a signiﬁcant problem for some tasks, there is\\na limit to how much harm it can do for the task of logo detection, due to most frames\\nin the transition being very similar. We observed that the models only misclassify the\\nearliest frames while still hitting all the rest. Since all the frames later in the transition\\nwere correctly classiﬁed, we did not consider the trade-off to be worth the change in\\nthis case and decided not to go forward with these models. It should also be noted that\\nthe results of our study are hard to generalize across different broadcaster domains and\\nproduction environments (e.g., different leagues). As can be seen from Tables 1 and 2,\\neven within the same content domain (soccer video), different models can be optimal for\\ndifferent datasets (Eliteserien and Premier League). Model conﬁgurations and training\\ndepend heavily on the target soccer league, as different leagues even have different logo\\ntransitions, and there are different production protocols on how scenes and replays are\\npresented. Therefore, one cannot necessarily rely on the current version of our pipeline\\nin plug-and-play form, and our selected models may need to be re-trained with relevant\\ndatasets from the corresponding soccer league.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 16}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1006\\n7. Conclusions\\nAn AI-based production pipeline can be of great value for broadcasters and streaming\\nservices that provide sports videos. In this context, we have experimented with automating\\nthe process of event clipping from soccer videos, using logo transition and scene boundary\\ndetection, and optional trimming. We found that different ML models work best for\\ndifferent datasets (soccer leagues from different countries). For logo detection, a VGG-\\ninspired CNN using a grayscale input resolution of 54 × 96 was best for the Norwegian\\nEliteserien dataset, achieving a 100% F1-score, but for the more complex English Premier\\nLeague logo dataset, a ResNet using an RGB input of resolution 108 × 192 was the best ﬁt,\\nachieving an 0.997 F1-score. Regarding scene boundary detection, we trained and evaluated\\nTransNet-V2 [10] on the SoccerNet shot boundary dataset but found the performance of the\\npre-trained version to be better, yielding an F1-score of 0.877. We combined these models\\nin an end-to-end event clipping pipeline. Running a subjective user study, we saw that\\nour pipeline could consistently produce more compelling highlight clips compared to the\\ntraditional method based on static clipping, which is currently used in Eliteserien.\\nBased on objective metrics as well as a subjective evaluation, we showed that an\\nautomated pipeline is able to provide highlight clips of a reliable standard. We also\\nidentiﬁed that what is considered a compelling highlight clip is subjective, and there might\\nbe differences in the production strategy preferred by different viewers. Nevertheless, our\\nwork gives a strong foundation and motivation for future studies using ML to generate\\nautomatic highlight clips from soccer videos. AI-based automated systems can potentially\\nsave a great deal of manual resources and reduce costs for existing tagging centers, as well\\nas facilitating the deployment of such centers for lower level leagues which do not have\\nsufﬁcient resources in the ﬁrst place.\\nThere are a number of aspects of our work that we plan to improve as future work.\\nThese include the handling of different types and even sub-types of events (e.g., different\\nevents in the same category such as corner goals, penalty goals, and counter-attack goals,\\nas well as altogether different events such as cards and substitutions), the investigation of\\nmultiple modalities for event detection and clipping (e.g., using stadium and commentary\\naudio alongside video), the exploration of different scene transitions (e.g., fade-in and\\nfade-out at the cuts), and a large-scale benchmark study to compare against professionally\\nclipped events in order to solidify the effectiveness of our system.\\nAuthor Contributions:\\nConceptualization, J.O.V., H.K., S.A.H., M.A.R. and P.H.; methodology,\\nJ.O.V., H.K., S.A.H., M.A.R. and P.H.; software, J.O.V. and H.K.; validation, J.O.V., H.K., S.A.H.,\\nM.A.R. and P.H.; investigation, all authors; resources, J.O.V., H.K., S.A.H., T.K., D.J., M.A.R. and\\nP.H.; data curation, J.O.V., H.K. and P.H.; writing, all authors; visualization, all authors; supervision,\\nS.A.H., V.L.T., D.J., M.A.R. and P.H.; funding acquisition, M.A.R. and P.H. All authors have read and\\nagreed to the published version of the manuscript.\\nFunding: This research was funded by the Norwegian Research Council, project number 327717\\n(AI-producer).\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: The SoccerNet dataset is made available by Giancola et al. [3].\\nAcknowledgments: The research has beneﬁted from the Experimental Infrastructure for Exploration\\nof Exascale Computing (eX3), which is ﬁnancially supported by the Research Council of Norway\\nunder contract 270053. We also acknowledge the use of video data from Norsk Toppfotball (NTF).\\nConﬂicts of Interest: T.K., D.J. and P.H. have interests in Forzasys AS, but the current study is not\\npart of any product or service from this company.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 17}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1007\\nReferences\\n1.\\nFIFA.com. More Than Half the World Watched Record-Breaking 2018 World Cup. 2018. Available online: https://www.ﬁfa.com/\\nworldcup/news/more-than-half-the-world-watched-record-breaking-2018-world-cup (accessed on 4 December 2021).\\n2.\\nWhy the Sports Industry is Booming in 2020 (and Which Key Players Are Driving Growth). 2020. Available online: https://www.\\ntorrens.edu.au/blog/why-sports-industry-is-booming-in-2020-which-key-players-driving-growth (accessed on 4 December\\n2021).\\n3.\\nGiancola, S.; Amine, M.; Dghaily, T.; Ghanem, B. SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos.\\nIn\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, Salt Lake City, UT, USA,\\n18–22 June 2018; pp. 1711–1721. [CrossRef]\\n4.\\nKapela, R.; McGuinness, K.; Swietlicka, A.; O’Connor, N.E. Real-Time Event Detection in Field Sport Videos. In Proceedings of\\nCVPR; Springer: Cham, Switzerland, 2014. [CrossRef]\\n5.\\nCioppa, A.; Deliege, A.; Giancola, S.; Ghanem, B.; Droogenbroeck, M.; Gade, R.; Moeslund, T. A Context-Aware Loss Function for\\nAction Spotting in Soccer Videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR), Seattle, WA, USA, 14–19 June 2020. [CrossRef]\\n6.\\nTomei, M.; Baraldi, L.; Calderara, S.; Bronzin, S.; Cucchiara, R. RMS-Net: Regression and Masking for Soccer Event Spotting.\\narXiv 2021, arXiv:2102.07624.\\n7.\\nRongved, O.A.N.; Hicks, S.A.; Thambawita, V.; Stensland, H.K.; Zouganeli, E.; Johansen, D.; Riegler, M.A.; Halvorsen, P. Real-\\nTime Detection of Events in Soccer Videos using 3D Convolutional Neural Networks. In Proceedings of the IEEE International\\nSymposium on Multimedia (ISM), Naples, Italy, 2–4 December 2020; pp. 135–144. [CrossRef]\\n8.\\nHe, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. arXiv 2015, arXiv:1512.03385.\\n9.\\nSimonyan, K.; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv 2015, arXiv:1409.1556.\\n10.\\nSouˇcek, T.; Lokoˇc, J. TransNet V2: An effective deep network architecture for fast shot transition detection. arXiv 2020,\\narXiv:2008.04838.\\n11.\\nTang, S.; Feng, L.; Kuang, Z.; Chen, Y.; Zhang, W. Fast Video Shot Transition Localization with Deep Structured Models. arXiv\\n2018, arXiv:1808.04234.\\n12.\\nAwad, G.; Butt, A.A.; Curtis, K.; Lee, Y.; Fiscus, J.; Godil, A.; Delgado, A.; Zhang, J.; Godard, E.; Diduch, L.; et al. TRECVID 2020:\\nComprehensive campaign for evaluating video retrieval tasks across multiple application domains. In Proceedings of TREC Video\\nRetrieval Evaluation (TRECVID); NIST: Gaithersburg, MD, USA, 2020.\\n13.\\nValand, J.O.; Haris, K.; Hicks, S.A.; Thambawita, V.; Midoglu, C.; Kupka, T.; Johansen, D.; Riegler, M.A.; Halvorsen, P. Automated\\nClipping of Soccer Events using Machine Learning. In Proceedings of the IEEE International Symposium on Multimedia (ISM),\\nNaples, Italy, 6–8 December 2021.\\n14.\\nKarpathy, A.; Toderici, G.; Shetty, S.; Leung, T.; Sukthankar, R.; Fei-Fei, L. Large-Scale Video Classiﬁcation with Convolutional\\nNeural Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Columbus, OH,\\nUSA, 23–28 June 2014; pp. 1725–1732. [CrossRef]\\n15.\\nSimonyan, K.; Zisserman, A. Two-Stream Convolutional Networks for Action Recognition in Videos. In Proceedings of the\\nAdvances in Neural Information Processing Systems (NIPS), Montreal, QC, Canada, 8–13 December 2014; pp. 568–576.\\n16.\\nFeichtenhofer, C.; Pinz, A.; Zisserman, A. Convolutional Two-Stream Network Fusion for Video Action Recognition.\\nIn\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27–30 June 2016;\\npp. 1933–1941. [CrossRef]\\n17.\\nCarreira, J.; Zisserman, A.Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 4724–4733.\\n18.\\nShou, Z.; Wang, D.; Chang, S.F. Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27–30 June 2016; pp. 1049–1058.\\n[CrossRef]\\n19.\\nWang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.; Gool, L.V. Temporal Segment Networks: Towards Good Practices for\\nDeep Action Recognition. In Proceedings of the European Conference Computer Vision (ECCV), Amsterdam, The Netherlands,\\n11–14 October 2016; pp. 20–36.\\n20.\\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; Paluri, M. Learning Spatiotemporal Features with 3D Convolutional Networks. In\\nProceedings of the IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 7–13 December 2015; pp. 4489–4497.\\n[CrossRef]\\n21.\\nTran, D.; Wang, H.; Torresani, L.; Ray, J.; LeCun, Y.; Paluri, M. A Closer Look at Spatiotemporal Convolutions for Action\\nRecognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT,\\nUSA, 18–22 June 2018; pp. 6450–6459. [CrossRef]\\n22.\\nIdrees, H.; Zamir, A.R.; Jiang, Y.; Gorban, A.; Laptev, I.; Sukthankar, R.; Shah, M. The THUMOS challenge on action recognition\\nfor videos “in the wild”. Comput. Vis. Image Underst. 2017, 155, 1–23. [CrossRef]\\n23.\\nRen, S.; He, K.; Girshick, R.; Sun, J. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In\\nProceedings of the International Conference on Neural Information Processing Systems (NIPS); Montreal, QC, Canada 7–12\\nDecember 2015; pp. 91–99.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-12-15T14:26:42+08:00', 'source': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'file_path': '../data/pdf_files/04.AI-Based Video Clipping of Soccer Events.pdf', 'total_pages': 19, 'format': 'PDF 1.7', 'title': 'AI-Based Video Clipping of Soccer Events', 'author': 'Joakim Olav Valand, Haris Kadragic, Steven Alexander Hicks, Vajira Thambawita, Cise Midoglu, Evi Zouganeli, Dag Johansen, Michael Alexander Riegler and Pål Halvorsen', 'subject': 'The current gold standard for extracting highlight clips from soccer games is the use of manual annotations and clippings, where human operators define the start and end of an event and trim away the unwanted scenes. This is a tedious, time-consuming, and expensive task, to the extent of being rendered infeasible for use in lower league games. In this paper, we aim to automate the process of highlight generation using logo transition detection, scene boundary detection, and optional scene removal. We experiment with various approaches, using different neural network architectures on different datasets, and present two models that automatically find the appropriate time interval for extracting goal events. These models are evaluated both quantitatively and qualitatively, and the results show that we can detect logo and scene transitions with high accuracy and generate highlight clips that are highly acceptable for viewers. We conclude that there is considerable potential in automating the overall soccer video clipping process.', 'keywords': 'event clipping; deep learning; logo transition; scene boundary detection; soccer; sports analysis; video', 'moddate': '2021-12-15T08:53:36+01:00', 'trapped': '', 'modDate': \"D:20211215085336+01'00'\", 'creationDate': \"D:20211215142642+08'00'\", 'page': 18}, page_content='Mach. Learn. Knowl. Extr. 2021, 3\\n1008\\n24.\\nXu, H.; Das, A.; Saenko, K. R-C3D: Region Convolutional 3D Network for Temporal Activity Detection. In Proceedings of the\\nIEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22–29 October 2017.\\n25.\\nLin, T.; Liu, X.; Li, X.; Ding, E.; Wen, S. BMN: Boundary-Matching Network for Temporal Action Proposal Generation. In\\nProceedings of the IEEE International Conference on Computer Vision (ICCV), Seoul, Korea, 27–28 October 2019.\\n26.\\nLin, T.; Zhao, X.; Su, H.; Wang, C.; Yang, M. BSN: Boundary Sensitive Network for Temporal Action Proposal Generation. In\\nProceedings of the European Conference Computer Vision (ECCV), Munich, Germany, 8–14 September 2018.\\n27.\\nKoumaras, H.; Gardikis, G.; Xilouris, G.; Pallis, E.; Kourtis, A. Shot boundary detection without threshold parameters. J. Electron.\\nImaging 2006, 15, 020503. [CrossRef]\\n28.\\nZawbaa, H.M.; El-Bendary, N.; Hassanien, A.E.; Abraham, A. SVM-based soccer video summarization system. In Proceedings of\\nthe World Congress on Nature and Biologically Inspired Computing, Salamanca, Spain, 19–21 October 2011; pp. 7–11. [CrossRef]\\n29.\\nZawbaa, H.; El-Bendary, N.; Hassanien, A.E.; Kim, T.H. Event Detection Based Approach for Soccer Video Summarization Using\\nMachine learning. Int. J. Multimed. Ubiquitous Eng. (IJMUE) 2012, 7, 63–80.\\n30.\\nXu, P.; Xie, L.; Chang, S.F.; Divakaran, A.; Vetro, A.; Sun, H. Algorithms and system for segmentation and structure analysis in\\nsoccer video. In Proceedings of the IEEE International Conference on Multimedia and Expo (ICME), Tokyo, Japan, 22–25 August\\n2001; pp. 721–724. [CrossRef]\\n31.\\nRaﬁq, M.; Raﬁq, G.; Agyeman, R.; Jin, S.I.; Choi, G.S. Scene Classiﬁcation for Sports Video Summarization Using Transfer\\nLearning. Sensors 2020, 20, 1702. [CrossRef] [PubMed]\\n32.\\nRen, R.; Jose, J.M. Football Video Segmentation Based on Video Production Strategy.\\nIn Proceedings of ECIR—Advances in\\nInformation Retrieval; Springer: Berlin/Heidelberg, Germany, 2005; pp. 433–446.\\n33.\\nRaventos, A.; Quijada, R.; Torres, L.; Tarres, F. Automatic Summarization of Soccer Highlights Using Audio-visual Descriptors.\\narXiv 2014, arXiv:1411.6496.\\n34.\\nTjondronegoro, D.; Chen, Y.P.P.; Pham, B. Sports video summarization using highlights and play-breaks. In Proceedings of\\nthe ACM SIGMM International Workshop on Multimedia Information Retrieval (MIR), Berkeley, CA, USA, 7 November 2003;\\npp. 201–208. [CrossRef]\\n35.\\nChen, C.Y.; Wang, J.C.; Wang, J.F.; Hu, Y.H. Motion Entropy Feature and Its Applications to Event-Based Segmentation of Sports\\nVideo. EURASIP J. Adv. Signal Process. 2008, 2008, 460913. [CrossRef]\\n36.\\nBaraldi, L.; Grana, C.; Cucchiara, R. Shot and Scene Detection via Hierarchical Clustering for Re-using Broadcast Video. In\\nProceedings of the International Conference on Computer Analysis of Images and Patterns (CAIP), Valletta, Malta, 2–4 September\\n2015; pp. 801–811. [CrossRef]\\n37.\\nBaraldi, L.; Grana, C.; Cucchiara, R. A Deep Siamese Network for Scene Detection in Broadcast Videos. In Proceedings of the\\nACM conference on multimedia (ACM MM), Brisbane, Australia, 26–30 October 2015; pp. 1199–1202. [CrossRef]\\n38.\\nSouˇcek, T. Deep Learning-Based Approaches for Shot Transition Detection and Known-Item Search in Video. 2019. Available\\nonline: https://github.com/soCzech/MasterThesis (accessed on 4 December 2021).\\n39.\\nDeliège, A.; Cioppa, A.; Giancola, S.; Seikavandi, M.J.; Dueholm, J.V.; Nasrollahi, K.; Ghanem, B.; Moeslund, T.B.; Droogen-\\nbroeck, M.V. SoccerNet-v2: A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos. arXiv 2020,\\narXiv:2011.13367.\\n40.\\nKeras. ResNet and ResNetV2. 2021. Available online: https://keras.io/api/applications/resnet/ (accessed on 4 December 2021).\\n41.\\nChollet, F. Keras. 2015. Available online: https://keras.io (accessed on 4 December 2021).\\n42.\\nGlorot, X.; Bengio, Y. Understanding the difﬁculty of training deep feedforward neural networks. J. Mach. Learn. Res.—Proc.\\nTrack 2010, 9, 249–256.\\n43.\\nKingma, D.P.; Ba, J. Adam: A Method for Stochastic Optimization. arXiv 2017, arXiv:1412.6980.\\n44.\\nDeng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Miami, FL, USA, 22–24 June 2009; pp. 248–255.\\n[CrossRef]'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 0}, page_content='SIViP (2010) 4:61–73\\nDOI 10.1007/s11760-008-0097-9\\nORIGINAL PAPER\\nClassiﬁcation of sport videos using edge-based features\\nand autoassociative neural network models\\nC. Krishna Mohan · B. Yegnanarayana\\nReceived: 30 November 2006 / Revised: 10 May 2008 / Accepted: 15 November 2008 / Published online: 10 December 2008\\n© Springer-Verlag London Limited 2008\\nAbstract\\nIn\\nthis\\npaper,\\nwe\\npropose\\na\\nmethod\\nfor\\nclassiﬁcation of sport videos using edge-based features,\\nnamely edge direction histogram and edge intensity histo-\\ngram. We demonstrate that these features provide discrimi-\\nnative information useful for classiﬁcation of sport videos, by\\nconsidering ﬁve sports categories, namely, cricket, football,\\ntennis, basketball and volleyball. The ability of autoassocia-\\ntive neural network (AANN) models to capture the distribu-\\ntion of feature vectors is exploited, to develop class-speciﬁc\\nmodels using edge-based features. We show that combin-\\ning evidence from complementary edge features results in\\nimproved classiﬁcation performance. Also, combination of\\nevidence from different classiﬁers like AANN, hidden Mar-\\nkov model (HMM) and support vector machine (SVM) helps\\nimprove the classiﬁcation performance. Finally, the perfor-\\nmance of the classiﬁcation system is examined for test videos\\nwhich do not belong to any of the above ﬁve categories. A\\nlow rate of misclassiﬁcation error for these test videos val-\\nidates the effectiveness of edge-based features and AANN\\nmodels for video classiﬁcation.\\nKeywords\\nVideo classiﬁcation · Edge-based features ·\\nAutoassociative neural network model · Hidden Markov\\nmodel · Support vector machines\\nC. Krishna Mohan (B)\\nIndian Institute of Technology Madras,\\nChennai 600036, Tamil Nadu, India\\ne-mail: ckm@cs.iitm.ernet.in\\nB. Yegnanarayana\\nInternational Institute of Information Technology,\\nHyderabad 500032, Andhra Pradesh, India\\ne-mail: yegna@iiit.ac.in\\n1 Introduction\\nClassiﬁcation of digital videos into various genres, or\\ncategories is an important task, and enables efﬁcient catalog-\\ningandretrievalwithlargevideocollections.Theobjectiveof\\nvideo classiﬁcation is to classify a given video clip into one\\nof the predeﬁned video categories. Many approaches have\\nbeen proposed for content-based classiﬁcation of video data.\\nThe problem of content-based classiﬁcation of video can be\\naddressed at different levels in the semantic hierarchy. For\\ninstance, video collections can be categorized into differ-\\nent program genres such as news, commercials and sports.\\nThen, videos of a particular genre, such as sports, can be fur-\\ntherclassiﬁedintosub-categorieslikesoccer,hockey,cricket,\\netc. A video sequence of a given sub-category can then be\\npartitioned into smaller segments, and these segments can be\\nclassiﬁed into semantically meaningful classes.\\nIn this paper, we address the problem of sport videos clas-\\nsiﬁcation for ﬁve classes, namely, cricket, football, tennis,\\nbasketball and volleyball. Sports videos represent an impor-\\ntant application domain due to their commercial appeal. Clas-\\nsiﬁcation of sports video data is a challenging problem,\\nmainly due to the similarity between different sports in terms\\nof entities such as playing ﬁeld, players and audience. Also,\\nthere exists significant variation in the video of a given cat-\\negory collected from different TV programs/channels. This\\nintra-class variability contributes to the difﬁculty of classiﬁ-\\ncation of sports videos.\\nContent-based video classiﬁcation is essentially a pat-\\ntern classiﬁcation problem [1] in which there are two basic\\nissues, namely, feature extraction and classiﬁcation based\\non the selected features. Feature extraction is the process of\\nextracting descriptive parameters from the video, which will\\nbe useful in discriminating between classes of video. The\\nclassiﬁer operates in two phases: training and testing phase.\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 1}, page_content='62\\nSIViP (2010) 4:61–73\\nTraining is the process of familiarizing the system with the\\nvideo characteristics of a given category, and testing is the\\nactual classiﬁcation task, where a test video clip is assigned\\na class label.\\nSeveral audio–visual features have been described for\\ncharacterizing semantic content in multimedia [2]. The gen-\\neral approach to video classiﬁcation involves extraction of\\nvisual features based on color, shape, and motion, followed\\nby estimation of class-speciﬁc probability density function of\\nthe feature vectors [3,4]. A criterion based on the total length\\nof edges in a given frame is used in [5]. The edges are com-\\nputed by transforming each block of 8 × 8 pixels using dis-\\ncrete cosine transform (DCT), and then processing the DCT\\ncoefﬁcients. A rule-based decision is then applied to clas-\\nsify each frame into one of the predeﬁned semantic catego-\\nries. Another edge-based feature, namely, the percentage of\\nedge pixels, is extracted from each keyframe for classifying\\na given sports video into one of the ﬁve categories, namely,\\nbadminton, soccer, basketball, tennis, and ﬁgure skating [6].\\nThe k-nearest neighbor algorithm was used for classiﬁca-\\ntion.Motionisanotherimportantfeatureforrepresentationof\\nvideo sequences. A feature, called motion texture, is derived\\nfrom motion ﬁeld between video frames, either in optical\\nﬂow ﬁeld or in motion vector ﬁeld in [7]. These features\\nare employed in conjunction with support vector machines\\n(SVMs) to devise a set of multicategory classiﬁers.\\nThe approach described in [8] deﬁnes local measurements\\nof motion, whose spatio-temporal distributions are modeled\\nusing statistical nonparametric modeling. To exploit the\\nstrong correlation between the camera motion and the actions\\ntaken in sports, sports videos are categorized on the basis\\nof camera motion parameters [9]. The camera motion pat-\\nterns such as ﬁx, pan, zoom, and shake are extracted from\\nthe video data. Motion dynamics such as foreground object\\nmotion and background camera motion are extracted in [10]\\nfor classiﬁcation of a video sequence into three broad cate-\\ngories, namely, sports, cartoons and news. Transform coefﬁ-\\ncients derived from DCT and Hadamard transform of image\\nframes are reduced in dimension using principal compo-\\nnent analysis (PCA) [11]. The probability density function\\nof the compressed features is then modeled using a mixture\\nof Gaussian densities. Dimension reduction of low-level fea-\\ntures such as color and texture, using PCA, has also been\\nattempted in [12,13]. Another approach, described in [14],\\nconstructs two hidden Markov models (HMMs), one from\\nthe principal motion direction, and the other from the princi-\\npal color of each frame. The decisions from both the models\\nare combined to obtain the ﬁnal score for classiﬁcation. Apart\\nfrom the above statistical models, rule-based methods have\\nalso been applied for classiﬁcation. A decision tree method\\nis used in [15] to classify videos into different genres. For\\nthis purpose, several attributes are derived from the video\\nsequences, such as length of the video clip, number of shots,\\naverage shot length and percentage of cuts. A set of decision\\nrules is derived using these attributes.\\nEdges constitute an important feature to represent the\\ncontent of an image. Human visual system is sensitive to\\nedge-speciﬁc features for image perception. In sports video\\nclassiﬁcation, images that contain the playing ﬁeld are signif-\\nicant for distinguishing among the classes of sports. This is\\nbecause, each sport has its own distinct playing ﬁeld, where\\nmost of the action takes place. Also, the interaction among\\nsubjects (players, referees and audience) and objects (ball,\\ngoal, basket) is unique to each sport. A few sample images\\nof each sports category are shown in Fig. 1. The correspond-\\ning edge images are shown in Fig. 2. Each playing ﬁeld has\\nseveral distinguishing features such as lines present on the\\nplaying ﬁeld, and regions of different textures. The subjects\\nare also prominent in the images, thus helping in distinguish-\\ning different sports. From Fig. 2, we can observe that edge\\nfeatures are important for representing the content of sports\\nvideo, and also these features carry sufﬁcient information\\nfor discriminating among classes. These observations sug-\\ngest that features derived to represent the edge information\\ncan be of significant help for discriminating various catego-\\nries of sports.\\nIn this paper, we propose to make use of two edge-based\\nfeatures which provide complementary information for clas-\\nsiﬁcation of sports videos. We exploit the capability of auto-\\nassociative neural network models to capture the distribution\\nof the feature vectors. Two other classiﬁer methodologies,\\nnamely, HMMs and support vector machines (SVMs), are\\nemployed for their ability to capture the sequence infor-\\nmation and discriminative learning, respectively. Evidences\\nfrom these classiﬁers are combined to improve the perfor-\\nmance of video classiﬁcation.\\nThe paper is organized as follows: In Sect. 2, edge direc-\\ntion histogram (EDH) and edge intensity histogram (EIH)\\nare extracted for representing the visual features inherent\\nin a video class. Section 3 gives a brief introduction to the\\nclassiﬁer methodologies used for video classiﬁcation. The\\nsection also describes a method to combine the evidences\\nfrom multiple classiﬁers. Section 4 describes experiments\\nfor classiﬁcation of videos of the ﬁve sports categories, and\\ndiscusses the performance of the system. Section 5 summa-\\nrizes the study.\\n2 Extraction of edge-based features\\nWe consider two features to represent the edge information,\\nnamely, EDH and EIH. Edge direction histogram is one of\\nthe standard visual descriptors deﬁned in MPEG-7 for image\\nand video, and provides a good representation of nonho-\\nmogeneous textured images [16]. This descriptor captures\\nthe spatial distribution of edges. Our approach to compute\\nthe EDH is a modiﬁed version of the approach given in\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 2}, page_content='SIViP (2010) 4:61–73\\n63\\nFig. 1 Sample images from\\nﬁve different sports video\\ncategories: a Basketball,\\nb cricket, c football, d tennis\\nand e volleyball\\n[16]. A given image is ﬁrst segmented into four sub-images.\\nThe edge information is then calculated for each sub-image\\nusing Canny algorithm [17]. The range of the edge directions\\n(0◦–180◦) is quantized into 5bins. Thus, an image partitioned\\ninto four sub-images results in a 20-dimensional EDH fea-\\nture vector for each frame of a video clip. The choice of\\npartitioning an image into four sub-images and quantization\\nof edge directions into 5 bins are found to be appropriate\\nbased on experimental evidence. Figure 3 shows 20-dimen-\\nsional EDHs for ﬁve different categories. Each histogram is\\nobtained by averaging the histograms obtained from individ-\\nual frames of a clip. The clips were selected randomly from\\nﬁve different classes. The ﬁgure shows that the pattern of\\nEDH is different for different classes, and that the selected\\nfeatures carry discriminative information among the differ-\\nent video classes.\\nWe also consider the distribution of the edge intensities\\nto evaluate the degree of uniformity of the edge pixels. This\\nfeature is derived from the magnitude information of the edge\\npixels. The range of magnitudes (0–255) is quantized into 16\\nbins, and a 16-dimensional EIH is derived from each frame\\nof a video clip. Figure 4 shows the 16-dimensional EIH for\\nﬁve different categories. Each histogram is obtained by aver-\\naging the histograms obtained from individual frames of a\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 3}, page_content='64\\nSIViP (2010) 4:61–73\\nFig. 2 Edge images\\ncorresponding to the ﬁve\\ndifferent images shown in Fig. 1,\\nfor the following categories:\\na Basketball,\\nb cricket, c football, d tennis\\nand e volleyball\\nclip. The clips were selected randomly from the ﬁve different\\nclasses. From Figs. 3 and 4, we observe that EDH carries\\nmore discriminative information among the classes than EIH.\\n3 Classiﬁer methodologies\\nOnce features are extracted, the next step is to model the\\nbehavior of the features for performing classiﬁcation. We\\nconsider three classiﬁer methodologies for our study, namely,\\nautoassociative neural networks (AANN), HMMs, and\\nSVMs. The AANNs are useful to model the video content,\\ndue to their ability to capture the distribution of the feature\\nvectors [18]. Given the temporal nature of the video, HMMs\\nare effective for modeling the time-varying patterns [19].\\nSupport vector machines areuseful for their inherent discrim-\\ninative learning ability and good generalization performance\\n[20]. In the following subsections, a brief introduction to the\\nthree classiﬁer methodologies is presented.\\n3.1 AANN models for estimating the density of feature\\nvectors\\nAutoassociative neural network models are feedforward neu-\\nral networks, performing an identity mapping of the input\\nspace [21,22]. From a different perspective, AANN models\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 4}, page_content='SIViP (2010) 4:61–73\\n65\\nFig. 3 Average edge direction\\nhistogram feature vectors of 20\\ndimension for sample clips\\nselected randomly from the ﬁve\\ndifferent classes: a Basketball,\\nb cricket, c football, d tennis\\nand e volleyball\\n0\\n5\\n10\\n15\\n20\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nBasketball\\n(a)\\n0\\n5\\n10\\n15\\n20\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nCricket\\n(b)\\n0\\n5\\n10\\n15\\n20\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nFootball\\n(c)\\n0\\n5\\n10\\n15\\n20\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nTennis\\n(d)\\n0\\n5\\n10\\n15\\n20\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nVolleyball\\n(e)\\ncan be used to capture the distribution of input data [18]. The\\ndistribution capturing ability of the AANN models is dis-\\ncussed in detail in Appendix. In this study, separate AANN\\nmodels are used to capture the distribution of feature vectors\\nof each sports video category. A ﬁve-layer AANN model is\\nshown in Fig. 5. The structure of the AANN model used in\\nthe present studies is 20L 40N 6N 40N 20L, where L denotes\\nlinear units and N denotes nonlinear units. This structure is\\narrived at experimentally. The activation function of the non-\\nlinear unit is a hyperbolic tangent function. The network is\\ntrained using error backpropagation learning algorithm for\\n500 epochs [21]. One epoch denotes the presentation of all\\nthe training examples (of a given class) to the neural network\\nexactly once. The number of epochs is chosen using cross-\\nvalidation for veriﬁcation, to obtain the best performance for\\nthe experimental data.\\nThe block diagram of the proposed sports video classiﬁca-\\ntion system based on EDH is shown in Fig. 6. For each video\\ncategory, a separate AANN model is developed. The model\\ngiving the strongest evidence for a given test clip is hypoth-\\nesized as the category of the test clip. Similar classiﬁcation\\nsystem is developed for features based on EIH. The EDH and\\nEIHfeaturevectors correspondingtoeachsports categoryare\\nused to train two separate AANN models for each feature\\ntype. The AANN models are trained using backpropagation\\nlearning algorithm in the pattern mode [21,22]. The learning\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 5}, page_content='66\\nSIViP (2010) 4:61–73\\nFig. 4 Average edge intensity\\nhistogram feature vectors of 16\\ndimension for sample clips\\nselected randomly from the ﬁve\\ndifferent classes: a Basketball,\\nb cricket, c football, d tennis\\nand e volleyball\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nBasketball\\n(a)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nCricket\\n(b)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nFootball\\n(c)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nTennis\\n(d)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nBin index\\nVolleyball\\n(e)\\nalgorithm adjusts weights of the network to minimize the\\nmean squared error obtained for each feature vector.\\nA test video clip is processed to extract the EDH and EIH\\nfeatures. These features are presented as input to the AANN\\nmodels of all thecategories. Theoutput of eachmodel is com-\\npared with its input to calculate the squared error for each\\nframe. The error Ek for the kth frame is transformed into\\na conﬁdence value by using the relation Ck = exp(−Ek).\\nFor a given test clip, the conﬁdence value is given by C =\\n1\\nN\\n\\x01N\\nk=1 Ck, where N is the total number of frames in the test\\nclip. For each category, two conﬁdence values are obtained,\\none for each feature type. These two scores are combined\\nusing linear weighted average rule to obtain a combined score\\nˆC given by\\nˆC = w × Cd + (1 −w) × Ci,\\n(1)\\nwhere Cd and Ci denote the conﬁdence scores for EDH and\\nEIH features, respectively. The value of w (0 ≤w ≤1) is\\nchosen to maximize the classiﬁcation performance for the\\ngiven data set. Thus, for each test video clip, ﬁve scores are\\nobtained. The category whose model gives the highest conﬁ-\\ndence value is hypothesized as the sports category of the test\\nclip. Experimental results are discussed in Sect. 4.\\n3.2 Hidden Markov models\\nThe hidden Markov model consists of ﬁnite number (N) of\\nstates. The state of the system at each time step is updated\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 6}, page_content='SIViP (2010) 4:61–73\\n67\\nInput layer\\nLayer\\n.\\n..\\n...\\n..\\n.\\n.\\n..\\n...\\nCompression\\nlayer\\nOutput layer\\n1\\n2\\n3\\n4\\n5\\nFig. 5 Structure of ﬁve-layer AANN model used for video classiﬁca-\\ntion\\naccording to a probability distribution that depends only on\\nthe previous state. Additionally, a symbol is generated in each\\nstate according to a probability distribution that depends on\\nthat state. The parameters of the HMM are adjusted using the\\ntraining data set [23]. Given a HMM model λ and an obser-\\nvation sequence O, the probability P(O/λ) that this obser-\\nvation sequence comes from the model λ is calculated as a\\nsum over all possible state sequences. The hidden Markov\\nmodel toolkit (HTK) was used for developing class-speciﬁc\\nmodels [24]. The number of states (N = 7) and number\\nof mixtures (M = 1) per state are chosen experimentally\\nto obtain the best classiﬁcation performance. During testing\\nphase, the HMM gives the log probability, representing the\\nlikelihood that the given clip belongs to that particular class.\\nThe test methodology is similar to the block schematic shown\\nin Fig. 6. Experimental results are discussed in Sect. 4.\\n3.3 Support vector machines for video classiﬁcation\\nSupport vector machines provide a new approach to the pat-\\ntern classiﬁcation problems with underlying basis in statis-\\ntical learning theory, in particular the principle of structural\\nrisk minimization [25]. The SVM models learn to separate\\nthe boundary regions between patterns belonging to two clas-\\nses by mapping the input patterns onto a high-dimensional\\nspace, and seeking a separating hyperplane in this space. The\\nseparating hyperplane is chosen in such a way as to maximize\\nits distance (margin) from the closest training examples. We\\nconsider SVM models for classiﬁcation due to their ability\\nto generalize from limited amount of training data, and also\\ndue to their inherent discriminative learning [22]. The SVM-\\nTorch-II [26] was used for developing class-speciﬁc SVM\\nmodels. When a given feature vector corresponding to a test\\nclip is presented to an SVM model, the result is a measure of\\nthe distance of the feature vector from the hyperplane con-\\nstructed as a decision boundary between a given class and\\nthe remaining classes.\\nThe performance of the pattern classiﬁcation problem\\ndepends on the type of kernel function chosen. Possible\\nchoices of kernel function include polynomial, Gaussian and\\nsigmoidal functions. In this work, we have used Gaussian\\nkernel, since it was empirically observed to perform better\\nthan the other two. This class of SVMs involves two param-\\neters, namely, the kernel width σ and the penalty param-\\neter P. In our experiments, the value of σ represents the\\ndynamic range of the features. The value of P was chosen\\nFig. 6 Block diagram of the\\nproposed video classiﬁcation\\nsystem using edge direction\\nhistogram features. Categories\\n1–5 are cricket, football, tennis,\\nbasketball and volleyball\\nrespectively\\nDecision Logic\\nFeature\\nVideo clip\\nExtraction\\nAANN_1\\nAANN_2\\nAANN_3\\nAANN_4\\nAANN_5\\nEvidence for\\nEvidence for\\nEvidence for\\nEvidence for\\nEvidence for\\nEdge direction\\nhistogram\\nfeatures\\nAANN models\\ncategory 1 \\ncategory 2\\ncategory 3\\ncategory 4\\ncategory 5\\nvideo category\\nHypothesised\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 7}, page_content='68\\nSIViP (2010) 4:61–73\\ncorresponding to the best classiﬁcation performance. The\\nSVMs were originally designed for two-class classiﬁcation\\nproblems. In our work, multi-class (M = 5) classiﬁcation\\ntask is achieved using one-against-rest approach, where an\\nSVM is constructed for each class by discriminating that\\nclass against the remaining (M −1) classes. The test meth-\\nodology is similar to the block schematic shown in Fig. 6.\\nExperimental results are discussed in Sect. 4.\\n3.4 Combining evidence due to multiple classiﬁers\\nIt has been shown in the literature [27–30] that combination\\nof evidence obtained from several complementary classiﬁers\\ncan improve the performance of classiﬁcation. The reasons\\nfor combining evidence from multiple classiﬁers/features are\\nas follows: (a) For a speciﬁc pattern recognition applica-\\ntion, each classiﬁer methodology can attain only a certain\\ndegree of success, but combining evidence from different\\nmethodologies can reinforce a positive decision and min-\\nimize the incidence of misclassiﬁcation. (b) It is hard to\\nlump different features together to design one single clas-\\nsiﬁer, due to the curse of dimensionality. (c) Combining\\nevidence from different features which provide complemen-\\ntary information about a given class may help in improving\\nclassiﬁcation.\\nThere are numerous types of features that can be extracted\\nfrom the same raw data. Based on each of these features, a\\nclassiﬁer or several different classiﬁers can be trained for the\\nsame classiﬁcation task. As a result, we need to combine the\\nresults from these classiﬁers to produce an improved result\\nfor the classiﬁcation task. The output information from a\\nclassiﬁer reﬂects the degree of conﬁdence that the speciﬁc\\ninput belongs to the given class. First, the evidence due to\\ntwo different features, namely, the EDH and EIH, are com-\\nbined using the rule of linear weighting, as described in Eq. 1.\\nAt the next level, evidence obtained from different classiﬁers\\nare combined using linear weighting. The outcome of such a\\ncombination of evidences is discussed in the next section.\\n4 Results and discussion\\n4.1 Data set\\nExperiments are carried out on about 5h and 30min of video\\ndata (1,000 video clips, 200 clips per sports category, and\\neach clip of 20s duration) comprising of cricket, football,\\ntennis, basketball and volleyball video categories. The video\\nclips were captured at the rate of 25 frames/s, at 320 × 240\\npixel resolution. The data were collected from different TV\\nchannels in different sessions to capture the variability due to\\nsessions. For each sports video category, 100 clips are used\\nfor training, and the remaining 100 clips are used for testing.\\n4.2 Performance of different classiﬁers\\nThe performance of the AANN based classiﬁcation system\\nusing EDH, EIH, and combined evidence from EDH and EIH\\nis shown in Table 1. The performances of the classiﬁcation\\nsystems based on HMMs and SVMs are given in Tables 2\\nand 3, respectively. From the results, it can be observed that\\nthe classiﬁcation performance is poorer for video clips of\\ncricket and football categories, compared to those of tennis,\\nbasketball and volleyball categories. This is because, in the\\nlatter three categories, the playing ﬁelds have well-deﬁned\\nlines, and they appear in a majority of frames of a video clip.\\nMoreover, a few well-deﬁned camera views dominate the\\nbroadcast. For example, such views may cover the full court\\nin tennis or volleyball. Thus, a large area of an image frame\\ncomprises the playing ﬁeld. On the other hand, in cricket and\\nfootball categories the camera view tends to change from one\\nposition to another depending on the action. Thus, continu-\\nous motion along with lack of well-manifested edge-speciﬁc\\ninformation results in poorer classiﬁcation. It is also evident\\nthat the edge direction is a stronger feature for discriminating\\nbetween the classes, compared to the edge intensity. This may\\nbe due to the fact that one can visually perceive the content of\\nan image from its binary edge image, which preserves only\\nthe edge directions but not the magnitudes. Performance of\\nTable 1\\nPerformance of\\nAANN based sports video\\nclassiﬁcation system using\\nEDH, EIH, and combined\\nevidence (correct classiﬁcation\\nin %)\\nCricket\\nFootball\\nTennis\\nBasketball\\nVolleyball\\nAverage\\nperformance\\nEDH\\n81\\n84\\n95\\n94\\n95\\n89.8\\nEIH\\n54\\n57\\n93\\n93\\n92\\n77.8\\nCombined\\n84\\n88\\n100\\n100\\n100\\n94.4\\nTable 2\\nPerformance of HMM\\nbased sports video classiﬁcation\\nsystem using EDH, EIH, and\\ncombined evidence (correct\\nclassiﬁcation in % )\\nCricket\\nFootball\\nTennis\\nBasketball\\nVolleyball\\nAverage\\nperformance\\nEDH\\n77\\n86\\n92\\n95\\n94\\n88.8\\nEIH\\n45\\n58\\n84\\n93\\n92\\n74.4\\nCombined\\n80\\n87\\n93\\n98\\n96\\n90.8\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 8}, page_content='SIViP (2010) 4:61–73\\n69\\nTable 3\\nPerformance of SVM\\nbased classiﬁcation system\\nusing EDH, EIH, and combined\\nevidence (correct classiﬁcation\\nin % )\\nCricket\\nFootball\\nTennis\\nBasketball\\nVolleyball\\nAverage\\nperformance\\nEDH\\n81\\n84\\n92\\n93\\n95\\n89.0\\nEIH\\n68\\n86\\n32\\n89\\n92\\n73.4\\nCombined\\n83\\n86\\n100\\n100\\n100\\n93.8\\nTable 4\\nClassiﬁcation\\nperformance obtained by\\ncombining evidence from\\ndifferent classiﬁers (correct\\nclassiﬁcation in % )\\nCricket\\nFootball\\nTennis\\nBasketball\\nVolleyball\\nAverage\\nperformance\\nAANN\\n84\\n88\\n100\\n100\\n100\\n94.0\\nSVM\\n83\\n86\\n100\\n100\\n100\\n93.8\\nHMM\\n80\\n87\\n93\\n98\\n96\\n90.8\\nAANN+SVM\\n96\\n94\\n100\\n100\\n100\\n98.0\\nAANN+HMM\\n92\\n92\\n100\\n100\\n100\\n96.8\\nHMM+SVM\\n90\\n92\\n100\\n100\\n100\\n96.4\\nAANN+HMM+SVM\\n96\\n94\\n100\\n100\\n100\\n98.0\\nthe SVM based classiﬁer is particularly poor for EIH features\\ncompared to AANN and HMM-based classiﬁers for the same\\nfeature. This is due to lack of discriminative information in\\nEIH, and also due to the fact that the SVMs are chosen for\\ntheir discriminative ability. Since edge direction and edge\\nintensity features can be viewed as complementary sources\\nof information, the evidence due to these features can be\\ncombined. Tables 1, 2, and 3 also show the performance of\\nclassiﬁcation obtained using weighted combination of evi-\\ndences using EDH and EIH from different classiﬁers. There\\nis an improvement in the performance of classiﬁcation due\\nto the combination of evidences, from all the classiﬁers.\\n4.3 Effect of duration and quality of test video data\\nThe duration of the test data (test video clip) has significant\\nbearing on the classiﬁcation performance. Several techniques\\nfor video classiﬁcation typically use test clips with durations\\nvarying from 60 to 180s [6,5,9,13,14,31]. The classiﬁca-\\ntion performance was found to improve with increase in the\\nduration of the test clip. The average edge ratio used in con-\\njunction with k-nearest neighbor algorithm requires 120s of\\ntest data to yield a classiﬁcation performance of 92.4% on a\\nﬁve-class problem [6]. The AANN based classiﬁer has bet-\\nter generalizing ability than the k-nearest neighbor classiﬁer.\\nSimilarly, a time-constrained clustering algorithm [13] using\\ncompressed color features requires a minimum of 50s of test\\ndata to yield a classiﬁcation performance comparable to the\\nproposed method. The proposed method was applied on test\\nclips of 20s duration in all the experiments on video classi-\\nﬁcation. The performance given in Tables 1, 2 and 3 is com-\\nparable to the result obtained using larger duration of test\\nclips. Apart from the duration of the test data, the quality of\\nthe test data also inﬂuences the classiﬁcation performance.\\nSome methods retain only the class-speciﬁc frames in the\\ntest data by editing out images related to crowd/audience or\\noff-ﬁeld action [13]. Such editing results in an improved per-\\nformance. In our experiments, no such editing of the test data\\nwas done.\\n4.4 Performance on combining evidence from multiple\\nclassiﬁers\\nThe normalized measurement values obtained from the three\\nclassiﬁers are combined using linear weighting. Table 4\\nshows classiﬁcationperformanceobtainedbycombiningevi-\\ndence from different combinations of the three classiﬁers.\\nIt is observed that the combination of evidences from any\\ntwo classiﬁers results in a better performance than those of\\nthe individual classiﬁers. The confusion matrix for the ﬁnal\\nclassiﬁer (combined AANN, HMM, and SVM) is given in\\nTable 5. The improvement in classiﬁcation due to combina-\\ntion of evidence can be attributed to the merits in different\\nclassiﬁer methodologies, which emphasize different types\\nTable 5\\nConfusion matrix of video classiﬁcation results (in %) cor-\\nresponding to the score obtained by combining evidence due to all the\\nthree classiﬁers (in %) (AANN, HMM, and SVM )\\nCricket\\nFootball\\nTennis\\nBasketball\\nVolleyball\\nCricket\\n96\\n00\\n04\\n00\\n00\\nFootball\\n02\\n94\\n04\\n00\\n00\\nTennis\\n00\\n00\\n100\\n00\\n00\\nBasketball\\n00\\n00\\n00\\n100\\n00\\nVolleyball\\n00\\n00\\n00\\n00\\n100\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 9}, page_content='70\\nSIViP (2010) 4:61–73\\nof information present in the features, such as their spatial\\ndistribution and temporal sequence.\\n4.5 Veriﬁcation of test video sequences using the classiﬁers\\nIt is necessary to examine the response of a classiﬁer for test\\ninputs of a different class. More specifically, if a test video\\nclip belongs to a class other than the above ﬁve classes, the\\nsystem is not expected to assign the label of any of the ﬁve\\nclasses. Instead, the system should assign a separate label to\\nall such test cases. This, however, depends on two factors:\\n(a) the nature of evidence/measurement output by a classi-\\nﬁer, and (b) the decision logic for assigning a class label to\\na test video clip. In SVM-based classiﬁers, one-against-rest\\napproach is used for decomposition of multi-class pattern\\nclassiﬁcation problem into several two-class pattern classiﬁ-\\ncation problems. Hence, one should ideally get all negative\\nconﬁdence scores as output of the SVM model for a test clip\\nwhich does not belong to any of the predeﬁned categories.\\nThus, a natural threshold of 0 helps in decision making in\\nthe case of SVM, although such a decision could also be in\\nerror.\\nIn the case of AANN models and HMMs, the training pro-\\ncess attempts to capture only the within-class properties, and\\nno speciﬁc attempt is made to distinguish a given class from\\nothers. Thus, a nonclass test input to these models still results\\nin positive measurements, although small. Figure 7 shows\\nthe histogram of in-class conﬁdence scores along with that\\nof nonclass conﬁdence scores, for AANN models, SVMs\\nand HMMs. The scores are normalized between 0 and 1.\\nThe in-class scores are obtained by presenting the test video\\nclips of a given category to the models of the same cate-\\ngory. The nonclass scores are obtained by presenting the test\\nvideo clips of a given category to the models of other cate-\\ngories. A total of 100 test video clips of each class are used\\nto obtain the in-class and nonclass conﬁdence scores. The\\nextent of separation of the histograms indicates the ability\\nof the model to discriminate between in-class and nonclass\\nexamples. The area of overlap of the two histograms is a\\nmeasure of the minimum classiﬁcation error. From Fig. 7,\\nwe observe that this area of overlap is least for SVM-based\\nclassiﬁer, followed by AANN-based classiﬁer. If the con-\\nﬁdence score corresponding to the intersection of the two\\nhistograms is chosen as threshold for decision, then such a\\nchoice results in minimum classiﬁcation error on the training\\ndata. The same threshold is used for decision in the case of\\ntest data. Tables 6, 7, and 8 indicate the outcome of present-\\ning test video clips of cartoon, commercial and news cate-\\ngories, to the models based on AANN, SVM, and HMM,\\nrespectively, trained on cricket, football, tennis, basketball,\\nand volleyball. The entries in the tables denote the percent-\\nage of misclassiﬁcation. For instance, if a test video clip of\\ncartoon category, when presented to the model of cricket\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nConfidence score (x)\\nin−class\\nnonclass\\np(x)\\n(a)\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nConfidence score (x)\\nin−class\\nnonclass\\np(x)\\n(b)\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nConfidence score (x)\\nin−class\\nnonclass\\n(c)\\np(x)\\nFig. 7 Histograms of in-class conﬁdence scores along with nonclass\\nconﬁdence scores for a AANN models, b HMMs, and c SVM models\\ncategory, is labeled as cricket, then the test video clip is said\\nto be misclassiﬁed. For veriﬁcation, 100 test video clips of\\neach of cartoon, commercial and news categories are used.\\nThe average misclassiﬁcation is less than 15% for classiﬁers\\nbased on AANN and SVM. Classiﬁer based on HMM does\\nnot seem to be very useful for discrimination. The misclassi-\\nﬁcation error may be reduced further by extracting features\\nspeciﬁc to a given class.\\n4.6 Performance comparison of proposed approach\\nwith existing approaches\\nThe performance of the proposed approach is compared\\nagainst some existing approaches in the literature [5–15,31].\\nIn [5], the DCT edge features are used to classify video\\nsequences into meaningful semantic segments of 24s dura-\\ntion. For edges and their duration as features, the correct\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 10}, page_content='SIViP (2010) 4:61–73\\n71\\nTable 6\\nPerformance of\\nmisclassiﬁcation ( in % )\\nobtained from AANN models,\\nfor test clips which do not\\nbelong to any of the ﬁve sports\\ncategory\\nCricket\\nFootball\\nTennis\\nBasketball\\nVolleyball\\nAverage\\nperformance\\nCartoon\\n08\\n06\\n02\\n01\\n01\\n3.60\\nCommercial\\n19\\n12\\n08\\n03\\n02\\n8.80\\nNews\\n24\\n17\\n21\\n05\\n04\\n14.20\\nTable 7\\nPerformance of\\nmisclassiﬁcation ( in % )\\nobtained from SVM models, for\\ntest clips which do not belong to\\nany of the ﬁve sports category\\nCricket\\nFootball\\nTennis\\nBasketball\\nVolleyball\\nAverage\\nperformance\\nCartoon\\n39\\n16\\n02\\n01\\n04\\n12.00\\nCommercial\\n34\\n03\\n02\\n01\\n01\\n8.40\\nNews\\n55\\n12\\n01\\n02\\n01\\n14.00\\nTable 8\\nPerformance of\\nmisclassiﬁcation ( in % )\\nobtained from HMM models, for\\ntest clips which do not belong to\\nany of the ﬁve sports category\\nCricket\\nFootball\\nTennis\\nBasketball\\nVolleyball\\nAverage\\nperformance\\nCartoon\\n47\\n16\\n27\\n01\\n25\\n22.20\\nCommercial\\n59\\n02\\n28\\n02\\n33\\n24.80\\nNews\\n11\\n08\\n01\\n02\\n01\\n4.40\\nclassiﬁcation is about 65%. In [6], the average edge ratio\\nis used in conjunction with k-nearest neighbor algorithm.\\nThe method requires 120s of test data to yield a classiﬁ-\\ncation performance of 92.4% on a ﬁve-class (badminton,\\nsoccer, basketball, tennis, and skating) problem. In [7], a\\nmotion pattern was used to classify the video contents at\\nthe semantic level using SVM. A 10-h long video program\\nincluding science and educational ﬁlms, sight-seeing videos,\\nstage performances, and sports video were segmented into\\nshots, and each shot was classiﬁed into semantic concepts.\\nAn average classiﬁcation performance of 94% was achieved.\\nThe approach described in [8] uses statistical nonparametric\\nmodeling of motion information to classify video shots into\\ntemporal texture (rivers, grass motion, trees in the wind),\\nsequences of pedestrians and trafﬁc video shots. The method\\nachieved mean recognition rate higher than 90%.\\nMotion dynamics such as foreground object motion and\\nbackground camera motion are extracted in [10] for classi-\\nﬁcation of video sequences into three categories, namely,\\nsports, cartoon and news using Gaussian mixture models\\n(GMMs). Using 30s clips, this method gives a classiﬁcation\\nperformance of about 94%. The approach described in [11]\\nuses statistical models (GMM) of reduced DCT or Hadamard\\ntransform coefﬁcients, and gives 87% correct classiﬁcation\\nrate for six different video shots consisting of presentation\\ngraphics, long shots of the projection screen both lit and unlit,\\nlong shots of the audience and medium close-ups of human\\nﬁgures on light and dark backgrounds.\\nIn [12], GMM was used to model low-level audio/video\\nfeatures for the classiﬁcation of ﬁve different categories,\\nnamely, sports, cartoon, news, commercial, and music. An\\naverage correct classiﬁcation rate of 86.5% was achieved\\nwith 1h of recordings per genre, consisting of continuous\\nsequences of 5min each and 40s decision window. In [13],\\nvector quantization and HMM are used to characterize sports\\nvideos such as basketball, ice hockey, soccer, and volley-\\nball based on motion information. The length of the video\\nsequences ranged from 43s to 1min. The method achieved\\nan average performance of about 82 and 88% using vector\\nquantization and HMM, respectively.\\nAnother approach described in [14] uses HMMs and\\nmotion and color features for classiﬁcation of four differ-\\nent sports videos, namely, ice hockey, basketball, football\\nand soccer. The method achieved an overall classiﬁcation\\naccuracy of 93%. Each testing sequence is of 100s dura-\\ntion. In [15], a decision tree method was used to classify\\nvideo shots into different genres such as movie, commer-\\ncial, music, and sports based on motion and color informa-\\ntion. This method achieved an average prediction accuracy\\nof nearly 75%. In [31], the C4.5 decision tree was used to\\nbuild the classiﬁer for genre labeling using a set of features\\nthat embody the visual characteristics of video sequences,\\nsuch as news, commercial, music, sports, and cartoon. The\\naverage classiﬁcation performance was between 80 and 83%\\nfor video clips of 60s duration.\\nThe method proposed in this paper uses AANNs to clas-\\nsify ﬁve sports categories, namely, cricket, football, tennis,\\nbasketball, and volleyball based on edge-speciﬁc features.\\nDespite using shorter duration test clips (20s), the proposed\\nmethod yields an average classiﬁcation performance of\\n94.4% which compares favorably with existing approaches\\nthat use longer duration test clip.\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 11}, page_content='72\\nSIViP (2010) 4:61–73\\nInput layer\\nLayer\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\nCompression\\nlayer\\nOutput layer\\n1\\n2\\n3\\n4\\n5\\nFig. 8 A ﬁve-layer AANN model\\n5 Summary\\nWe have presented an approach for classiﬁcation of sports\\nvideobasedonedge-speciﬁcfeatures,namely,EDHandEIH.\\nWe have exploited the ability of AANNs to capture the dis-\\ntribution of feature vectors. Two other classiﬁers, namely,\\nHMMs and SVMs were also examined. A video database\\nof TV broadcast programs containing ﬁve sports video\\ncategories, namely, cricket, football, tennis, basketball and\\nvolleyball was used for training and testing the models.\\nExperimental results indicate that the edge-based features\\ncan provide useful information for discriminating the classes\\nconsidered, and that the EDH is a superior feature compared\\nto the EIH. It was shown that combining evidences from\\ncomplementary edge features and from different classiﬁers\\nresultsinanimprovementintheperformanceofclassiﬁcation.\\nIt is also observed that the classiﬁcation system is able to\\ndecide, whether a given test video clip belongs to one of the\\nﬁve predeﬁned video categories or not. In order to achieve\\nbetter classiﬁcation performance, evidence from audio and\\nvisual features may be combined.\\nAppendix\\nAutoassociative neural network models\\nAutoassociative neural network models are feedforward neu-\\nral networks performing an identity mapping of the input\\nspace, and are used to capture the distribution of the input\\ndata [18,32]. The distribution capturing ability of the AANN\\nmodel is described in this section. Let us consider the ﬁve-\\nlayer AANN model shown in Fig. 8, which has three hidden\\nlayers. In this network, the second and fourth layers have\\nmore units than the input layer. The third layer has fewer\\nunits than the ﬁrst or ﬁfth. The processing units in the ﬁrst\\nand third hidden layer are nonlinear, and the units in the sec-\\nond compression/hidden layer can be linear or nonlinear. As\\nthe error between the actual and the desired output vectors\\nis minimized, the cluster of points in the input space deter-\\nmines the shape of the hypersurface obtained by the projec-\\ntion onto the lower dimensional space. Figure 9b shows the\\nspace spanned by the 1-dimensional compression layer for\\nthe2-dimensionaldatashowninFig.9aforthenetworkstruc-\\nture 2L 10N 1N 10N 2L, where L denotes a linear unit and\\nN denotes a nonlinear unit. The integer value indicates the\\nnumber of units used in that layer. The nonlinear output func-\\ntion for each unit is tanh(s), where s is the activation value\\nof the unit. The network is trained using backpropagation\\nalgorithm [21,22]. The solid lines shown in Fig. 9b indicate\\nmapping of the given input points due to the 1-dimensional\\nFig. 9 Distribution capturing\\nability of AANN model.\\na Artiﬁcial 2-dimensional\\ndata. b 2-dimensional output\\nof AANN model with the\\nstructure 2L 10N 1N 10N 2L.\\nc Probability surfaces\\nrealized by the network\\nstructure 2L 10N 1N 10N 2L\\n−4\\n−2\\n0\\n2\\n4\\n−5\\n0\\n5\\n10\\n0\\n0.05\\n0.1\\n(a)\\n−4\\n−2\\n0\\n2\\n4\\n−5\\n0\\n5\\n10\\n0\\n0.05\\n0.1\\n(b)\\n(c)\\n123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0.1 (Windows)', 'creator': 'LaTeX with hyperref package', 'creationdate': '2010-02-10T11:31:58+05:30', 'source': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'file_path': '../data/pdf_files/10.Classification of sport videos using edge-based features and autoassociative neural network models.pdf', 'total_pages': 13, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2010-02-10T11:34:35+05:30', 'trapped': '', 'modDate': \"D:20100210113435+05'30'\", 'creationDate': \"D:20100210113158+05'30'\", 'page': 12}, page_content='SIViP (2010) 4:61–73\\n73\\ncompressionlayer.Thus,onecansaythattheAANNcaptures\\nthe distribution of the input data depending on the constraints\\nimposed by the structure of the network, just as the number\\nof mixtures and Gaussian functions do in the case of GMM.\\nIn order to visualize the distribution better, one can plot\\nthe error for each input data point in the form of some prob-\\nability surface as shown in Fig. 9c. The error Ei for the data\\npoint i in the input space is plotted as pi = exp(−Ei/α), where\\nα is a constant. Note that pi is not strictly a probability den-\\nsity function, but we call the resulting surface as probability\\nsurface. The plot of the probability surface shows a large\\namplitude for smaller error Ei, indicating better match of the\\nnetwork for that data point. The constraints imposed by the\\nnetwork can be seen by the shape the error surface takes in\\nboth the cases. One can use the probability surface to study\\nthe characteristics of the distribution of the input data cap-\\ntured by the network. Ideally, one would like to achieve the\\nbest probability surface, best deﬁned in terms of some mea-\\nsure corresponding to a low average error.\\nReferences\\n1. Jain, A., Duin, R., Mao, J.: Statistical pattern recognition: a\\nreview. IEEE Trans. Pattern Anal. Mach. Intell. 22, 4–37 (2000)\\n2. Wang, Y., Liu, Z., Huang, J.C.: Multimedia content analysis using\\nboth audio and visual clues. IEEE Signal Process. Mag. 17, 12–36\\n(2000)\\n3. Assﬂag,J.,Bertini,M.,Colombo,C.,Bimbo,A.D.: Semantic anno-\\ntation of sports videos. IEEE Multimedia 9, 52–60 (2002)\\n4. Kokaram, A., Rea, N., Dahyot, R., Tekalp, A.M.: Browsing sports\\nvideo. IEEE Signal Process. Mag. 5021, 47–58 (2006)\\n5. Lee, M.H., Nepal, S., Srinivasan, U.: Edge-based semantic classi-\\nﬁcation of sports video sequences. In: International Conference on\\nMultimedia and Expo, Baltimore, MD, USA (2003)\\n6. Yuan, Y., Wan, C.: The application of edge features in automatic\\nsports genre classiﬁcation. In: Proceedings of IEEE Conference on\\nCybernetics and Intelligent Systems, Singapore (2004)\\n7. Ma, Y.F., Zhang, H.J.: Motion pattern based video classiﬁcation\\nusing support vector machines. In: Proceedings of the IEEE Inter-\\nnational Symposium on Circuit and Systems, Arizona, USA (2002)\\n8. Fablet, R., Bouthemy, P.: Statistical modeling for motion-based\\nvideo classiﬁcation and retrieval. In: Proceedings of the Work-\\nshop on Multimedia Content Based Indexing and Retrieval, France\\n(2001)\\n9. Takagi, S., Hattori, S., Yokoyama, K., Kodate, A., Taminaga, H.:\\nSportsvideocategorizingmethodusingcameramotionparameters.\\nIn: International Conference on Multimedia and Expo, Baltimore,\\nMD, USA (2003)\\n10. Roach, M.J., Mason, J.S.D., Pawlewski, M.: Video genre classiﬁ-\\ncation using dynamics. In: Proceedings of the International Con-\\nference on Acoustics, Speech and Signal Processing, Utah, USA\\n(2001)\\n11. Girgensohn, A., Foote, J.: Video classiﬁcation using transform\\ncoefﬁcients. In: Proceedings of the Internatioanl Conference on\\nAcoustics, Speech and Signal Processing, Arizona, USA (1999)\\n12. Xu, L.Q., Li, Y.: Video classiﬁcation using spatial–temporal fea-\\ntures and PCA. In: International Conference on Multimedia and\\nExpo, pp. 345–348 (2003)\\n13. Sahouria, E., Zakhor, A.: Content analysis of video using principal\\ncomponents. IEEE Trans. Circuits Syst. Video Technol. 9, 1290–\\n1298 (1999)\\n14. Gibert, X., Li, H., Doermann, D.: Sports video classiﬁcation using\\nHMMs. In: International Conference on Multimedia and Expo,\\nBaltimore, MD, USA (2003)\\n15. Yuan, Y., Song, Q.B., Shen, J.Y.: Automatic video classiﬁcation\\nusing decision tree method. In: Proceedings of the IEEE Interna-\\ntional Conference on Machine Learning and Cybernetics, Beijing\\n(2002)\\n16. (UPM-GTI, ES), J.M.M.: MPEG-7 Overview (version 8). ISO/IEC\\nJTC1/SC29/WG11 N4980, Klangenfurt (July, 2002)\\n17. Canny, J.: A computational approach to edge detection. IEEE\\nTrans. Pattern Anal. Mach. Intell. 8, 679–698 (1986)\\n18. Yegnanarayana, B., Kishore, S.: AANN: an alternative to GMM\\nfor pattern recognition. Neural Netw. 15, 459–469 (2002)\\n19. Rabiner, L., Juang, B.: An introduction to hidden Markov mod-\\nels. IEEE Acoust. Speech Signal Process. Mag. 3, 4–16 (1986)\\n20. Collobert, R., Bengio, S.: SVMTorch: support vector machines\\nfor large-scale regression problems. J. Mach. Learn. Res. 1, 143–\\n160 (2001)\\n21. Yegnanarayana, B.: Artiﬁcial Neural Networks. Prentice-Hall\\nIndia, New Delhi (1999)\\n22. Haykin, S.: Neural Networks: A Comprehensive Foundation.\\nPrentice-Hall International, New Jersey (1999)\\n23. Baum, L.E., Petrie, T., Soules, G., Weiss, N.: A maximization tech-\\nnique occurring in the statistical analysis of probabilistic functions\\nof Markov chains. Ann. Math. Stat. 41, 164–171 (1970)\\n24. HMM Tool Kit. Available at: http://htk.eng.cam.ac.uk/\\n25. Burges, C.: A tutorial on support vector machines for pattern rec-\\nognition. Data Mining Knowl. Discov. 2, 121–167 (1998)\\n26. SVM\\nTool\\nKit.\\nAvailable\\nat:\\nhttp://www.idiap.ch/~bengio/\\nprojects/SVMTorch.html\\n27. Kittler, J., Hatef, M., Duin, R.P.W., Matas, J.: On combining clas-\\nsiﬁers. IEEE Trans. Pattern Anal. Mach. Intell. 20, 226–239 (1998)\\n28. Rohlﬁng, T., Russakoff, D.B., Maurer, C.R.: Performance-based\\nclassiﬁer combination in atlas-based image segmentation using\\nexpectation-maximization parameter estimation. IEEE Trans.\\nMed. Imaging 23, 983–994 (2004)\\n29. Lam, L., Suen, C.Y.: Optimal combinations of pattern classiﬁ-\\ners. Pattern Recognit. Lett. 16, 945–954 (1995)\\n30. Xu, L., Krzyzak, A., Suen, C.Y.: Methods of combining multiple\\nclassiﬁers and their applications to handwriting recognition. IEEE\\nTrans. Syst. Man, Cybern. 2, 418–435 (1992)\\n31. Truong, B.T., Venkatesh, S., Dorai, C.: Automatic genre identiﬁ-\\ncation for content-based video categorization. In: Proceedings of\\nthe International Conference on Pattern Recognition, Barcelona,\\nSpain (2000)\\n32. Yegnanarayana, B., Gangashetty, S.V., Palanivel, S.: Autoassocia-\\ntive neural network models for pattern recognition tasks in speech\\nand image. In: Soft Computing Approach to Pattern Recognition\\nand Image Processing, World Scientiﬁc Publishing Co. Pte. Ltd,\\nSingapore (2002) 283–305\\n123'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 0}, page_content='Research Article\\nA Deep Learning Algorithm for Special Action\\nRecognition of Football\\nSheng Wang\\nCollege of Arts and Sciences, Yangtze University, Jingzhou 434020, China\\nCorrespondence should be addressed to Sheng Wang; shengwang83@yangtzeu.edu.cn\\nReceived 27 May 2022; Revised 7 July 2022; Accepted 21 July 2022; Published 11 August 2022\\nAcademic Editor: Liping Zhang\\nCopyright © 2022 Sheng Wang. Tis is an open access article distributed under the Creative Commons Attribution License, which\\npermits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\\nSoccer (football) is a popular form of exercise on the planet. Tere are a lot of individuals who tune into football matches in real\\ntime on television or the Internet. A game of American football lasts 90 minutes, but to save time, spectators may simply want to\\nsee a few highlights. As far as we know, no such tool exists that can be used to extract intelligent highlights from a football match.\\nIn this research, we present a technique for clever editing of live football matches. Our technology allows for the automatic\\nextraction of key players’ goals, shots, corner kicks, red and yellow cards, and the presence of key players from a football match’s\\nlive stream. During the 2018 FIFA World Cup, our solution was integrated into live streaming platforms and it\\nfunctioned admirably.\\n1. Introduction\\nExtensive research has been conducted on video analytics\\ntechnology in order to provide customers with more rapid\\nand accessible access to engaging or critical segments of ﬁlms\\n[1–3]. Te demand for high-performance image and video\\nindexing and retrieval solutions has skyrocketed as the\\nquantity of multimedia videos has increased tremendously.\\nFrom their perspective, the video summary is very important\\nto them [4]. Users just need to view a few crucial areas to save\\ntime. It requires considerable time and eﬀort to manually\\nevaluate and summarize video recordings. Given the\\nnumber of distinct sequences and the length of time nec-\\nessary, an automated motion video sequence highlighting\\nstrategy is quite valuable.\\nSports video games are one of the most studied video\\ngenres due to their large audience and more consistent\\ncharacteristics than other video genres [5–8]. Te high-\\nlighting strategy, which focuses on how to compose a match\\nrecap that incorporates all of its important aspects, is an\\neﬀective abstraction technique for sports ﬁlm [9, 10]. Pre-\\nvious researchers have provided several highlighting strat-\\negies for a wide range of sports contests, from the most basic\\nto the most specialized ones. To conclude, in sports video\\nfootage, Ekin and Tekalp discovered play and break events\\n[11]. Other research studies utilized slow-motion playback\\nto describe sports videos [12–14]. However, evaluating\\ngeneral sports games remains diﬃcult due to the variety and\\ncomplexity of the games. Basketball, diving, and football are\\njust a few of the sports in which academicians have chosen to\\nconcentrate. Tis research focuses on methods for extracting\\nhighlights from football videos. SVM classiﬁers were sug-\\ngested by Ancona et al. as a tool for recognizing soccer\\ntargets [15]. When a video sequence is divided into distinct\\nshot types, it is then organized into smaller video shots using\\nthe technique described by Zawbaa and colleagues [16].\\nUsing SVM and an artiﬁcial neural network approach, the\\nsystem then picks segments with outstanding performance.\\nTe device subsequently detects the vertical goalpost and\\ngoal net. Finally, in the football video recap, the game’s most\\nsigniﬁcant moments will be highlighted. Focusing on low-\\nlevel and text-based processing, for soccer ﬁlms, Fendri et al.\\ndeveloped a segmentation and indexing system based on\\nscientiﬁc principles [17]. Automated methods for assessing\\nand summarizing football ﬁlms based on cinematic- and\\nobject-based criteria have been suggested by Ekin et al. [18].\\nIn order to summarize the ﬁlm, Lofti and Pourreza came up\\nwith a way of removing unnecessary footage [19]. Tabii and\\nHindawi\\nMobile Information Systems\\nVolume 2022, Article ID 6315648, 9 pages\\nhttps://doi.org/10.1155/2022/6315648'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 1}, page_content='Tami proposed a new approach for automatically creating\\nsummaries of soccer videos based on lens recognition,\\ncategorization of lenses, and ﬁnite state machine technology\\n[20]. All of the following methods process the video using\\nartiﬁcially created qualities that lack generalizability and are\\nchallenging to implement in reality. An innovative “One to\\nKey” notion developed by Yan et al. aims to enhance the\\ndetection of group activity by aggregating the temporal\\ndynamics between key players with varying levels of in-\\nvolvement across time [21]. It is quick and knowledgeable,\\nbut it cannot recognize the diﬀerence between the game’s\\nbest moments. Automated and intelligent lives are becoming\\nmore commonplace, and future multimedia processing\\ntechnologies will be more sophisticated and eﬃcient.\\nIn recent years, deep learning has been widely used in\\nimage processing [22–25] and pattern recognition [26–29].\\nUsing deep learning, we have developed a technology that\\nallows for intelligent editing that creates highlights from\\nwatching a football game on TV or watching it live online\\nwhile also resolving the concerns that an in-depth study\\ncould cause. Feature extraction techniques do not need to be\\ncreated by hand, and algorithms do not need to be updated\\nfor diﬀerent scenarios, as was the case with previous\\nmethods. After the live broadcast is over, the technology may\\nproduce a wide variety of entertaining short videos, such as\\ngoal celebrations, penalty kicks, red and yellow cards, corner\\nkicks, and so on. Te method achieved will not be forgotten\\nduring the FIFA World Cup 2018!\\n2. System Overview\\nDeep learning plays a positive role in football teaching. Te\\ntraining objective of deep learning is consistent and relevant\\nto the teaching characteristics of football. Deep learning can\\nstimulate students to actively participate in classroom\\nteaching and actively observe, think, summarize, internalize,\\nand practice what they have learned, which is in line with the\\nteaching characteristics of football projects. Teachers should\\nactively design teaching situations in football teaching, let\\nstudents practice football skills in themed, targeted, and\\npractical teaching situations, and develop the students’ high-\\nlevel sports ability by completing challenging learning tasks.\\nIn the process of developing students’ football skills, we\\nshould promote the all-round development of students\\nthrough continuous evaluation, realize the multidimen-\\nsional value of sports, and cultivate students’ lifelong sports\\nawareness. In order to capture the best moments from live\\nfootball games, we created an automated editing system. Live\\nfootage of a football match is used to create shot boundary\\nframes, special action frames, and star player frames by using\\nshot segmentation, red and yellow card recognition, corner\\nkick detection, penalty kick detection, shoot and celebration\\ndetection, score detection, and face identiﬁcation. After-\\nward, the data are integrated to provide highlights in an\\nintegration module. Figure 1 depicts our system’s ﬂow di-\\nagram. To do this, we annotate the objects we need from\\nfootball match ﬁlms, such as the 2018 World Cup in Brazil,\\nas well as footballs, players, and red and yellow cards.\\n3. Methods\\n3.1. Segmentation of Photographs. Since highlights are\\ncreated by synthesizing continuous photos in order to\\navoid discontinuities, shot segmentation is at the heart of\\nour technique. Figure 2’s shot boundary frame is what we\\nare looking for. For football match recordings, we employ a\\nhistogram technique to determine shot boundaries, which\\nmay reduce oscillations caused by the movement of objects\\nin the frame. Te most basic histogram approach compares\\nthe histograms of two sequential frames in grayscale or\\ncolor, respectively. When two histograms are compared,\\nthe bin-wise diﬀerence between them is used to identify a\\nshot boundary. To determine shot boundaries, Ueda et al.\\nturned to the color histogram change rate [30]. In contrast,\\nthey just employ the absolute value ratio between two\\nframes in the color histogram; hence, the outcome is\\ninsensitive.\\nA shot’s average DAV is believed to be bigger than the\\ndiﬀerence Dcur between the shot’s boundary frame and the\\nframe immediately preceding it. Dcur and Davg are made up\\nof the following:\\nDcur \\x88 \\U0010ff58\\n(h(c) −h(c −1))2\\nmax(h(c), h(c −1)) + ∈,\\nc ≥2,\\nDavg \\x88 \\U0010ff50c−1\\ni\\x882Dcur(i)\\nc −2\\n,\\nc ≥3,\\n(1)\\nwhere h (·) is a histogram calculation function and c is the\\nframe index. ϵ is an arbitrarily small positive quantity to\\nprevent the denominator being 0. It was set to 0.0001. When\\nthe ratio of Dcur to Davg exceeds a threshold, we use the last\\nframe as a shot boundary frame. Cross-validation on a set of\\nfootball match videos yielded the threshold.\\n3.2. Detection of Red/Yellow Cards. In several sports, in-\\nfractions are punished by the presentation of a red or yellow\\ncard. Te yellow card acts as a cautionary note, while the red\\ncard serves as an escape sign. If a player receives two yellow\\ncards during a game, they will be issued with a red card.\\nObject recognition techniques must be used to identify\\nred and yellow cards in football match recordings. Multi-\\nstage, two-stage, and one-stage object identiﬁcation tech-\\nniques are available. Early examples of cross-strategic\\nnetworks are R–CNN and SPPNet [31–33]. Each of the\\nstages of a search may be taught on its own or in conjunction\\nwith the others. It is possible to train R–CNN simultaneously\\nfor feature extraction, location regression, and classiﬁcation.\\nFor this reason, the two-stage process is referred to as a “two-\\nstep” process. Te removed region-proposal network, one-\\nstage network, and SSD of the Yolo series were able to\\nsigniﬁcantly increase object recognition speed. SSD was used\\nin the development of our object identiﬁcation algorithm in\\norder to achieve a good balance between speed and preci-\\nsion. For each frame of the football game video, as shown in\\nFigure 3, we apply SSD and output the frames having red or\\nyellow cards for analysis.\\n2\\nMobile Information Systems'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 2}, page_content='Shoot 1\\nShoot 2\\nLens catalogue\\nFigure 2: Shot segmentation. On each side of the red arrow, there are various shots. Boundary frames are used to describe the ﬁrst frame of\\neach shot.\\nVideo directory\\nLens slice\\nGoal short video\\nShoot short video\\nCelebration short video\\nCorner kick short video\\nRed/Yellow short video\\nPenalty kick short video\\nIntelligent\\ncompilation\\nFacial expression\\nrecognition\\nPenalty kick\\nRed/Yellow card\\nComer kick\\nFeast\\nShoot\\nGoal ocr\\nFigure 1: Te overall system.\\nFigure 3: Detection of a yellow or red card. Using the red and yellow boxes in the graphic, this module is able to identify the red and yellow\\ncards in the frame.\\nMobile Information Systems\\n3'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 3}, page_content='3.3. Corner Kick. When the ball crosses the goal line without\\nbeing scored on and is touched by one player from the\\nopposing team, the game is restarted with a corner kick.\\nMany fans enjoy watching highlights of corner kicks because\\nthey are considered excellent goal-scoring opportunities for\\nthe attacking team.\\nTe SSD is also responsible for detecting corner kicks. A\\ncorner kick frame should show the player, ball, and corner\\nﬂag all at the same time as seen in Figure 4. An SSD that has\\nbeen taught to recognize what is in a frame is what we use to\\nprocess frames. We output a corner kick keyframe when a\\nplayer, the ball, and the corner ﬂag can all be found in the\\nsame frame.\\n3.4. Penalty Kick Detection. In competitive football, the goal\\nof the game is to win. Choosing a suitable tactic is very\\nimportant for the preparation before the game. When an-\\nalyzing tactical behavior, the concept of tactics is deﬁned by\\nsome researchers as the actions taken by players to adapt to\\nthe dynamic changes of the game situation. Other re-\\nsearchers try to distinguish tactics from strategy and believe\\nthat strategy can be described as a preplanned competition\\nelement after considering the inﬂuence of available infor-\\nmation. From this perspective, the diﬀerence between tactics\\nand strategy lies in the relationship between them and time.\\nTe implementation of the strategy allows for longer\\nplanning and careful consideration, while the tactical be-\\nhavior is done under strong time pressure. Tactics determine\\nhow a team manages space, time, and personal actions to win\\ngames. In this context, space refers to the speciﬁc actions that\\ntake place on the court or the area of the court that the team\\nwants to cover during attack and defense. Time describes\\nthings such as the frequency and duration of the event (such\\nas ball control) or the speed at which the action is initiated.\\nIndividual actions specify the type of action being per-\\nformed, such as errors, passes, and passes. According to the\\nnumber of players, it can be further divided into individual\\ntactics, group tactics, team tactics, and game tactics. Tactical\\nbehavior analysis at the personal level can be used to study\\nthe kinematic relationship between a player and his oppo-\\nnents or teammates, such as the interaction between a full\\nback marking a winger or two central defenders. At the\\ngroup level, the team’s tactical movements of the selected\\ngroup of players can be analyzed, such as the oﬀside “trap”\\ncreated by the consistent movement of the guard line. At the\\nteam level, tactical behavior analysis can capture the vari-\\nables of all players’ team actions, such as analyzing the space\\noccupied by the team. Tactical behavior at the game level can\\nbe studied by examining the interaction of team actions\\nbetween two opposing teams, such as the distance between\\nteams. In some cases, strategic success depends on tactical\\nsuccess at diﬀerent organizational levels.\\nA penalty kick is a means of resuming a play in football\\nin which a player is permitted to attempt a single shot to the\\ngoal while the other team’s goalkeeper is the only one\\ndefending it. It is awarded when a player commits a foul in\\nhis or her own penalty area that results in a straight free kick.\\nTis module also makes use of the SSD. As indicated in\\nFigure 5, we must determine the player’s and goal’s posi-\\ntions. As long as there are more than ten opposing players on\\none team, this period is used for the penalty kick shot and\\noutput.\\n3.5. Fire and Commemorate Detection. We are down to our\\nlast play of the football game. Fans are captivated by the\\ngame because they know that each shot they take has the\\npotential to result in a goal. Celebrating also serves as a way\\nto honor key occurrences in one’s life. In order to complete\\nthis lesson, we must locate examples of goals and celebra-\\ntions at live football games around the country. Despite the\\nfact that ﬁlming and celebrating are both transient activities,\\nwe need a steady stream of frames to help us track them\\ndown. As an action recognition problem, we consider\\nshooting and celebrating as a means of distinguishing the\\nkind of human activity in a ﬁlm. Prior to the widespread use\\nof deep learning, IDT was the most stable and reliable\\ntechnique for action detection, but it is also the slowest [34].\\nMatching the optical ﬂow between two frames of video and\\nSURF critical places reduces the impact of camera move-\\nment. As deep learning has developed, there have been\\nseveral action recognition systems based on deep learning.\\nFor every two frames in a video sequence, a dense optical\\nstream is generated and the CNN model is trained for both\\nthe video image and this dense optical stream. To arrive at\\nthe ﬁnal classiﬁcation result, two independent networks are\\nfused together and the class scores of each network are\\nFigure 4: Slamming the ball into the corner. Te players, the ball,\\nand the corner ﬂag are shown in the picture through the use of the\\nred, blue, and yellow boxes in the frame.\\n4\\nMobile Information Systems'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 4}, page_content='directly compared. In order to process video, C3D employs\\n3D convolution and 3D pooling. Pre-trained on the kinetic\\ndataset, I3D has a better architecture and can be used for a\\nwider range of tasks. P3D conducts a research study to\\nhighlight the relevance of time domain information in action\\nrecognition. Figure 6 shows the results of using the well-\\nknown action detection model I3D to identify shots and\\ncelebrations. Data augmentation is necessary due to the\\ngame’s restricted number of shots and celebrations. We\\nutilize clips with gunshots or rejoicing as examples of good\\nexamples. I3D will be trained using enough data from\\npositive samples that were oﬀset back and forth in time at\\nrandom. After receiving a video feed of some kind, a module\\ncreates a point in the video that was identiﬁed as either a\\nﬁring or celebratory event in the video.\\n3.6.DetectionofScores. Te most thrilling aspect of a game is\\nthe goal, which is one of the most signiﬁcant components of\\nhighlights. By keeping an eye on how the score changes, we\\ndevelop a scoring system that can identify goals. Te three\\nparts of this system are text area identiﬁcation, text recog-\\nnition, and postprocessing. CTPN is used to recognize text\\nregions. To recognize text areas in each frame of football\\nmatch footage, we train our model on a vast number of text\\npictures. However, in football match footage, the score\\noccupies relatively little space, resulting in erratic outcomes.\\nAs a result, we have devised a straightforward method for\\nreducing the size of the gap between the scoring area and the\\ninput frame when using CTPN. To get more precise ﬁndings,\\nwe re-enter the text sections containing the scores that we\\nfound into CTPN. Boxes (B1(c), B2(c), . . . BN(c)) are the text\\nareas found in this frame, where Nc is the number of boxes.\\nAs long as the intersection over union (IoU) between them is\\ngreater than a threshold, we consider these regions in\\nsubsequent frames as possible score regions. According to\\nseveral deﬁnitions, the IoU between two locations is as\\nfollows:\\nIoui,j(c) \\x88\\nBi(c) ∩Bj(c −1)\\nBi(c) ∪Bj(c −1),\\nc > 1.\\n(2)\\nIf you look at the cth frame at the ith box, you will notice\\nthat there is an IoU between the ith and the jth boxes (c).\\nFrame c will then be expanded to include more potential\\nsites. Te length and breadth of our system have both grown\\nby 150 pixels. As a last step, we run the potential regions\\nthrough CTPN to create scores for each of them. In order to\\nrecognize text, we use OCR technology to identify words\\ninside text ﬁelds. We use Tesseract since it is an open source\\nand mature. Te result of this component is a list of all the\\nwords it has found. It is nice to have a long list of words to\\nwork with. Postprocessing is required to retrieve the scores\\nfrom this sequence. Regular expressions are the perfect tool\\nfor this. It is capable of ﬁnding words in a string that\\nconform to the score’s structure. We note the score and\\nFigure 5: Free throw. Te yellow and red boxes in the ﬁgure indicate how this module uses a frame to depict the players and the goal.\\n0\\n20\\n15\\n10\\nP3D\\nC3D\\nI3D\\n5\\n0\\n0\\n2\\n4\\n6\\nMissed\\n8\\n10\\n12\\n14\\n20\\n40\\n60\\nFalsePositive\\n80\\n100\\n120\\nFigure 6: Shoot detection and celebration.\\nMobile Information Systems\\n5'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 5}, page_content='verify the time range when the score goes from 0 colon 0 to 1\\ncolon 0.\\n3.7. Recognition of a Person’s Face. For a long period, net-\\nworks with a pure cascade design led the WIDER FACE\\nchallenge. Te MTCNN detector, which conducts both face\\ndetection and face landmarking, was one of the most\\ncommonly utilized. Te network is divided into three\\nsubnetworks: P-Net, R-Net, and O-Net. Te ﬁrst step does\\ncoarse face detection, resulting in proposal zones. Te non-\\nmaximum suppression method next minimizes the amount\\nof overlapping boxed areas, resulting in more speciﬁc re-\\ngions that are supplied to R-Net. R-Net reﬁnes the chosen\\nsuggestions, while O-Net does face landmarking. MTCNN is\\nstill being recommended for use in the cutting-edge facial\\nrecognition system described in. Here, we want to use face-\\nrecognition technology to identify soccer stars in match\\nrecordings. Due to the obscurity of soccer match footage,\\nthis is a challenging task but it can be done. Te bulk of facial\\nrecognition training datasets uses just photos of the front of\\npeople’s faces. In the end, performance is harmed because of\\nthe data bias. It is not necessary to ﬁnetune the ArcFace face-\\nrecognition model since the current model performs well.\\nWe used the MTCNN face detection model, which is a quick\\nand eﬀective model. Prior to recognizing a face, a picture\\nneeds to be registered. Since registration pictures are often\\ncurrent front-facing shots of the subject to be recognized, we\\nare unable to collect recent front-facing photos of soccer\\nstars. In our opinion, soccer match ﬁlms beneﬁt more from\\nthe use of photos than web images do. As a result, we\\nrecommend a two-step approach to spot soccer stars in\\nmatch ﬁlms. Te ﬁrst step is to ﬁnd soccer stars in recent\\nmatch videos by utilizing web photographs as registration\\nimages. For registration photos, we search the Internet for\\nfront-facing photographs of these superstars. Every ﬁve\\nframes in a match video, a face is detected and preprocessed\\nin the same way as a picture is registered on a discovered face\\narea. After preprocessing, we send the identiﬁed facial re-\\ngions to ArcFace for face recognition. In order to get as many\\nstar face shots as possible in match records, we have lowered\\nthe bar. Second, we personally look for a clean front-face\\nphotograph of each celebrity. Each star has a total of around\\n15 oﬃcial photographs. During the trial period, the previ-\\nously prepared registration photographs are utilized. What\\nthe face-recognition module produces is the frame number\\nand location of each star’s face.\\n3.8. Integration Module. Te positions of key frames of\\nhighlights and shot boundary frames are output by the\\nmodules mentioned above. We make a short ﬁlm that lasts\\naround a minute by combining a photo containing a critical\\nframe with neighboring shots. Te highlights of our system\\nare these bundled short videos as shown in Figure 7.\\n4. System Details\\nEach module needs its own speed settings to ensure that the\\nwhole system is real time during the testing period. Te shot\\nsegmentation module is used in every frame. Te input\\nstream is evaluated every eight frames for red and yellow\\ncards, as well as corner kick detection. A 2.56-second movie\\nis created by selecting one frame from the live stream for\\neach eight that are shown throughout the detection process.\\nIn order to import these movies, I3D will be used. Detection\\nof scores, faces, and identiﬁcation are carried out every 25\\nframes.\\n5. Experiments\\n5.1. Aspect Ratio Calculation. Shot segmentation was tested\\nin three 2014 World Cup games: Costa Rica vs. Greece,\\nPortugal vs Ghana, and Belgium vs. Algeria. Te ﬁrst 10,000\\nframes of every 720p video are included in every movie.\\nFigure 8 displays the statistical analysis of the test results. As\\ndemonstrated in Figure 9, our method outperforms the\\ncompetition in terms of accuracy and stability. Figure 8\\nshows the number of applications in each class as well as the\\napplication lists that have been chosen.\\nNo\\nShoot\\nNo\\nNo\\nI3D\\nI3D\\nI3D\\nI3D\\nFigure 7: Detection of both the shooting and the celebration. Te\\nlive feed is broken up into a series of short movies to forecast\\nwhether there will be a shooting or a celebration.\\n0\\nNumber of frames\\n10\\n20\\nCuts\\n30\\n40\\n50\\nCosta Rica vs\\nGreece\\nPortugal vs\\nGhana\\nBelgium vs\\nAlgeria\\n10004\\n10003\\n10002\\n10001\\n10000\\n9999\\n9998\\n9997\\n9996\\nFigure 8: Te data statistics used in the experiments.\\n6\\nMobile Information Systems'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 6}, page_content='5.2. Detection of Shooting and Celebration. In a football\\nmatch video, we examined the C3D, P3D, or I3D renderings\\nto see which one was the best. Based on a movie, we extract\\n1000 segments, each of which is 2.56 seconds in length and\\nconsists of 16 frames. To put it another way, we choose one\\nframe from every four that are available. In addition to the\\n1,000 footage, there are 13 gunshots and ﬁve celebrations.\\nTe outcomes are shown in Figure 6. Tere will be no further\\nattention given to outcomes with less than 0.5 conﬁdences. A\\ngreater percentage of false positives than with C3D have\\nbeen seen using I3D’s test samples, but the experimental data\\nshow that it is capable of identifying all essential actions. We\\nwere more concerned with the model’s ability to distinguish\\nbetween the photography and the celebration, since we\\nwanted to see whether the model was capable of doing so.\\n5.3.FindingScores. A brief match ﬁlm was used to compare the\\nresults of CTPN with reﬁned CTPN. Figure 10 displays the\\nresults. It is the ratio of properly recognized scores to total oc-\\ncurrences that determines a system’s accuracy. In comparison to\\nthe previous technique, our new approach is far more accurate.\\n5.4. Face Detection. Video footage from two World Cup\\nmatches in Brazil is used to assess the facial recognition\\nmodule’s eﬀectiveness. Figure 11 displays the results. Our\\nmethod has a recognition accuracy of more than 98 percent\\nfor most stars. Even in exceedingly intricate landscapes, as\\nseen in Figure 12, these stars may be reliably identiﬁed.\\n5.5. System as a Whole. During the 2018 World Cup, our\\nmethod was used. Highlights are brief movies that cover things\\nlike shooting, red and yellow cards, corner kicks, penalty kicks,\\ngoals, and celebrations. During the game, our system obtained\\n100% recall, but not with great precision. Before uploading\\nsmall movies to the network, manual ﬁltering is necessary.\\nTerefore, recall is more critical than perfection.\\nW/O Reﬁned\\nAccuracy\\nReﬁned (ours)\\nMethod\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nFigure 10: Te score detection results.\\n0\\n120\\n115\\n110\\n105\\n100\\nAbs W/O Avg (ours) Avg (ours)\\nFalsePositive\\n95\\n90\\n85\\n80\\n0\\n10\\n20\\n30\\n40\\nMissed\\n20 40 60 80 100120\\nFigure 9: Shot segmentation results.\\n-4\\n-10\\n0\\n10\\n20\\n30 40\\n50\\n700\\n600\\n500\\n400\\n300\\nRamos Pique de Gea Silva\\nHemandez Manjukie Rakitic Modric\\n200\\n100\\n0\\n600\\n400\\n200\\n0\\n-100\\n0.0\\n0.0\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0.4\\nAccuracy\\nAccuracy\\n0.6\\n0.8\\n1.0\\n-2 0 2 4 6\\nFalsePositive\\nFalsePositive\\n8 101214\\nFigure 11: Face-recognition research projects.\\nMobile Information Systems\\n7'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 7}, page_content='6. Conclusion\\nIn this research, we provide a unique technique for auto-\\nmatically extracting highlights from soccer ﬁlms or match\\nreplays. Tis system is made up of deep learning-based\\naction recognition, object identiﬁcation, and facial recog-\\nnition components. To extract shot boundary frames, we\\ninitially used color histogram characteristics. Te keyframes\\nof the highlights and star shots are then found using deep\\nlearning algorithms. Finally, by merging pictures using\\nkeyframes, stunning short ﬁlms and star clips are created.\\nTe experimental ﬁndings reveal that the diﬀerent com-\\nponents of this system function well. During the 2018 FIFA\\nWorld Cup, our solution was also implemented onto live\\nstreaming platforms and it functioned admirably.\\nData Availability\\nAll data, models, and code generated or used during the\\nstudy appear in the submitted article.\\nConflicts of Interest\\nTe authors declare that they have no conﬂicts of interest\\nand they do not have any commercial or associative interest\\nin connection with the work submitted.\\nAcknowledgments\\nTis work was supported by the College of Arts and Sciences,\\nYangtze University.\\nReferences\\n[1] M. Stoeve, D. Schuldhaus, A. Gamp, C. Zwick, and\\nB. M. Eskoﬁer, “From the laboratory to the ﬁeld: IMU-based\\nshot and pass detection in football training and game sce-\\nnarios using deep learning,” Sensors, vol. 21, no. 9, p. 3071,\\n2021.\\n[2] C. Merhej, R. J. Beal, and T. Matthews, “What happened next?\\nUsing deep learning to value defensive actions in football\\nevent-data,” in Proceedings of the 27th ACM SIGKDD Con-\\nference on Knowledge Discovery & Data Mining, pp. 3394–\\n3403, New York, NY, USA, August 2021.\\n[3] B. Wang, W. Shen, and F. S. Chen, “Football match intelligent\\nediting system based on deep learning,” KSII Transactions on\\nInternet and Information Systems (TIIS), vol. 13, no. 10,\\npp. 5130–5143, 2019.\\n[4] M. R. Begum, M. R. NarasingaRao, and N. Polala, “Deep\\nlearning based Football player’s Health Analysis,” Journal For\\nInnovative Development in Pharmaceutical and Technical\\nScience (JIDPTS), vol. 4, no. 8, 2021.\\n[5] D. Tang, “Hybridized hierarchical deep convolutional neural\\nnetwork for sports rehabilitation exercises,” IEEE Access,\\nvol. 8, pp. 118969–118977, 2020.\\n[6] H. Li, “Analysis on the construction of sports match pre-\\ndiction model using neural network,” Soft Computing, vol. 24,\\nno. 11, pp. 8343–8353, 2020.\\n[7] N. Abdelhade, T. H. A. Soliman, and H. M. Ibrahim,\\n“Detecting twitter users’ opinions of Arabic comments during\\nvarious time episodes via deep neural network,” in Pro-\\nceedings of the International Conference on Advanced Intel-\\nligent Systems and Informatics, pp. 232–246, Cairo, Egypt,\\nOctober 2017.\\n[8] A. Tejero-de-Pablos, Y. Nakashima, T. Sato, N. Yokoya,\\nM. Linna, and E Rahtu, “Summarization of user-generated\\nsports video by using deep action recognition features,” IEEE\\nTransactions on Multimedia, vol. 20, no. 8, pp. 2000–2011,\\n2018.\\n[9] H. Ma and X. Pang, “Research and analysis of sport medical\\ndata processing algorithms based on deep learning and In-\\nternet of things,” IEEE Access, vol. 7, pp.118839–118849, 2019.\\n[10] X. Ning, W. Tian, W. Li et al., “BDARS_CapsNet: Bi-direc-\\ntional attention routing sausage capsule network,” IEEE Ac-\\ncess, vol. 8, pp. 59059–59068, 2020.\\n[11] A. Ekin and M. Tekalp, “Generic play-break event detection\\nfor summarization and hierarchical sports video analysis,” in\\nProceedings of the 2003 international conference on multi-\\nmedia and Expo. ICME’03, pp. 1–169, Baltimore, MD, USA,\\nJuly 2003.\\nFigure 12: In the 2018 FIFA World Cup, face detection and recognition will be used. Here are some instances of how to correctly identify\\nand recognize objects in diﬃcult situations, such as those including side faces, occlusion, and blur.\\n8\\nMobile Information Systems'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-08-11T05:35:21+05:30', 'source': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/02.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-08-11T01:22:56+00:00', 'trapped': '', 'modDate': 'D:20220811012256Z', 'creationDate': \"D:20220811053521+05'30'\", 'page': 8}, page_content='[12] K. Tirdad, A. Dela Cruz, A. Sadeghian, and M. Cusimano, “A\\ndeep neural network approach for sentiment analysis of\\nmedically related texts: an analysis of tweets related to con-\\ncussions in sports,” Brain informatics, vol. 8, no. 1, pp. 12–17,\\n2021.\\n[13] H. Pan, B. Li, and M. I. Sezan, “Automatic detection of replay\\nsegments in broadcast sports programs by detection of logos\\nin scene transitions,” in Proceedings of the 2002 IEEE Inter-\\nnational Conference on Acoustics, Speech, and Signal Pro-\\ncessing, pp. 3385–3388, Orlando, USA, 2002.\\n[14] X. Du and W. Cai, “Simulating a basketball game with hdp-\\nbased models and forecasting the outcome,” in Proceedings of\\nthe 2018 7th International Conference on Digital Home\\n(ICDH), pp. 193–199, Guilin, China, December 2018.\\n[15] N. Ancona, G. Cicirelli, and A. Branca, “Goal detection in\\nfootball by using support vector machines for classiﬁcation,”\\nin Proceedings of the International Joint Conference on Neural\\nNetworks, pp. 611–616, Washington, DC, USA, July 2001.\\n[16] H. M. Zawbaa, N. El-Bendary, and A. E. Hassanien, “Event\\ndetection based approach for soccer video summarization\\nusing machine learning,” International Journal of Multimedia\\nand Ubiquitous Engineering, vol. 7, no. 2, pp. 63–80, 2012.\\n[17] E. Fendri, H. Ben-Abdallah, and A. B. Hamadou, “A novel\\napproach for soccer video summarization,” in Proceedings of\\nthe 2010 second international conference on multimedia and\\ninformation technology, pp. 138–141, IEEE, Kaifeng, China,\\nApril 2010.\\n[18] A. Ekin, A. M. Tekalp, and R. Mehrotra, “Automatic soccer\\nvideo analysis and summarization,” IEEE Transactions on\\nImage Processing, vol. 12, no. 7, pp. 796–807, 2003.\\n[19] E. Lotﬁand H. R. Pourreza, “Event detection and automatic\\nsummarization in soccer video,” in Proceedings of the 4th\\nIranian Conference on Machine Vision and Image Processing\\n(MVIP07), Mashhad, Iran, 2007.\\n[20] Y. Tabii and R. O. Tami, “A new method for soccer video\\nsummarizing based on shot detection, classiﬁcation and ﬁnite\\nstate machine,” in Proceedings of the 5th international con-\\nference SETIT, March 2009.\\n[21] R. Yan, J. Tang, and X. Shu, “Participation-contributed\\ntemporal dynamic model for group activity recognition,” in\\nProceedings of the 26th ACM international conference on\\nMultimedia, pp. 1292–1300, New York, NY, USA, October\\n2018.\\n[22] P. Chen, Q. Xiao, and J. Xu, “Facial attribute editing using\\nsemantic segmentation,” in Proceedings of the 2019 Interna-\\ntional Conference on High Performance Big Data and Intel-\\nligent Systems (HPBD&IS), pp. 97–103, IEEE, Shenzhen,\\nChina, May 2019.\\n[23] P. Chen, Q. Xiao, and J. Xu, “Harnessing semantic seg-\\nmentation masks for accurate facial attribute editing,” Con-\\ncurrency and Computation: Practice and Experience, vol. 34,\\nno. 12, Article ID e5798, 2020.\\n[24] X. Ning, D. Gou, and X. Dong, “Conditional generative\\nadversarial networks based on the principle of homo-\\nlogycontinuity for face aging,” Concurrency and Computation:\\nPractice and Experience, vol. 34, no.12, Article ID e5792, 2020.\\n[25] S. Li, L. Sun, and X. Ning, “Head pose classiﬁcation based on\\nline portrait,” in Proceedings of the 2019 International Con-\\nference on High Performance Big Data and Intelligent Systems\\n(HPBD&IS), pp. 186–189, Shenzhen, China, May 2019.\\n[26] X. Ning, W. Li, B. Tang, and H. He, “BULDP: biomimetic\\nuncorrelated locality discriminant projection for feature ex-\\ntraction in face recognition,” IEEE Transactions on Image\\nProcessing, vol. 27, no. 5, pp. 2575–2586, 2018.\\n[27] W. Cai, B. Zhai, Y. Liu, R. Liu, and X. Ning, “Quadratic\\npolynomial guided fuzzy C-means and dual attention\\nmechanism for medical image segmentation,” Displays,\\nvol. 70, Article ID 102106, 2021.\\n[28] W. Cai, D. Liu, X. Ning, C. Wang, and G. Xie, “Voxel-based\\nthree-view hybrid parallel network for 3D object classiﬁca-\\ntion,” Displays, vol. 69, no. 1, Article ID 102076, 2021.\\n[29] Z. Chen, J. Huang, and H. Ahn, “Costly features classiﬁcation\\nusing Monte Carlo tree search,” in Proceedings of the 2021\\nInternational Joint Conference on Neural Networks (IJCNN),\\npp. 1–8, Shenzhen, China, June 2021.\\n[30] H. Ueda, T. Miyatake, and S. Yoshizawa, “IMPACT: an in-\\nteractive\\nnatural-motion-picture\\ndedicated\\nmultimedia\\nauthoring system,” in Proceedings of the SIGCHI conference on\\nHuman factors in computing systems, pp. 343–350, Massa-\\nchusetts, USA, April 1991.\\n[31] R. Girshick, J. Donahue, and T. Darrell, “Rich feature hier-\\narchies for accurate object detection and semantic segmen-\\ntation,” in Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pp. 580–587, 2014.\\n[32] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling\\nin deep convolutional networks for visual recognition,” IEEE\\nTransactions on Pattern Analysis and Machine Intelligence,\\nvol. 37, no. 9, pp. 1904–1916, 2015.\\n[33] X. Du, S. Tang, Z. Lu, K. Gai, J. Wu, and P. C. K. Hung,\\n“Scientiﬁc workﬂows in IoT environments: a data placement\\nstrategy based on heterogeneous edge-cloud computing,”\\nACM Transactions on Management Information Systems,\\n2022.\\n[34] H. Wang and C. Schmid, “Action recognition with improved\\ntrajectories,” in Proceedings of the IEEE International Con-\\nference on Computer Vision, pp. 3551–3558, Sydney, NSW,\\nAustralia, December 2013.\\nMobile Information Systems\\n9'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 0}, page_content='applied  \\nsciences\\nArticle\\nShot Classiﬁcation of Field Sports Videos Using\\nAlexNet Convolutional Neural Network\\nRabia A. Minhas 1, Ali Javed 1\\n, Aun Irtaza 2, Muhammad Tariq Mahmood 3\\nand\\nYoung Bok Joo 3,*\\n1\\nDepartment of Software Engineering, University of Engineering and Technology, Taxila 47050, Pakistan;\\nafzalminhasrabia@gmail.com (R.A.M); ali.javed@uettaxila.edu.pk (A.J.)\\n2\\nDepartment of Computer Science, University of Engineering and Technology, Taxila 47050, Pakistan;\\naun.irtaza@uettaxila.edu.pk\\n3\\nSchool of Computer Science and Engineering, Korea University of Technology and Education,\\n1600 Chungjeolno, Byeogchunmyun, Cheonan 31253, South Korea; tariq@koreatech.ac.kr\\n*\\nCorrespondence: ybjoo@koreatech.ac.kr; Tel.: +82-041-560-1487\\nReceived: 25 November 2018; Accepted: 28 January 2019; Published: 30 January 2019\\n\\x01\\x02\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\nAbstract: Broadcasters produce enormous numbers of sport videos in cyberspace due to massive\\nviewership and commercial beneﬁts. Manual processing of such content for selecting the important\\ngame segments is a laborious activity; therefore, automatic video content analysis techniques\\nare required to effectively handle the huge sports video repositories. The sports video content\\nanalysis techniques consider the shot classiﬁcation as a fundamental step to enhance the probability\\nof achieving better accuracy for various important tasks, i.e., video summarization, key-events\\nselection, and to suppress the misclassiﬁcation rates. Therefore, in this research work, we propose\\nan effective shot classiﬁcation method based on AlexNet Convolutional Neural Networks (AlexNet\\nCNN) for ﬁeld sports videos. The proposed method has an eight-layered network that consists of ﬁve\\nconvolutional layers and three fully connected layers to classify the shots into long, medium, close-up,\\nand out-of-the-ﬁeld shots. Through the response normalization and the dropout layers on the feature\\nmaps we boosted the overall training and validation performance evaluated over a diverse dataset\\nof cricket and soccer videos. In comparison to Support Vector Machine (SVM), Extreme Learning\\nMachine (ELM), K-Nearest Neighbors (KNN), and standard Convolution Neural Network (CNN),\\nour model achieves the maximum accuracy of 94.07%. Performance comparison against baseline\\nstate-of-the-art shot classiﬁcation approaches are also conducted to prove the superiority of the\\nproposed approach.\\nKeywords: AlexNet CNN; convolutional neural networks; deep learning; rectiﬁed linear unit layer;\\nshot classiﬁcation\\n1. Introduction\\nThe last few decades have witnessed the production and transmission of a massive amount of\\nmultimedia content in the World Wide Web. Sports videos contribute a major portion of the available\\nmultimedia content in cyberspace. Sports video content analysis has been explored heavily due to\\nthe potential commercial beneﬁts and massive viewership around the world. The manual annotation\\nof such an enormous video content is a challenging task. Therefore, automated methods for sports\\nvideo content analysis and management are required. Humans are inclined towards meaningful\\nconcepts, while viewing and exploring interactive databases; there is a growing need for indexing and\\nsemantic video content analysis. The major goal towards video content analysis and management is\\nto provide semantic information relevant to the user. For this purpose, researchers have proposed\\nAppl. Sci. 2019, 9, 483; doi:10.3390/app9030483\\nwww.mdpi.com/journal/applsci'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 1}, page_content='Appl. Sci. 2019, 9, 483\\n2 of 21\\nvarious automated and semi-automated techniques to analyze the sports videos for shot classiﬁcation,\\nkey-events detection, and video summarization.\\nShot Classiﬁcation approaches are commonly applied initially for sports video summarization\\nand content retrieval applications to provide the semantic information. Shot Classiﬁcation techniques\\nare employed to categorize the sports videos into different views, i.e., long, medium, close-up, etc.\\nThe classiﬁcation makes it convenient to further process the sports videos for various applications\\nmentioned above. The effectiveness of these applications largely depends on accurate shot classiﬁcation\\nof the sports videos. However, shot classiﬁcation is very challenging in the presence of camera\\nvariations, scene change, action speeds, illumination conditions (i.e., daylight, artiﬁcial light, and\\nshadow), etc. Therefore, there exists a need to develop an effective shot classiﬁcation method that is\\nrobust to the above-mentioned limitations. The main idea is to develop a shot classiﬁcation scheme\\nthat can accurately classify the frames of ﬁeld sports videos in the presence of the aforementioned\\nlimitations. Soccer and Cricket ﬁeld sports are selected to evaluate the effectiveness of our framework\\ndue to the massive viewership of these sports across the globe.\\nSports videos contain different types of shots that can be categorized into following categories:\\n(i) Long shot, (ii) medium shot, (iii) close-up shot, and (iv) crowd/out-of-the-ﬁeld shots. Long shots\\nin sports videos are considered zoomed-out shots that covers a large area or a landscape of the ﬁeld,\\nthe audience and the players are presented on a small scale. Medium shots are zoomed-in shots\\nthat mostly cover a full body of the player from top head of the body to the bottom of the feet or\\nmore than one player within the playground ﬁeld. Close-up shots are the most zoomed-in shots that\\ncontain the players upper body footage in the scene. Crowd shots/out-of-the-ﬁeld shots consist of\\nviewers/audience footage in the sports videos.\\nMost of the existing methods classify the in-ﬁeld segments of sports videos, i.e., long, medium,\\nand close-up shots as these shots are more useful to generate the highlights. After viewing massive\\nnumber of sports videos, it has been observed that crowd/out-of-the-ﬁeld shots also contain few\\nexciting events that can be included in the summarized video. Therefore, we proposed an automated\\nshot classiﬁcation method of sports videos into long, medium, close-up and crowd/out-of-the-ﬁeld\\nshots. Shot classiﬁcation methods have applications in various domains, i.e., sports [1], medical [2],\\nnews [3], entertainment [4], and documentaries [5].\\nExisting approaches of shot classiﬁcation can be categorized into (i) learning-based, and (ii)\\nnon-learning-based. Learning-based approaches employ various classiﬁers (i.e., Support Vector\\nMachines (SVM), Kth Nearest Neighbor (KNN), decision trees, neural networks, etc.) to perform shot\\nclassiﬁcation. Whereas, non-learning-based approaches use various statistical features, i.e., dominant\\ngrass color, histogram difference comparison, etc., to design different shot classiﬁcation methods.\\nExisting methods [6–10] have used non-learning-based approaches for shot classiﬁcation of sports\\nvideos. Divya et al. [6] proposed an effective shot classiﬁcation method for basketball videos. Edge\\npixel ratio was used to classify the shots into two categories only: (i) Close-up view, and (ii) long view.\\nLogo transition detection and logo sample selection were used to identify the replays. Rahul et al. [7]\\nproposed a method to assign the semantic information to cricket videos. The input video was\\nsegmented into shots using the scene information extracted from the text commentary. Video shots were\\nclassiﬁed into different categories, i.e., batsman action of pull shot, etc. Rodrigo et al. [8] proposed a shot\\nclassiﬁcation method for soccer videos using the Bhattacharyya distance [9]. Murat et al. [10] proposed\\na soccer video summarization method using cinematic and objects-based features. The dominant\\nﬁeld color was used to detect the soccer ﬁeld. Grass pixel ratio and color histogram comparison were\\nused to detect the shot boundaries. Finally, statistical analysis was used to classify the shots into long,\\nmedium, and close-up shots.\\nIn literature, approaches [11–22] also use learning-based techniques for shot classiﬁcation.\\nLearning-based approaches provide better accuracy as compared to non-learning-based approaches,\\nbut at the expense of increased computational cost. Rahul et al. [11] presented an automated approach\\nto identify the types of views and detection of the signiﬁcant events such as goal, foul, etc, using bag'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 2}, page_content='Appl. Sci. 2019, 9, 483\\n3 of 21\\nof words and SVM. Sigari et al. [12] used rule-based heuristics and SVM to classify the shots into\\nfar view/long shot, medium view, close-up view, and out-ﬁeld view. Ling-Yu et al. [13] proposed a\\nframework for semantic shot classiﬁcation in sports videos. Low-level features were derived using\\ncolor, texture, and DC images in compressed domain. These low-level features were converted to\\nmid-level features such as camera motion, action regions and ﬁeld shape properties, etc. Mid-level\\nrepresentation was necessary to use the classiﬁers that do not map on low-level features. Finally, shots\\nwere classiﬁed using different supervised learning algorithms like decision trees, neural networks,\\nsupport vectors machine (SVM), etc. Camera shots were classiﬁed into mid shots, long shots, and\\nclose-up shots. Matthew et al. [14] proposed and evaluated deep neural network architectures that were\\nable to combine image information across a video over longer time periods. They [14] have proposed\\ntwo methods for handling full length videos. Results were evaluated over previously published\\ndatasets like Sports 1 million dataset, UCF-101 Dataset. The ﬁrst method explored convolutional neural\\nnetworks that was used to examine the design choices needed while adapting to this framework.\\nThe second method used an ordered sequence of frames that were employed to recurrent neural\\nnetwork based on Long Short-Term Memory (LSTM) cells connected to the output of underlying CNN.\\nKarmaker et al. [15] proposed a cricket shot classiﬁcation approach using motion vector detection\\nthrough the optical ﬂow method. 3D MACH ﬁlter was used for action recognition that was trained\\nover six cricket videos to classify the different shots. Kapela et al. [16] proposed a learning-based\\nmethod that used feed-forward neural networks, ELMAN neural network, and decision trees to\\nclassify the different events in ﬁeld sports videos. Konstantinos et al. [17] presented a shot classiﬁcation\\nmodel using linear discriminant analysis (LDA) method to categorize the shots into full long view\\nand player medium view. Atika et al. [18] proposed a shot classiﬁcation approach using statistical\\nfeatures to classify the crowd shots. Nisarg et al. [19] proposed a multi labelled video dataset that\\ncontained over eight million videos, 500k hours of video, annotated with 4800 visual entities. The\\nvideos were labelled using Youtube vide annotation system. Each video was decoded using Deep CNN\\npre-trained on ImageNet to extract hidden representation immediately. Training was performed on\\ndifferent classiﬁcation models on the dataset. The videos were classiﬁed onto different categories like\\nvehicles, sports, concert, and animated, etc. Jungheon et al. [20] proposed a video event classiﬁcation\\nalgorithm using audio-visual features. Convolutional neural networks were applied on the frames to\\nextract the features followed by performing the classiﬁcation. In addition, Mel Frequency Cepstral\\nCoefﬁcients (MFCCs) were also used to train the CNN for shot classiﬁcation. Loong et al. [21] proposed\\na semantic shot classiﬁcation algorithm for cinematography. Markov random ﬁeld model based on\\nmotion segmentation was used to classify the ﬁlm video into three types of shots, i.e., long, medium,\\nand close-up. Ashok et al. [22], proposed a hybrid classiﬁer-based approach for activity detection of\\ncricket videos. Low-level features (i.e., grass pixel ratio) and mid-level features (i.e., camera view,\\ndistance, etc.) were used to train a hybrid classiﬁer comprising of Naïve bias, KNN and multi-class\\nSVM for shot classiﬁcation into ﬁeld-view and non-ﬁeld views.\\nAs we already discussed that effective shot classiﬁcation improves the accuracy of video content\\nanalysis applications. However, shot classiﬁcation is very challenging in the presence of camera\\nvariations, scene change, action speeds, illumination conditions (i.e., daylight, artiﬁcial light, shadow),\\netc. To address the challenges associated with shot classiﬁcation, we proposed an effective shot\\nclassiﬁcation framework for ﬁeld sports videos. The major contributions of the proposed research\\nwork are as follows:\\n•\\nAlexNet convolution neural network is designed to effectively classify the video shots into\\ndifferent views (i.e., long, medium, close-up, crowd/out-of-the-ﬁeld) which is promising and\\nnovel in terms of its application to shot classiﬁcation.\\n•\\nThe proposed framework is robust to camera variations, scene change, action speeds, and\\nillumination conditions and can be reliably used for shot classiﬁcation in the presence of\\nthese limitations.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 3}, page_content='Appl. Sci. 2019, 9, 483\\n4 of 21\\nMoreover, existing shot classiﬁcation approaches focus on generating more classes for in-ﬁeld\\nsegments (long, medium and close-up shots) because in-ﬁeld segments are commonly used to generate\\nthe sports highlights. However, it has been observed after watching many sports videos that the crowd\\nshots also contain few exciting segments that can be included in the summarized video. Therefore,\\nthe proposed research work focuses on classifying the shots of ﬁeld sports videos into long, medium,\\nclose-up, and crowd/out-of-the-ﬁeld shots. We categorize the crowd shots into separate class so that\\nthese shots can also be analyzed to further identify the interesting segments for video summarization.\\nThese crowd shots can also be used for different applications, i.e., activity detection, etc. The details of\\nthe proposed deep learning framework are provided in the next section.\\n2. Proposed Method\\nThis section provides a comprehensive discussion on the proposed shot classiﬁcation method.\\nThe proposed deep learning scheme applies the AlexNet Convolutional Neural Network (CNN)\\narchitecture to classify the shots into long, medium, close-up, and crowd/out-of-the-ﬁeld shots.\\nThe input layer of the CNN is in three dimensions: The width, height, and depth of the pixel. The width\\nand height represent horizontal and vertical pixels, whereas depth represents the RGB color channel.\\nWe have transformed the input sports video dataset in frames to reduce computational complexity\\ninvolved in training the network. The process ﬂow of the proposed framework is presented in Figure 1.\\nFigure 1. Proposed framework for shot classiﬁcation.\\n2.1. AlexNet Architecture\\nIn the proposed work, we employed the Alexnet CNN deep learning architecture [23] for shot\\nclassiﬁcation in ﬁeld sports videos. The network is deeper than standard CNN with ﬁve convolution\\nlayers followed by three maximum pooling layers. Dropout of 0.5% is applied on the fully connected\\nlayers to avoid over ﬁtting of the data. The architecture consists of the following components:\\n•\\n1 Convolution with 11 × 11 kernel size (1CONV)\\n•\\nRectiﬁed Linear Unit Layer Activation (RELU)'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 4}, page_content='Appl. Sci. 2019, 9, 483\\n5 of 21\\n•\\nResponse Normalization Layer\\n•\\n1 Maximum Pooling (4 × 4 kernel)\\n•\\n2 Convolution with 5 × 5 kernel size (2CONV)\\n•\\nRectiﬁed Linear Unit Layer (RELU)\\n•\\nResponse Normalization Layer\\n•\\n2 Maximum Pooling (3 × 3)\\n•\\n3 Convolution with 3 × 3 kernel size (3CONV)\\n•\\nRectiﬁed Linear Unit Layer Activation (RELU)\\n•\\n4 Convolution with 3 × 3 kernel size\\n•\\nRectiﬁed Linear Unit Layer Activation (RELU)\\n•\\n3 Maximum Pooling (3 × 3)\\n•\\nFully Connected Layer (4096 nodes)\\n•\\nRectiﬁed Linear Unit Layer Activation (RELU)\\n•\\nFully Connected Layer (4096 nodes)\\n•\\nRectiﬁed Linear Unit Layer (RELU)\\n•\\nSoft-max out\\nThe proposed AlexNet CNN architecture is presented in Figure 2. In the proposed research work,\\nimage input layer is deﬁned as a pre-processing layer where the input frames are down-sampled from\\n640 × 480 to 227 × 227 in terms of spatial resolution to reduce the computational cost of our deep\\nlearning framework. The proposed system uses ﬁve convolutional (CONV) layers followed by three\\npooling layers (POOL) and Rectiﬁed Linear Unit (RELU). For the ﬁrst convolutional layer, 96 kernels of\\nrelatively large size 11 × 11 × 3 are used. For the second convolutional layer, 256 kernels of size 5 × 5\\nare used. For the third, fourth, and ﬁfth layers, 384 kernels of size 3 × 3 are used. Each convolutional\\nlayer generates a feature map. The feature maps of ﬁrst, second and ﬁfth convolutional layer are used\\nin combination with pooling layers of 3 × 3 and stride of 2 × 2. The framework is comprised of eight\\nlayered architecture with 4096 nodes. This generates the trainable feature maps, i.e., feature extraction\\nphenomena are performed in these layers. These feature maps are subjected to fully connected layers\\n(FC) and then Soft-max activation is performed to determine the classiﬁcation probabilities used by\\nthe ﬁnal output classiﬁcation layer. These classiﬁcation probabilities in the Soft-max layer can make\\ncategories up to 1000 different classes, but in our dataset, we have four classes.\\n \\nFigure 2. AlexNet convolution neural network architecture of the proposed framework.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 5}, page_content='Appl. Sci. 2019, 9, 483\\n6 of 21\\n2.1.1. Convolution Network Layer\\nThis is the most signiﬁcant layer in deep learning phenomena of neural networks that generates\\nthe feature maps which are subjected to classiﬁcation layers. It consists of a kernel that slides over\\nthe input frame, which generates the output known as feature map. At every location on the input,\\nwe performed matrix multiplication followed by integrating the result. The output feature map is\\ndeﬁned as:\\nNr\\nx = Nr−1\\nx\\n−Lr\\nx\\nSrx\\n+ 1; Nr\\ny =\\nNr−1\\ny\\n−Lr\\ny\\nSry\\n+ 1\\n(1)\\nwhere (Nx, Ny) is the width and height of the output feature map of the last layer and (Lx, Ly) is the\\nkernel size, (Sx,, Sy) that deﬁnes the number of pixels skipped by the kernel in horizontal and vertical\\ndirections and index r indicates the layer i.e., r = 1. Convolution is applied on the input feature map\\nand a kernel to get the output feature map that is deﬁned as:\\nX1(m, n) = (J ∗R)(m, n)\\n(2)\\nwhere X1 (m, n) is a two-dimensional output feature map obtained by convolving the two-dimensional\\nkernel R of size (Lx, Ly) and input feature map J. The sign * is used to represent the convolution between\\nJ and R. The convolution operation is expressed as;\\nX1(m, n) =\\np=+ Lx\\n2\\n∑\\np=−Lx\\n2\\nq=+\\nLy\\n2\\n∑\\nq=−\\nLy\\n2\\nJ(m −p, n −q)R(p, q)\\n(3)\\nIn the proposed framework, we used ﬁve CONV layers with RELU layer and response\\nnormalization layer to extract the maximum feature maps form the input frames to train the dataset\\nwith maximum accuracy.\\n2.1.2. Rectiﬁed Linear Unit Layer\\nIn the next stage, we applied the RELU activation function to all the trainable layers to strengthen\\nour network by making it non-linear. This layer accounts the non-linearities in an adequate manner.\\nIt is applied over the output feature map which is generated from the convolutional layer. The use of\\ntanh(.) and RELU activation function saturates the non-linear gradient descent in terms of training\\ntime. tanh(.) is expressed as:\\nX2(m, n) = tanh(X1(m, n)) = sinh(X1(m, n))\\ncosh(X1(m, n)) = 1 + 1 −e−2∗X1(m,n)\\n1 + e−2∗X1(m,n)\\n(4)\\nwhere X2(m, n) is a two-dimensional output feature map after applying tanh(.) on the input feature\\nmap X1(m, n), which is achieved after passing through the convolutional layer. The values in the ﬁnal\\nfeature map are obtained after applying the RELU function as follows:\\nX(m, n) =\\n(\\n0, i f X2(m, n < 0)\\nX2(m, n), i f X2(m, n ≥0)\\n(5)\\nwhere X(m, n) is obtained by transforming the negative values into zero and returns the same value\\nback on receiving any positive value. We included the RELU layer in our proposed framework since\\ndeep convolutional neural networks train at a much faster pace when intact with the RELU layer.\\n2.1.3. Maximum Pooling Layer\\nA pooling layer is included in the proposed architecture after ﬁrst and second convolution layer\\nand then after the ﬁfth convolution layer to decrease the spatial size of each frame to reduce the'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 6}, page_content='Appl. Sci. 2019, 9, 483\\n7 of 21\\ncomputational cost of the proposed deep learning framework. The pooling operation usually averages\\nor simply pick the maximum value for each slice of the image. In the proposed work, we apply\\npooling by using the maximum value against each slice as we obtained better results on this setting.\\nThe application of the maximum pooling layer on the activation output for down-sampling the images\\nis demonstrated in Figure 3.\\n \\nFigure 3. Maximum pooling layer.\\n2.1.4. Response Normalization Layer and the Softmax Activation\\nResponse Normalization is performed after ﬁrst two sessions to reduce the test error rate of the\\nproposed network. This layer normalizes the input layers within networks along with the input of\\nentire network. Normalization is performed as follows:\\nNx\\ne,q =\\nbx\\ne,f\\n\\x10\\nz + α ∑\\nmin(T−1,x+c/2)\\nj=max(0,x−c/2)\\n\\x00bx\\ne,f\\n\\x012\\x11γ\\n(6)\\nwhere Nx\\ne,f represents the normalization of activity bx\\ne,f of neurons which is computed at position (e,f)\\nwith the use of kernel k. T is the total range of kernels within the layers. z, c, α, and γ are the constants\\nhyperparameters and their values are adjusted by applying a validation set respectively.\\nSoft-max is a classiﬁer on top of the extracted features. After performing ﬁve series of the\\nconvolutional network layer, the output is fed to the Soft-max layer for multi class classiﬁcation\\nthat helps to determine the classiﬁcation probabilities. These probabilities are then used by the ﬁnal\\nclassiﬁcation layer to classify the frames into long, medium, close-up, and crowd/out-ﬁeld views.\\n2.1.5. Dropout Layer\\nThe dropout layer is applied in the ﬁrst two fully connected layers when the number of iterations\\ndoubles in our network to avoid overﬁtting of the data by increasing number of iterations by a factor\\nof two, making the neurons dense. It performs the model averaging with neural networks and is a very\\nefﬁcient technique to regularize training data. Maximum pooling layers, kernel sizes of convolutional\\nlayer, and their skipping factors are processed such that the output feature maps are down sampled to\\none pixel per map. Fully connected layer also connects the output of the top most layers to 1D feature\\nvector. The upper layer is always completely connected with the output unit for class label, such that\\nextracting high level features form the training data Figure 4 depicts the regularization technique on\\nfully connected layers before and after applying dropout.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 7}, page_content='Appl. Sci. 2019, 9, 483\\n8 of 21\\nFigure 4. Fully connected layers (FC) before and after applying dropout.\\n3. Results and Discussion\\nThis section presents the experiments designed to evaluate the performance of the proposed\\nframework. The results of these experiments are also reported along with the discussion. Objective\\nevaluation metrics (i.e., precision, recall, accuracy, error rate, F1-score) are employed for performance\\nevaluation. The details of the dataset are also provided in this section.\\n3.1. Dataset\\nFor performance evaluation we selected a diverse dataset comprising of soccer and cricket videos\\nfrom YouTube as done by the comparative methods, i.e., [24–26]. The dataset includes 10 videos of\\n13 h from six major broadcasters, i.e., ESPN, Star Sports, Ten Sports, Sky Sports, Fox Sports, and Euro\\nSports. In addition, we included the sports videos of different genre and tournaments in our dataset.\\nCricket videos consist of 2014 One Day International (ODI) tournament between South Africa and\\nNew Zealand, 2006 ODI tournament between Australia and South Africa, 2014 test series and ODI\\nseries between Australia and Pakistan, 2014 ODI series between South Africa and New Zealand, 2014\\nTwenty20 cricket world-cup tournament, and 2015 ODI cricket world-cup tournament. For soccer\\nvideos, we considered the 2014 FIFA world-cup and 2016 Euro-cup videos. Soccer videos consist of\\nBrazil vs. Germany semiﬁnal match and Argentina vs. Germany ﬁnal match of 2014 FIFA world-cup,\\nPortugal vs. France ﬁnal match and France vs. Germany semiﬁnal match of 2016 Euro-cup.\\nEach video has a frame resolution of 640 × 480, frame rate of 30 fps and recorded in MPEG-1\\nformat.\\nThe videos represent different illumination conditions (i.e., daylight, artiﬁcial lights).\\nThe dataset videos are comprised of different shot types, i.e., long, medium, close-up, and crowd shots,\\nas shown in Figure 5. We used 70% frames of our dataset for training purpose and rest of the 30% for\\nvalidation purpose. Our dataset videos can also be accessed at Reference [27] for research purposes.\\n \\nFigure 5. Snapshots for cricket (row-1) and soccer (row-2) videos.\\n3.2. Experimental Setup\\nWe have trained our dataset using Alexnet CNN to classify four different classes presented in our\\ndataset. Transfer learning of a network is presented in Figure 6.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 8}, page_content='Appl. Sci. 2019, 9, 483\\n9 of 21\\nFigure 6. Transfer learning of AlexNet CNN network.\\nTraining\\nThe network takes four epochs in four to ﬁve days to train on two GTX 580 Graphic Processing\\nUnits (GPU). An epoch is the number of times training vectors are used once to update the weights.\\nIn our system, each epoch has 500 iterations for our dataset. A stochastic approximation of gradient\\ndescent is used to perform training iterations on the dataset. The stochastic gradient descent (SGD) is\\napplied with a learning rate of 0.0001, momentum of 0.9 and weight decay of 0.0005, respectively. SGD\\nis represented in Equations (7) and (8).\\nsl+1 := 0.9. sl −0.0005. ϵ. xl −ϵ.⟨∂L\\n∂x |xl⟩Bl\\n(7)\\nxl+1 = xl + sl+1\\n(8)\\nwhere l is the iteration index, s is the momentum variable, and ϵ is the learning rate. ⟨∂L\\n∂x |xl⟩Bl is\\nconstant over the lth iteration of batch Bl of x evaluated at xl. All the layers in our network have equal\\nlearning rate that can be adjusted during the training. Experiments have proved that, by increasing\\nthe learning features, validation set achieves better accuracy. We divided our dataset on videos level,\\nthat means we performed training on a dataset of soccer and cricket videos and tested the unique\\nvideo dataset of soccer and cricket videos on the proposed network. Snapshots of the training progress\\nare presented in Figure 7.\\n \\nFigure 7. Training progress report.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 9}, page_content='Appl. Sci. 2019, 9, 483\\n10 of 21\\n3.3. Evaluation Parameters\\nWe employed objective metrics to evaluate the performance of the proposed framework. For this\\npurpose, we used precision (P), recall (R), accuracy (A), error rate (E), and F1-score to measure the\\nperformance. These metrics are computed in terms of correct/incorrect classiﬁcation of shot types for\\neach category. Finally, the results of all types of shots are averaged to obtain the ﬁnal values.\\nFor shot classiﬁcation of sports videos, accuracy represents the ratio of correctly classiﬁed shots\\n(True Positives and True Negatives) out of the total number of shots. Accuracy is computed as follows:\\nA = TP + TN\\nP + N\\n(9)\\nwhere true positives (TP) represent the correctly classiﬁed shots of a positive (P) class (i.e., long shot\\nif we are measuring the accuracy of long shot). And, true negatives (TN) represent the correctly\\nclassiﬁed shots of negative (N) class (i.e., medium, close-up and crowd shots in case of measuring long\\nshot accuracy).\\nError rate refers to the ratio of misclassiﬁed shots (False Positives (FP) and False Negatives (FN))\\nto the total examined shots. Error rate is computed as:\\nE = FP + FN\\nP + N\\n(10)\\nwhere FP represent the N class shots misclassiﬁed as positive class shots. Additionally, FNrepresent\\nthe P class shot misclassiﬁed as the negative class shot.\\nPrecision represents the ratio of correctly labelled shots over the total number of shots and\\ncomputed as follows:\\nP =\\nTP\\nTP + FP\\n(11)\\nRecall is the fraction of true detection of the shots over a total number of shots in the video and\\ncomputed as:\\nR =\\nTP\\nTP + FN\\n(12)\\nF1-score represents the harmonic mean of precision and recall. F1-score is useful metric for\\nperformance comparison in cases where some method have better precision but lower recall rate than\\nthe other method. In this scenario, precision and recall rates independently are unable to provide\\ntrue comparison. Therefore, F1-score can reliably be used in such cases for performance comparison.\\nF1-score is computed as:\\nF1 = P ∗R\\nP + R\\n(13)\\n3.4. Performance Evaluation\\nIn this experiment, we computed the precision, recall, accuracy, error rate, and F-1 score against\\neach shot category of 10 different sports videos (soccer and cricket). The results obtained for each\\nclass of shot are presented in Table 1. The proposed method achieves an average precision of 94.8%,\\nrecall of 96.24 %, F1-score of 96.49%, accuracy of 94.07% and error rate of 5.93% on these videos. These\\nresults indicate the effectiveness of our proposed AlexNet CNN framework for shot classiﬁcation of\\nsports videos.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 10}, page_content='Appl. Sci. 2019, 9, 483\\n11 of 21\\nTable 1. Performance evaluation for the proposed system.\\nVideos\\nAccuracy (%)\\nError Rate (%)\\nPrecision (%)\\nRecall (%)\\nF1-Score (%)\\nVideo 1\\n93.99\\n6.01\\n94.5\\n95.46\\n96.39\\nVideo 2\\n94.67\\n5.33\\n93.6\\n97.73\\n94.38\\nVideo 3\\n92.39\\n7.61\\n96.7\\n96.7\\n98.7\\nVideo 4\\n95.67\\n4.33\\n92.34\\n94.32\\n95.67\\nVideo 5\\n96.78\\n3.22\\n94.35\\n97.8\\n96.78\\nVideo 6\\n90.56\\n9.44\\n95.9\\n96.54\\n97.98\\nVideo 7\\n94.23\\n5.77\\n95.46\\n95.46\\n96.97\\nVideo 8\\n95.49\\n4.51\\n96.4\\n96.4\\n96.48\\nVideo 9\\n95.64\\n4.36\\n94.32\\n97.56\\n95.34\\nVideo 10\\n91.28\\n8.72\\n94.5\\n94.5\\n96.21\\nAverage\\n94.07\\n5.93\\n94.807\\n96.24\\n96.49\\n3.5. Performance Evaluation Using Different Classiﬁers\\nIn this experiment, we compared the performance of the proposed method against different\\nclassiﬁers. For this purpose, we designed different experiments to test the accuracy of shot classiﬁcation\\non standard convolutional neural networks (CNNs), SVM, KNN, Centroid displacement-based\\nK-Nearest Neighbors (CDNN) and ELM classiﬁers. We also employed different feature descriptors\\nand classiﬁers for shot classiﬁcation and compared the classiﬁcation accuracy with our framework.\\nMore speciﬁcally, we used Local Binary Patterns (LBPs) and Local Tetra Patterns (LTRPs) descriptors\\nfor feature extraction and trained them on SVM and Extreme Learning Machine (ELM) classiﬁers.\\nFor SVM classiﬁer we employed different kernel functions like linear, quadratic, multi-layer perception\\n(MLP), and radial basis functions (RBF) to analyze the classiﬁcation performance. The analysis and\\nevaluation of these experiments are presented in detail in this section.\\n3.5.1. Classiﬁcation Using Convolutional Neural Network\\nIn our experiments, we used the CNN architecture having three convolutional layers followed\\nby a batch normalization layer, a RELU layer and one fully connected layer (FC). The input video\\nframes are transformed into grayscale and down-sampled from 640 × 480 to 64 × 64 resolution to\\nreduce the computational cost. Each convolutional layer has a ﬁlter size of 3 × 3, and max-pooling was\\nperformed on every 2 × 2-pixel block. The output is fed to Soft-max layer for classiﬁcation that helps\\nto determine the classiﬁcation probabilities used by the ﬁnal classiﬁcation layer. We used the learning\\nrate of 0.01 in our experiments as we received the best results on this parameter setting. The details of\\neach layer with the feature maps are as follows:\\n•\\n1st Convolutional layer (Conv1) with a kernel size of 3 × 3 and 64 feature maps.\\n•\\n2nd Convolutional layer (Conv2), kernel size of 3 × 3 and 96 feature maps.\\n•\\n3rd Convolutional layer (Conv3), kernel size of 3 × 3 and 128 feature maps.\\n•\\nFully connected layer, hidden units (4000).\\n•\\nFC hidden units equal to several classes i.e., 4.\\n•\\nSoft-max layer provides the classiﬁcation probabilities.\\n•\\nFinal classiﬁcation layer.\\nThe results achieved by the standard CNN architecture for shot classiﬁcation are presented in\\nTable 2 that shows the classiﬁcation accuracy, error rate, precision, recall, and F1-score.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 11}, page_content='Appl. Sci. 2019, 9, 483\\n12 of 21\\nTable 2. Performance evaluation using CNN.\\nVideos\\nAccuracy\\nError Rate\\nPrecision\\nRecall\\nF1-Score\\nVideo 1\\n91.89\\n8.11\\n92.79\\n97.45\\n92.97\\nVideo 2\\n91.27\\n8.73\\n91.78\\n95.67\\n93.34\\nVideo 3\\n91.67\\n8.33\\n90.23\\n96.89\\n91.78\\nVideo 4\\n92.01\\n7.99\\n92.01\\n98.42\\n91.37\\nVideo 5\\n91.02\\n8.98\\n91.02\\n97.1\\n91.26\\nVideo 6\\n92.35\\n7.65\\n92.31\\n94.32\\n90.65\\nVideo 7\\n89.21\\n10.79\\n89.21\\n93.29\\n92.87\\nVideo 8\\n90\\n10\\n90\\n95.79\\n92.8\\nVideo 9\\n89\\n11\\n89\\n92.36\\n92.35\\nVideo 10\\n90.13\\n9.87\\n90.13\\n97.89\\n93.45\\nAverage\\n90.855\\n9.145\\n90.848\\n95.918\\n92.284\\n3.5.2. Classiﬁcation Using Support Vector Machine (SVM)\\nFor classiﬁcation using SVM, we ﬁrst extract features from the dataset and then performed training\\non the extracted features. We used Local Binary Patterns (LBPs) and Local Tetra patterns (LTrPs) for\\nfeature extraction [28], which is discussed in detail in this section.\\nWe computed the LBPs by comparing the grayscale values of the center pixel of the given image\\nwith its neighbor as follows:\\nLBPQ,R =\\nQ\\n∑\\ni=1\\n\\x10\\n2(i−1) × f1(Si −Sc)\\n\\x11\\n(14)\\nf1(x) =\\n(\\n1,\\nx ≥0\\n0,\\nelse\\n(15)\\nwhere LBPQ,R represents the LBP value at the center pixel Sc. Sc and Si represents the grayscale value\\nof the center pixel and the neighboring pixels, respectively. Q is the number of neighbors and R is the\\nradius of the neighborhood.\\nFor LTrPs computation, we calculated the ﬁrst order derivative in the vertical and horizontal\\ndirections and encoded the relationships between the referenced pixel and its neighbors. For image K,\\nthe ﬁrst order derivative along zero and 90 degrees are calculated as:\\nK1\\n0 (Sc) = K\\n\\x00Sg\\n\\x01 −K(SC)\\n(16)\\nK1\\n90 (Sc) = K(Sh) −K(SC)\\n(17)\\nwhere Sc denotes the center pixel in K, Sg and Sh represents the horizontal and vertical neighbors of S.\\nThe direction of center pixel Sc is calculated as::\\nK1\\nDIR(Sc) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n1,\\nK1\\n0(Sc) ≥0 and K1\\n90(Sc) ≥0\\n2,\\nK1\\n0(Sc) < 0 and K1\\n90(Sc) ≥0\\n3,\\nK1\\n0(Sc) < 0 and K1\\n90(Sc) < 0\\n4,\\nK1\\n0(Sc) ≥0 and K1\\n90(Sc) < 0\\n(18)\\nFrom Equation (18), four different values, i.e., 1, 2, 3, and 4 is calculated and these values are\\nnamed as the direction values. The second order derivative is calculated which converts the values into\\nthree binary patterns called local ternary patterns and direction is deﬁned using the Equation (18). Local\\ntetra patterns are generated by calculating Euclidean distance with respect to reference direction pixels.\\nAfter creating the local patterns (LBPs, LTrPs), we represented each image through the histogram as:'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 12}, page_content='Appl. Sci. 2019, 9, 483\\n13 of 21\\nH =\\n1\\nM × N\\nM\\n∑\\nk=1\\nN\\n∑\\nl=1\\nf (LP(k, l), w)\\n(19)\\nf (x, y) =\\n(\\n1,\\ni f x = y\\n0,\\nOtherwise\\n)\\n(20)\\nwhere LP represents the local patterns (LBPs, LTrPs) and M × N is the size of the image.\\nWe applied a multi-class SVM classiﬁer using different kernels for performance evaluation.\\nWe computed the SVM classiﬁer through minimizing the following expression:\\n1\\n2wTw + C\\nN\\n∑\\ni=1\\nεi\\n(21)\\nSubject to the constraints:\\nyi\\n\\x10\\nwT∅(xi) + b\\n\\x11\\n≥1 −εi and εi ≥0, i = 1, . . . , N\\nwhere C is the capacity constant, w is the vector of coefﬁcients, b is a constant, and εi depicts\\nparameters for handling non-separable data. i is the index that labels the N training cases. xi depicts\\nthe independent variable. ∅is the kernel that is used to transform data from input to the feature space.\\nThe hyper-parameter for SVM classiﬁer is the margin (C = 1) between two different classes.\\nThe largest the margin, the better will be the classiﬁer results. Margin is the maximal width of the\\nhyper-plane that has no interior data points. It has been observed that the larger the C, the more the\\nerror is penalized. For SVM classiﬁcation, we obtained the average accuracy of 73.05% using LBP\\ndescriptor. The accuracy against each shot category is provided in Table 3. Similarly, we obtained\\nan average accuracy of 74.46% for LTrP descriptor with SVM classiﬁer. The accuracy for each shot\\ncategory is presented in Table 4.\\nTable 3. Classiﬁcation accuracy using SVM (linear kernel) with LBP features.\\nVideos\\nCrowd Shot\\nLong Shots\\nMedium Shots\\nClose-Up Shots\\nVideo 1\\n77.89\\n72.12\\n70.33\\n72.34\\nVideo 2\\n76.51\\n72.56\\n70.23\\n73.45\\nVideo 3\\n78.34\\n73.57\\n70.75\\n73.41\\nVideo 4\\n76.88\\n72.53\\n70.65\\n72.35\\nVideo 5\\n72.34\\n71.91\\n70.55\\n72.85\\nVideo 6\\n77.82\\n72.85\\n70.5\\n71.23\\nVideo 7\\n76.43\\n73.44\\n70.46\\n72.46\\nVideo 8\\n76.82\\n72.56\\n70.23\\n73.83\\nVideo 9\\n76.23\\n71.23\\n70.43\\n72.76\\nVideo 10\\n77.21\\n70.34\\n70.34\\n73.62\\nAverage\\n76.64%\\n72.31%\\n70.45%\\n72.8%\\nTable 4. Classiﬁcation accuracy using SVM (linear kernel) with LTrP features.\\nVideos\\nCrowd Shot\\nLong Shots\\nMedium Shots\\nClose-Up Shots\\nVideo 1\\n77.49\\n74.32\\n72.32\\n73.21\\nVideo 2\\n78.91\\n73.45\\n73.45\\n74.67\\nVideo 3\\n77.65\\n75.62\\n72.78\\n73.91\\nVideo 4\\n77.89\\n73.32\\n73.32\\n73.32\\nVideo 5\\n76.57\\n75.4\\n72.11\\n72.11\\nVideo 6\\n76.66\\n74.31\\n71.25\\n73.45\\nVideo 7\\n78.64\\n74.81\\n70.19\\n74.56\\nVideo 8\\n79.49\\n75.37\\n72.35\\n73.44\\nVideo 9\\n76.34\\n73.67\\n71.23\\n72.16\\nVideo 10\\n76.34\\n76.34\\n72.31\\n73.71\\nAverage\\n77.59%\\n74.67%\\n72.12%\\n73.45%'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 13}, page_content='Appl. Sci. 2019, 9, 483\\n14 of 21\\nThe experiments reveal that the combination of LTrP with SVM provides better accuracy as\\ncompared to LBP with the SVM. Experiments signify that crowd shots were categorized effectively in\\nthe remaining shots, which is attributed to the fact that the crowd shots contain less dominant grass\\nﬁeld color as compared to other categories.\\nWe also used different kernels of SVM like quadratic, Multi-layer Perception (MLP) and radial\\nbasis function (RBF) during experimentation. MLP is the most popular kernel of SVM, it is the class of\\nfeed forward neural networks and is used when response variable is categorical. RBF kernel is used\\nwhen there is no prior knowledge about the data as it induces Gaussian distributions. Each point in\\nthe RBF kernel becomes a probability density function of normal distribution. It is a sort of rough\\ndistribution of data. Whereas quadratic kernel is used to induce a polynomial combinations of the\\nfeatures, working with bended decision boundaries. Quadratic and RBF kernel are expressed in\\nEquations (23) and (24), respectively.\\nK(p, q) =\\n\\x00ptq + e\\n\\x01d\\n(22)\\nwhere p and q represent input space vectors that are generated from training or validation sets. Note\\nthat e ≥0 is a free parameter that inﬂuences higher order versus lower order terms in the polynomial.\\nr(s) =\\nN\\n∑\\ni=1\\nvi∅(||s −si||)\\n(23)\\nwhere r(s) is the approximating function, which is expressed as a sum of N radial basis functions, sj is\\nthe center value weighted by vj. vj is the estimated weight. The results obtained on LBP features with\\nSVM using different kernel functions (quadratic, radial basis function (RBF) and MLP kernel) for shot\\nclassiﬁcation are presented in Table 5.\\nTable 5. Classiﬁcation accuracy using SVM (Quadratic, MLP, and RBF kernels) with LBP.\\nSVM Kernels\\nCrowd Shots\\nClose-Up Shots\\nLong Shots\\nMedium Shots\\nRBF\\n72.13%\\n68.73%\\n65.34%\\n67.65%\\nQuadratic\\n77.56%\\n76.24%\\n73.21%\\n72.34%\\nMLP\\n58.67%\\n60.98%\\n59.53%\\n61.23%\\nSimilarly, the results of LTrP features with SVM using different kernel functions for shot\\nclassiﬁcation are presented in Table 6.\\nTable 6. Classiﬁcation accuracy using SVM (Quadratic, MLP and RBF kernels) with LTrP.\\nSVM Kernels\\nCrowd Shots\\nClose-Up Shots\\nLong Shots\\nMedium Shots\\nRBF\\n72.27%\\n71.97%\\n70.27%\\n73.37%\\nQuadratic\\n72.34%\\n78.34%\\n73.67%\\n73.45%\\nMLP\\n59.67%\\n59.23%\\n60.78%\\n62.23%\\nIt has been observed from Tables 3–6 that linear SVM provides better performance as compared to\\nquadratic, MLP, and RBF kernels. In addition, there is a slight difference in the performance accuracy\\nbetween RBF and quadratic kernels, however, we received a very low accuracy rate for MLP kernel.\\n3.5.3. Classiﬁcation Using ELM\\nWe also designed an experiment to measure shot classiﬁcation performance using the ELM\\nclassiﬁer. For this purpose, we extracted the LBP and LTrP features in the similar manner as discussed\\nin Section 3.1. For ELM classiﬁcation, we obtained an average accuracy of 73.45%, precision of 72.67%,\\nrecall of 76.39%, and error rate of 26.55% using LBP descriptor. Similarly, an average accuracy of'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 14}, page_content='Appl. Sci. 2019, 9, 483\\n15 of 21\\n75.56%, precision of 76.43%, recall of 77.89%, and error rate of 24.44% and 75.50% using LTrP descriptor\\nwas achieved. The results of LBP and LTrP descriptors with the ELM classiﬁer are provided in Tables 7\\nand 8, respectively.\\nTable 7.\\nPerformance evaluation using extreme learning machine classiﬁer with local binary\\npattern features.\\nVideos\\nAccuracy\\nPrecision\\nRecall\\nError\\nVideo 1\\n73.21\\n73.21\\n76.32\\n26.79\\nVideo 2\\n74.67\\n72.13\\n76.34\\n25.33\\nVideo 3\\n73.91\\n73.91\\n76.58\\n26.09\\nVideo 4\\n73.32\\n73.86\\n75.49\\n26.68\\nVideo 5\\n72.11\\n72.69\\n76.45\\n27.89\\nVideo 6\\n73.45\\n73.45\\n76.77\\n26.55\\nVideo 7\\n74.56\\n71.73\\n77.49\\n25.44\\nVideo 8\\n73.44\\n73.44\\n76.45\\n26.56\\nVideo 9\\n72.16\\n72.16\\n75.86\\n27.84\\nVideo 10\\n73.71\\n70.12\\n76.21\\n26.29\\nAverage\\n73.45%\\n72.67%\\n76.39%\\n26.55%\\nTable 8.\\nPerformance evaluation using extreme learning machine classiﬁer with local tetra\\npattern features.\\nVideos\\nAccuracy\\nPrecision\\nRecall\\nError\\nVideo 1\\n76.54\\n77.43\\n79.45\\n23.46\\nVideo 2\\n77.23\\n77.23\\n77.49\\n22.77\\nVideo 3\\n75.21\\n75.21\\n77.48\\n24.79\\nVideo 4\\n75.95\\n75.95\\n77.48\\n24.05\\nVideo 5\\n74.28\\n77.84\\n78.81\\n25.72\\nVideo 6\\n75.11\\n75.17\\n77.34\\n24.89\\nVideo 7\\n74.56\\n76.39\\n78.39\\n25.44\\nVideo 8\\n76.57\\n76.57\\n78.91\\n23.43\\nVideo 9\\n75.66\\n77.65\\n77.65\\n24.34\\nVideo 10\\n74.52\\n74.85\\n75.97\\n25.48\\nAverage\\n75.56%\\n76.43%\\n77.89%\\n24.44%\\nFrom the results presented in Tables 7 and 8, it can be clearly observed that the combination of\\nLTrP with ELM provides better performance as compared to LBP with the ELM. This illustrates that\\nLTrP descriptors are more effective in comparison of LBP for shot classiﬁcation because it includes\\nmagnitude and direction of the neighboring pixels, whereas in case of LBP only magnitude of the\\nvertical and horizontal neighbor is concerned. It is to be noted that crowd shots classiﬁcation results\\nare far better than the remaining types of shot classiﬁcation. One signiﬁcant reason is the absence of\\nplayﬁeld in crowd shots, whereas playﬁeld exists for all the in-ﬁeld shots.\\n3.5.4. Classiﬁcation Using KNN\\nWe also implemented and tested the shot classiﬁcation performance on our cricket and soccer\\nvideo dataset using K-Nearest Neighbor (K-NN) classiﬁer. In KNN classiﬁcation, an object is classiﬁed\\naccording to majority vote of its neighbors, with the object assigned to the most common class among\\nits k nearest neighbors. We performed this experiment on different values of k and obtained the best\\nresults with k = 5, therefore, the value for K in KNN is set to ﬁve in our experiments. Nearest neighbors\\nare computed by calculating the Euclidean distance formula. The results obtained using the KNN are\\nprovided in Table 9.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 15}, page_content='Appl. Sci. 2019, 9, 483\\n16 of 21\\nTable 9. Performance evaluation using k-nearest neighbor.\\nApproach\\nAccuracy\\nPrecision\\nRecall\\nError Rate\\nVideo 1\\n90.45\\n90\\n92.31\\n9.55\\nVideo 2\\n91.27\\n90.27\\n91.54\\n8.73\\nVideo 3\\n91.67\\n90.12\\n90.12\\n8.33\\nVideo 4\\n92.01\\n91.67\\n91.67\\n7.99\\nVideo 5\\n91.02\\n91.02\\n91.02\\n8.98\\nVideo 6\\n92.35\\n91.48\\n91.48\\n7.65\\nVideo 7\\n89\\n89\\n90.89\\n11\\nVideo 8\\n90.32\\n90\\n92.34\\n9.68\\nVideo 9\\n89.24\\n89.24\\n90\\n10.76\\nVideo 10\\n90.13\\n91\\n91.12\\n9.87\\nAverage\\n90.75%\\n90.37%\\n91.25%\\n9.25%\\n3.5.5. Classiﬁcation Using Centroid Displacement-Based K-Nearest Neighbors (CDNN)\\nWe also implemented and tested the shot classiﬁcation performance on our cricket and soccer\\nvideo dataset using centroid displacement-based K-Nearest Neighbors (CDNN) [29] classiﬁer.\\nIn CDNN classiﬁcation, along with the distance parameter, an integral learning component that learns\\nthe weight of the view is added which helps in classifying new shots in the test dataset. The value of\\nk for CDNN is set to ﬁve for our experiments as we obtained the optimal results on this value after\\nchecking the classiﬁer performance on different values of k. The results obtained using the CDNN\\nare far better than SVM and ELM classiﬁers, but lesser than our proposed method. The results are\\npresented in Table 10.\\nTable 10. Performance evaluation using CDNN.\\nVideos\\nAccuracy\\nPrecision\\nRecall\\nError Rate\\nVideo 1\\n93.45\\n93.23\\n94.67\\n6.55\\nVideo 2\\n91.54\\n92.12\\n93.32\\n8.46\\nVideo 3\\n93.67\\n93.45\\n93.78\\n6.33\\nVideo 4\\n92.31\\n91.29\\n94.56\\n7.69\\nVideo 5\\n91.02\\n92.49\\n92.36\\n8.98\\nVideo 6\\n92.34\\n93.56\\n94.67\\n7.66\\nVideo 7\\n93.47\\n91.23\\n94.78\\n6.53\\nVideo 8\\n92.89\\n92.78\\n93.19\\n7.11\\nVideo 9\\n92.91\\n92.89\\n93.67\\n7.09\\nVideo 10\\n92.13\\n93.14\\n93.81\\n7.87\\nAverage\\n92.5%\\n92.62%\\n93.9%\\n7.5%\\nPerformance comparison of the proposed method with SVM, KNN, CDNN, ELM, and standard\\nCNN classiﬁers are shown in Figure 8. The proposed method achieves an average accuracy of 94.07%\\nin comparison with CDNN, CNN, KNN, ELM and SVM that provides 92.5%, 91.67%, 91.75%, 74.50%\\nand 69.45%, respectively. From the results in Figure 8, it can be clearly observed that the proposed\\nmethod performs far superior, as compared to SVM, ELM, and marginally better than KNN, CDNN,\\nand standard CNN for shot classiﬁcation. Therefore, we can argue that the proposed method is very\\neffective in terms of shot classiﬁcation of sports videos.'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 16}, page_content='Appl. Sci. 2019, 9, 483\\n17 of 21\\nFigure 8. Performance comparison with different classiﬁers.\\n3.6. Performance Comparison with Existing Methods\\nIn the last experiment, we compared the performance of our method against the existing shot\\nclassiﬁcation methods [24–26] and [30–33] for sports videos.\\nTavassolipour et al. [24] proposed a Bayesian network-based model for event detection and video\\nsummarization in soccer videos. Markov model and hue histogram differences were used to detect the\\nshot boundaries. These shots were classiﬁed as long view, medium view, and close-up view depending\\nupon the size of the players and dominant color. Bayesian networks were applied to classify the\\nevents, i.e., goal, foul, etc. Khaligh et al. [25] proposed a shot classiﬁcation method for soccer videos.\\nFirst, the in-ﬁeld video frames were separated from the out-ﬁeld frames. In the next stage, three\\nfeatures (i.e., number of connected components, shirt color in vertical and horizontal strips) were\\nextracted from the in-ﬁeld frames and fed to SVM to classify the long, medium, and close-up shots.\\nKapela et al. [26] used radial basis decomposition function (RBF) and Gabor wavelets to propose a\\nmethod for scene classiﬁcation of sports videos. The input image was transformed into HSV color\\nspace followed by applying the Fourier transform on the image. Gabor ﬁlters were applied and trained\\nthe SVM to classify the scene into different shots. Fani et al. [30] proposed a deep learning fused\\nfeatures-based framework to classify the shot types using the camera zoom and out-ﬁeld information.\\nThe soft-max and fussing Bayesian layers were used to classify the shots into long, medium, close-up,\\nand out-ﬁeld shots. Chun et al. [31] proposed a system for automatic segmentation of basketball\\nvideos based on GOP (group of pictures). Long view, medium view and full court view were classiﬁed\\nusing the dominant color feature and length of the video clips. Kolekar et al. [32] proposed a system\\nthat generated highlights from the soccer videos. Bayesian network was employed to classify the\\nvideo into replay, player, referee, spectator and playing gathering shots based on the audio features.\\nExciting segments from the soccer videos were detected that are assigned semantic concept labels like\\ngoals, save, yellow-cards, and kicks in sequence. Classiﬁcation accuracy for the exciting segments was\\nobserved to be 86%. Raventos et al. [33] proposed a video summarization method for soccer based\\non audio-visual features. Shot segmentation was performed initially to select the frames for video\\nsummarization based on the relevance. Rule-based thresholding was applied on the grass ﬁeld color\\npixels to detect the long shots in soccer videos. The average accuracy of the proposed and existing shot\\nclassiﬁcation approaches is provided in Table 11.\\nFrom the results shown in Table 11, we can clearly observe that the proposed method was\\nevaluated over a diverse dataset of sports videos; and achieved the highest precision and recall values,\\nas compared to the existing state-of-the-art shot classiﬁcation methods. Although the videos used\\nin our method and comparative methods are different but the experimental setup of our method\\nand comparative methods is similar in terms of video source and content selection. We selected\\nthe videos from YouTube as done by the comparative methods and the selected videos represent'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 17}, page_content='Appl. Sci. 2019, 9, 483\\n18 of 21\\ndifferent broadcasters, different genres, and different tournaments. The videos also represent different\\nillumination conditions (i.e., all-day videos, day, and night videos, and night videos recorded in electric\\nlights), and various camera shot types (i.e., long, medium, close-up and crowd/out-ﬁeld shots) as\\nselected by the comparative approaches. By testing our method on a diverse set of sports videos\\ncaptured under the challenging conditions for shot-classiﬁcation we ensured the fair comparison\\nagainst the state-of-the-art methods. Hence, based on our results, we can say that the proposed method\\nis a reliable approach for the shot classiﬁcation of the sports videos.\\nTable 11. Performance comparison with existing shot classiﬁcation methods.\\nShot Classiﬁcation\\nMethods\\nTechnique\\nDataset\\nShots Type\\nRecall\\nPrecision\\nTavassolipour et al. [24]\\nSVM, KNN Classiﬁers\\nSoccer Videos\\nLong, medium, close, out-ﬁeld\\n89.93\\n90.3\\nKhaligh et al. [25]\\nSVM\\nSoccer\\nLong, medium, close, out-ﬁeld\\n93.85\\n91.07\\nKapela et al. [26]\\nSVM Classiﬁer\\nField Sports\\nClose-up, Long\\n84.2\\n82.5\\nFani et al. [30]\\nPFF-Net\\nSoccer Videos\\nClose, medium, long,\\nout-of-the-ﬁeld\\n91.3\\n90.6\\nChun et al. [31]\\nDeep net (CC-Net)\\nBasketball videos\\nClose, medium close, medium,\\nmedium long, long, extreme\\nlong\\nNA\\n90\\nKolekar et al. [32]\\nBayesian Based\\nNetwork (BB-Net)\\nSoccer videos\\nLong, Close-up\\n84\\n86\\nRaventos et al. [33]\\nRule-based\\nThresholding\\nSoccer Videos\\nLong\\n80\\n86\\nProposed System\\nAlexNet CNN\\nCricket & Soccer\\nVideos\\nClose, medium, long,\\ncrowd/out-ﬁeld\\n96.24\\n94.07\\n3.7. Discussion\\nDifferent classiﬁcation frameworks were presented using supervised and un-supervised\\nlearning-based approaches in the past. Experiments prove that convolution neural networks are\\neffective for shot classiﬁcation. We evaluated the performance of different convolution networks, and\\nit has been observed that the proposed AlexNet convolution network performed better in classifying\\ndifferent shots of the sports videos. The use of response normalization rectiﬁed linear unit layer\\nand the drop out layer on the training data makes the training much faster. In fact, once validation\\nloss is observed to be zero, the network stops training and is ready for classiﬁcation. In comparison\\nwith different classiﬁers like KNN++, KNN, SVM, ELM, and standard CNN, we found that the\\nproposed system can train and validate the data by itself. It has also been brought into consideration\\nthat enhanced KNN and KNN classiﬁers perform signiﬁcantly better than SVM and ELM classiﬁer.\\nThe major reason is due to the integral weight factor and the distance parameters of these classiﬁers.\\nMoreover, we also observed during experimentation of the proposed method that the computers\\nembedded with high performance Graphics Processing Unit (GPU) can further increase the speed and\\naccuracy of the proposed framework. For fast training of dataset, AlexNet uses a Graphical Processing\\nUnit (GPU) if its integrated on a system. It requires a parallel computing toolbox with CUDA enabled\\nGPU, otherwise it uses Central Processing Unit (CPU) of a system. Our system is not integrated with\\nGPU, therefore the proposed framework used CPU for training of sports videos.\\nIt has also been observed that the proposed network stops training once it is conﬁrmed that\\nno validation loss is taking place, i.e., the dataset has been trained to a maximum limit. This is the\\nadvantage of our proposed network over standard CNN. Moreover, if Weight Learn Rate Factor\\n(WLRF) and Bias Learn Rate Factor (BLRF) values of fully connected layers are increased, the leaning\\nrate of training rises signiﬁcantly. In addition, we observed during the experimentation that decreases\\nthe size of dataset to 25% increases the learning rate of the data.\\n4. Conclusions and Future Work\\nWe proposed an AlexNet convolutional neural network-based model for shot classiﬁcation of\\nﬁeld sports videos. Our framework is robust to camera variations, scene change, action speeds, and'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 18}, page_content='Appl. Sci. 2019, 9, 483\\n19 of 21\\nillumination conditions (i.e., daylight, artiﬁcial light, shadow). The proposed framework successfully\\nclassiﬁes the input sports video into long, medium, close-up, and crowd/out-ﬁeld shots in the presence\\nof these limitations. Experimental results signify the effectiveness of our framework in terms of shot\\nclassiﬁcation of ﬁeld sports videos. We compared the performance of the proposed method with\\nexisting state-of-the-art methods. In addition, we speciﬁcally designed an experiment to extract LBP\\nand LTrP features and trained them on SVM and ELM separately for shot classiﬁcation. We also\\nevaluated the performance of shot classiﬁcation using the KNN and standard CNN. Afterwards,\\nwe compared the results obtained on SVM, K-NNELM and standard CNN with the proposed method.\\nThe comparison clearly shows that the proposed framework provides better classiﬁcation performance,\\nas compared to SVM ELM, K-NN, and standard CNN classiﬁers. It is to be noted that the use of CUDA\\nGraphics Processing Unit namely GTX 580 can further increase the processing speed of the proposed\\nmethod. Optimization level is achieved by just changing CUDA 480 to CUDA 580 GB graphics card.\\nCurrently we are investigating the performance of the proposed method on a more diverse and\\nlarger dataset. We would preferably be working on small data that is able to deﬁne some training and\\nvalidation percentage of data required to make a trade-off between efﬁcient and effective classiﬁcation\\nphenomena. Moreover, it would be interesting to investigate the performance of various classiﬁers\\nfor the proposed features and the undertaken problem. Particularly, in future work, the performance\\nof the resultant classiﬁers from the combination of weak classiﬁers such as random forest will be\\nanalyzed. In addition, these excellent results of shot classiﬁcation can be further used to increase the\\naccuracy of video summarization systems.\\nAuthor Contributions: Data curation, J.B.J.; Funding acquisition, M.T.M.; Investigation, A.I.; Methodology, A.I.;\\nProject administration, J.B.J.; Resources, M.T.M.; Software, R.A.M. and A.J.; Validation, A.J.; Writing—original\\ndraft, R.A.M.; Writing-review & editing, A.J., A.I., M.T.M. and J.B.J.\\nFunding: This research work was supported by Korea University of Technology and Education (KOREATECH),\\nEducation and Research Promotion Program (2017).\\nConﬂicts of Interest: The authors have no conﬂict of interest.\\nReferences\\n1.\\nChoro´s, K. Automatic playing ﬁeld detection and dominant color extraction in sports video shots of different\\nview types. In Multimedia and Network Information Systems; Springer: Cham, Switzerland, 2017; pp. 39–48.\\n2.\\nPetscharnig, S.; Schöffmann, K. Learning laparoscopic video shot classiﬁcation for gynecological surgery.\\nIn Multimedia Tools and Applications; Springer: New York, NY, USA, 2018; Volume 77, pp. 8061–8079.\\n3.\\nChoro´s, K. Application of the temporal aggregation and pre-categorization of news video shots to reduce\\nthe time of content analysis. J. Intell. Fuzzy Syst. 2017, 32, 1615–1626. [CrossRef]\\n4.\\nWei, W.-L.; Lin, J.-C.; Liu, T.-L.; Yang, Y.-H.; Wang, H.-M.; Tyan, H.-R.; Liao, H.-Y.M. Deep-net fusion to\\nclassify shots in concert videos. In Proceedings of the 2017 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), New Orleans, LA, USA, 5–9 March 2017; pp. 1383–1387.\\n5.\\nHmayda, M.; Ejbali, R.; Zaied, M. Automatic topics segmentation for TV news video. In Proceedings of\\nthe Ninth International Conference on Machine Vision (ICMV 2016), Nice, France, 18–20 November 2016;\\nInternational Society for Optics and Photonics: Bellingham, DC, USA, 2017; Volume 10341, p. 1034114.\\n6.\\nChauhan, D.; Patel, N.M.; Joshi, M. Automatic summarization of basketball sport video. In Proceedings of\\nthe 2016 2nd International Conference on Next Generation Computing Technologies (NGCT), Dehradun,\\nIndia, 14–16 October 2016; pp. 670–673.\\n7.\\nSharma, R.A.; Gandhi, V.; Chari, V.; Jawahar, C.V. Automatic analysis of broadcast football videos using\\ncontextual priors. Signal Image Video Process. 2017, 11, 171–178. [CrossRef]\\n8.\\nChacon-Quesada, R.; Siles-Canales, F. Evaluation of Different Histogram Distances for Temporal\\nSegmentation in Digital Videos of Football Matches from TV Broadcast. In Proceedings of the 2017 International\\nConference and Workshop on Bioinspired Intelligence (IWOBI), Funchal, Portugal, 10–12 July 2017; pp. 1–7.\\n9.\\nChattopadhyay, A.; Chattopadhyay, A.K.; B-Rao, C. Bhattacharyya’s distance measure as a precursor of\\ngenetic distance measures. J. Biosci. 2004, 29, 135. [CrossRef] [PubMed]'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 19}, page_content='Appl. Sci. 2019, 9, 483\\n20 of 21\\n10.\\nEkin, A.; Tekalp, A.M.; Mehrotra, R. Automatic soccer video analysis and summarization. IEEE Trans.\\nImage Process. 2003, 12, 796–807. [CrossRef] [PubMed]\\n11.\\nSharma, R.A.; Sankar, K.P.; Jawahar, C.V. Fine-grain annotation of cricket videos. In Proceedings of the 2015\\n3rd IAPR Asian Conference on Pattern Recognition (ACPR), Kuala Lumpur, Malaysia, 3–6 November 2015;\\npp. 421–425.\\n12.\\nSigari, M.-H.; Soltanian-Zadeh, H.; Kiani, V.; Pourreza, A.-R. Counterattack detection in broadcast soccer\\nvideos using camera motion estimation. In Proceedings of the 2015 International Symposium on Artiﬁcial\\nIntelligence and Signal Processing (AISP), Mashhad, Iran, 3–5 March 2015; pp. 101–106.\\n13.\\nDuan, L.-Y.; Xu, M.; Tian, Q.; Xu, C.-S.; Jin, J.S. A uniﬁed framework for semantic shot classiﬁcation in sports\\nvideo. IEEE Trans. Multimed. 2005, 7, 1066–1083. [CrossRef]\\n14.\\nJoe, Y.-H.N.; Hausknecht, M.; Vijayanarasimhan, S.; Vinyals, O.; Monga, R.; Toderici, G. Beyond short\\nsnippets: Deep networks for video classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, Boston, MA, USA, 7–12 June 2015; pp. 4694–4702.\\n15.\\nKarmaker, D.; Chowdhury, A.Z.M.E.; Miah, M.S.U.; Imran, M.A.; Rahman, M.H. Cricket shot classiﬁcation\\nusing motion vector. In Proceedings of the 2015 Second International Conference on Computing Technology\\nand Information Management (ICCTIM), Johor, Malaysia, 21–23 April 2015; pp. 125–129.\\n16.\\nKapela, R.; ´Swietlicka, A.; Rybarczyk, A.; Kolanowski, K. Real-time event classiﬁcation in ﬁeld sport videos.\\nSignal Process. Image Commun. 2015, 35, 35–45. [CrossRef]\\n17.\\nPapachristou, K.; Tefas, A.; Nikolaidis, N.; Pitas, I. Stereoscopic video shot classiﬁcation based on Weighted\\nLinear Discriminant Analysis. In Proceedings of the 2014 IEEE International Workshop on Machine Learning\\nfor Signal Processing (MLSP), Reims, France, 21–24 September 2014; pp. 1–6.\\n18.\\nBurney, A.; Syed, T.Q. Crowd video classiﬁcation using convolutional neural networks. In Proceedings\\nof the 2016 International Conference on Frontiers of Information Technology (FIT), Islamabad, Pakistan,\\n19–21 December 2016; pp. 247–251.\\n19.\\nAbu-El-Haija, S.; Kothari, N.; Lee, J.; Natsev, P.; Toderici, G.; Varadarajan, B.; Vijayanarasimhan, S.\\nYoutube-8m: A large-scale video classiﬁcation benchmark. arXiv 2016, arXiv:1609.08675.\\n20.\\nLee, J.; Koh, Y.; Yang, J. A deep learning-based video classiﬁcation system using multimodality correlation\\napproach. In Proceedings of the 2017 17th International Conference on Control, Automation and Systems\\n(ICCAS), Jeju, Korea, 18–21 October 2017; pp. 2021–2025.\\n21.\\nWang, H.L.; Cheong, L.-F. Taxonomy of directing semantics for ﬁlm shot classiﬁcation. IEEE Trans. Circuits\\nSyst. Video Technol. 2009, 19, 1529–1542. [CrossRef]\\n22.\\nKumar, A.; Garg, J.; Mukerjee, A. Cricket activity detection. In Proceedings of the 2014 First International\\nImage Processing, Applications and Systems Conference (IPAS), Sfax, Tunisia, 5–7 November 2014; pp. 1–6.\\n23.\\nKrizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classiﬁcation with deep convolutional neural networks.\\nIn Advances in Neural Information Processing Systems; Neural Information Processing System Foundations Inc.:\\nSan Diego, CA, USA, 2012; pp. 1097–1105.\\n24.\\nTavassolipour, M.; Karimian, M.; Kasaei, S. Event detection and summarization in soccer videos using\\nBayesian network and copula. IEEE Trans. Circuits Syst. Video Technol. 2014, 24, 291–304. [CrossRef]\\n25.\\nBagheri-Khaligh, A.; Raziperchikolaei, R.; Moghaddam, M.E. A new method for shot classiﬁcation in soccer\\nsports video based on SVM classiﬁer. In Proceedings of the 2012 IEEE Southwest Symposium on Image\\nAnalysis and Interpretation (SSIAI), Santa Fe, NM, USA, 22–24 April 2012; pp. 109–112.\\n26.\\nKapela, R.; McGuinness, K.; O’Connor, N.E. Real-time ﬁeld sports scene classiﬁcation using colour and\\nfrequency space decompositions. J. Real-Time Image Process. 2017, 13, 725–737. [CrossRef] [PubMed]\\n27.\\nRabia\\nand\\nAli\\nJaved.\\nAvailable\\nonline:\\nhttps://datadryad.org/handle/10255/2/submit/\\n58131f06892432862112314b2c7134460f387284.continue?processonly=true?processonly=true (accessed on\\nJuly 2018).\\n28.\\nMurala, S.; Maheshwari, R.P.; Balasubramanian, R. Local tetra patterns: A new feature descriptor for\\ncontent-based image retrieval. IEEE Trans. Image Process. 2012, 21, 2874–2886. [CrossRef] [PubMed]\\n29.\\nNguyen, B.P.; Tay, W.L.; Chui, C.K. Robust Biometric Recognition from Palm Depth Images for Gloved\\nHands. IEEE Trans. Human Mach. Syst. 2015, 45, 799–804. [CrossRef]\\n30.\\nFani, M.; Yazdi, M.; Clausi, D.A.; Wong, A. Soccer Video Structure Analysis by Parallel Feature Fusion\\nNetwork and Hidden-to-Observable Transferring Markov Model.\\nIEEE Access 2017, 5, 27322–27336.\\n[CrossRef]'),\n",
       " Document(metadata={'producer': 'iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2019-02-01T10:11:51+00:00', 'source': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'file_path': '../data/pdf_files/08.Shot Classification of Field Sports Videos Using AlexNet Convolutional Neural Network.pdf', 'total_pages': 21, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-02-01T10:11:51+00:00', 'trapped': '', 'modDate': 'D:20190201101151Z', 'creationDate': 'D:20190201101151Z', 'page': 20}, page_content='Appl. Sci. 2019, 9, 483\\n21 of 21\\n31.\\nTien, M.-C.; Chen, H.-T.; Chen, Y.-W.; Hsiao, M.-H.; Lee, S.-Y. Shot classiﬁcation of basketball videos and\\nits application in shooting position extraction. In Proceedings of the IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP 2007), Honolulu, HI, USA, 15–20 April 2007; Volume 1,\\npp. 1085–1088.\\n32.\\nKolekar, M.H.; Sengupta, S. Bayesian network-based customized highlight generation for broadcast soccer\\nvideos. IEEE Trans. Broadcast. 2015, 61, 195–209. [CrossRef]\\n33.\\nRaventos, A.; Quijada, R.; Torres, L.; Tarrés, F. Automatic summarization of soccer highlights using\\naudio-visual descriptors. SpringerPlus 2015, 4, 301. [CrossRef] [PubMed]\\n© 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access\\narticle distributed under the terms and conditions of the Creative Commons Attribution\\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-04-14T20:40:22-07:00', 'source': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'file_path': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': 'Large-scale Video Classification with Convolutional Neural Networks', 'author': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', 'subject': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'moddate': '2014-04-14T20:40:22-07:00', 'trapped': '', 'modDate': \"D:20140414204022-07'00'\", 'creationDate': \"D:20140414204022-07'00'\", 'page': 0}, page_content='Large-scale Video Classiﬁcation with Convolutional Neural Networks\\nAndrej Karpathy1,2\\nGeorge Toderici1\\nSanketh Shetty1\\nkarpathy@cs.stanford.edu\\ngtoderici@google.com\\nsanketh@google.com\\nThomas Leung1\\nRahul Sukthankar1\\nLi Fei-Fei2\\nleungt@google.com\\nsukthankar@google.com\\nfeifeili@cs.stanford.edu\\n1Google Research\\n2Computer Science Department, Stanford University\\nhttp://cs.stanford.edu/people/karpathy/deepvideo\\nAbstract\\nConvolutional Neural Networks (CNNs) have been es-\\ntablished as a powerful class of models for image recog-\\nnition problems.\\nEncouraged by these results, we pro-\\nvide an extensive empirical evaluation of CNNs on large-\\nscale video classiﬁcation using a new dataset of 1 million\\nYouTube videos belonging to 487 classes. We study mul-\\ntiple approaches for extending the connectivity of a CNN\\nin time domain to take advantage of local spatio-temporal\\ninformation and suggest a multiresolution, foveated archi-\\ntecture as a promising way of speeding up the training.\\nOur best spatio-temporal networks display signiﬁcant per-\\nformance improvements compared to strong feature-based\\nbaselines (55.3% to 63.9%), but only a surprisingly mod-\\nest improvement compared to single-frame models (59.3%\\nto 60.9%). We further study the generalization performance\\nof our best model by retraining the top layers on the UCF-\\n101 Action Recognition dataset and observe signiﬁcant per-\\nformance improvements compared to the UCF-101 baseline\\nmodel (63.3% up from 43.9%).\\n1. Introduction\\nImages and videos have become ubiquitous on the in-\\nternet, which has encouraged the development of algo-\\nrithms that can analyze their semantic content for vari-\\nous applications, including search and summarization. Re-\\ncently, Convolutional Neural Networks (CNNs) [15] have\\nbeen demonstrated as an effective class of models for un-\\nderstanding image content, giving state-of-the-art results\\non image recognition, segmentation, detection and retrieval\\n[11, 3, 2, 20, 9, 18]. The key enabling factors behind these\\nresults were techniques for scaling up the networks to tens\\nof millions of parameters and massive labeled datasets that\\ncan support the learning process. Under these conditions,\\nCNNs have been shown to learn powerful and interpretable\\nimage features [28]. Encouraged by positive results in do-\\nmain of images, we study the performance of CNNs in\\nlarge-scale video classiﬁcation, where the networks have\\naccess to not only the appearance information present in\\nsingle, static images, but also their complex temporal evolu-\\ntion. There are several challenges to extending and applying\\nCNNs in this setting.\\nFrom a practical standpoint, there are currently no video\\nclassiﬁcation benchmarks that match the scale and variety\\nof existing image datasets because videos are signiﬁcantly\\nmore difﬁcult to collect, annotate and store. To obtain sufﬁ-\\ncient amount of data needed to train our CNN architectures,\\nwe collected a new Sports-1M dataset, which consists of 1\\nmillion YouTube videos belonging to a taxonomy of 487\\nclasses of sports. We make Sports-1M available to the re-\\nsearch community to support future work in this area.\\nFrom a modeling perspective, we are interested in an-\\nswering the following questions: what temporal connectiv-\\nity pattern in a CNN architecture is best at taking advantage\\nof local motion information present in the video? How does\\nthe additional motion information inﬂuence the predictions\\nof a CNN and how much does it improve performance over-\\nall? We examine these questions empirically by evaluating\\nmultiple CNN architectures that each take a different ap-\\nproach to combining information across the time domain.\\nFrom a computational perspective, CNNs require exten-\\nsively long periods of training time to effectively optimize\\nthe millions of parameters that parametrize the model. This\\ndifﬁculty is further compounded when extending the con-\\nnectivity of the architecture in time because the network\\nmust process not just one image but several frames of video\\nat a time. To mitigate this issue, we show that an effec-\\ntive approach to speeding up the runtime performance of\\nCNNs is to modify the architecture to contain two separate\\nstreams of processing: a context stream that learns features\\non low-resolution frames and a high-resolution fovea stream\\nthat only operates on the middle portion of the frame. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-04-14T20:40:22-07:00', 'source': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'file_path': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': 'Large-scale Video Classification with Convolutional Neural Networks', 'author': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', 'subject': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'moddate': '2014-04-14T20:40:22-07:00', 'trapped': '', 'modDate': \"D:20140414204022-07'00'\", 'creationDate': \"D:20140414204022-07'00'\", 'page': 1}, page_content='observe a 2-4x increase in runtime performance of the net-\\nwork due to the reduced dimensionality of the input, while\\nretaining the classiﬁcation accuracy.\\nFinally, a natural question that arises is whether features\\nlearned on the Sports-1M dataset are generic enough to\\ngeneralize to a different, smaller dataset. We investigate\\nthe transfer learning problem empirically, achieving sig-\\nniﬁcantly better performance (65.4%, up from 41.3%) on\\nUCF-101 by re-purposing low-level features learned on the\\nSports-1M dataset than by training the entire network on\\nUCF-101 alone. Furthermore, since only some classes in\\nUCF-101 are related to sports, we can quantify the relative\\nimprovements of the transfer learning in both settings.\\nOur contributions can be summarized as follows:\\n• We provide extensive experimental evaluation of mul-\\ntiple approaches for extending CNNs into video clas-\\nsiﬁcation on a large-scale dataset of 1 million videos\\nwith 487 categories (which we release as Sports-1M\\ndataset) and report signiﬁcant gains in performance\\nover strong feature-based baselines.\\n• We highlight an architecture that processes input at two\\nspatial resolutions - a low-resolution context stream\\nand a high-resolution fovea stream - as a promising\\nway of improving the runtime performance of CNNs\\nat no cost in accuracy.\\n• We apply our networks to the UCF-101 dataset and re-\\nport signiﬁcant improvement over feature-based state-\\nof-the-art results and baselines established by training\\nnetworks on UCF-101 alone.\\n2. Related Work\\nThe standard approach to video classiﬁcation [26, 16,\\n21, 17] involves three major stages: First, local visual fea-\\ntures that describe a region of the video are extracted ei-\\nther densely [25] or at a sparse set of interest points [12, 8].\\nNext, the features get combined into a ﬁxed-sized video-\\nlevel description. One popular approach is to quantize all\\nfeatures using a learned k-means dictionary and accumulate\\nthe visual words over the duration of the video into his-\\ntograms of varying spatio-temporal positions and extents\\n[13]. Lastly, a classiﬁer (such as an SVM) is trained on\\nthe resulting ”bag of words” representation to distinguish\\namong the visual classes of interest.\\nConvolutional Neural Networks [15] are a biologically-\\ninspired class of deep learning models that replace all three\\nstages with a single neural network that is trained end to\\nend from raw pixel values to classiﬁer outputs. The spa-\\ntial structure of images is explicitly taken advantage of for\\nregularization through restricted connectivity between lay-\\ners (local ﬁlters), parameter sharing (convolutions) and spe-\\ncial local invariance-building neurons (max pooling). Thus,\\nthese architectures effectively shift the required engineer-\\ning from feature design and accumulation strategies to de-\\nsign of the network connectivity structure and hyperparam-\\neter choices. Due to computational constraints, CNNs have\\nuntil recently been applied to relatively small scale image\\nrecognition problems (on datasets such as MNIST, CIFAR-\\n10/100, NORB, and Caltech-101/256), but improvements\\non GPU hardware have enabled CNNs to scale to networks\\nof millions of parameters, which has in turn led to signif-\\nicant improvements in image classiﬁcation[11], object de-\\ntection [20, 9], scene labeling [3], indoor segmentation [4]\\nand house number digit classiﬁcation [19]. Additionally,\\nfeatures learned by large networks trained on ImageNet\\n[7] have been shown to yield state-of-the-art performance\\nacross many standard image recognition datasets when clas-\\nsiﬁed with an SVM, even with no ﬁne-tuning [18].\\nCompared to image data domains, there is relatively lit-\\ntle work on applying CNNs to video classiﬁcation. Since\\nall successful applications of CNNs in image domains share\\nthe availability of a large training set, we speculate that this\\nis partly attributable to lack of large-scale video classiﬁca-\\ntion benchmarks.\\nIn particular, commonly used datasets\\n(KTH, Weizmann, UCF Sports, IXMAS, Hollywood 2,\\nUCF-50) only contain up to few thousand clips and up to\\nfew dozen classes. Even the largest available datasets such\\nas CCV (9,317 videos and 20 classes) and the recently in-\\ntroduced UCF-101[22] (13,320 videos and 101 classes) are\\nstill dwarfed by available image datasets in the number of\\ninstances and their variety [7]. Despite these limitations,\\nsome extensions of CNNs into the video domain have been\\nexplored.\\n[1] and [10] extend an image CNN to video\\ndomains by treating space and time as equivalent dimen-\\nsions of the input and perform convolutions in both time\\nand space. We consider these extensions as only one of the\\npossible generalizations in this work. Unsupervised learn-\\ning schemes for training spatio-temporal features have also\\nbeen developed, based on Convolutional Gated Restricted\\nBoltzmann Machines [23] and Independent Subspace Anal-\\nysis [14]. In contrast, our models are trained end to end\\nfully supervised.\\n3. Models\\nUnlike images which can be cropped and rescaled to a\\nﬁxed size, videos vary widely in temporal extent and can-\\nnot be easily processed with a ﬁxed-sized architecture. In\\nthis work we treat every video as a bag of short, ﬁxed-sized\\nclips. Since each clip contains several contiguous frames\\nin time, we can extend the connectivity of the network in\\ntime dimension to learn spatio-temporal features. There are\\nmultiple options for the precise details of the extended con-\\nnectivity and we describe three broad connectivity pattern\\ncategories (Early Fusion, Late Fusion and Slow Fusion) be-\\nlow. Afterwards, we describe a multiresolution architecture\\nfor addressing the computational efﬁciency.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-04-14T20:40:22-07:00', 'source': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'file_path': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': 'Large-scale Video Classification with Convolutional Neural Networks', 'author': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', 'subject': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'moddate': '2014-04-14T20:40:22-07:00', 'trapped': '', 'modDate': \"D:20140414204022-07'00'\", 'creationDate': \"D:20140414204022-07'00'\", 'page': 2}, page_content='Figure 1: Explored approaches for fusing information over\\ntemporal dimension through the network. Red, green and\\nblue boxes indicate convolutional, normalization and pool-\\ning layers respectively. In the Slow Fusion model, the de-\\npicted columns share parameters.\\n3.1. Time Information Fusion in CNNs\\nWe investigate several approaches to fusing information\\nacross temporal domain (Figure 1): the fusion can be done\\nearly in the network by modifying the ﬁrst layer convolu-\\ntional ﬁlters to extend in time, or it can be done late by\\nplacing two separate single-frame networks some distance\\nin time apart and fusing their outputs later in the process-\\ning. We ﬁrst describe a baseline single-frame CNN and then\\ndiscuss its extensions in time according to different types of\\nfusion.\\nSingle-frame. We use a single-frame baseline architec-\\nture to understand the contribution of static appearance to\\nthe classiﬁcation accuracy. This network is similar to the\\nImageNet challenge winning model [11], but accepts in-\\nputs of size 170 × 170 × 3 pixels instead of the original\\n224 × 224 × 3. Using shorthand notation, the full architec-\\nture is C(96, 11, 3)-N-P-C(256, 5, 1)-N-P-C(384, 3, 1)-\\nC(384, 3, 1)-C(256, 3, 1)-P-FC(4096)-FC(4096), where\\nC(d, f, s) indicates a convolutional layer with d ﬁlters of\\nspatial size f ×f, applied to the input with stride s. FC(n)\\nis a fully connected layer with n nodes. All pooling layers P\\npool spatially in non-overlapping 2 × 2 regions and all nor-\\nmalization layers N are deﬁned as described in Krizhevsky\\net al. [11] and use the same parameters: k = 2, n = 5, α =\\n10−4, β = 0.5. The ﬁnal layer is connected to a softmax\\nclassiﬁer with dense connections.\\nEarly Fusion. The Early Fusion extension combines in-\\nformation across an entire time window immediately on the\\npixel level. This is implemented by modifying the ﬁlters on\\nthe ﬁrst convolutional layer in the single-frame model by\\nextending them to be of size 11 × 11 × 3 × T pixels, where\\nT is some temporal extent (we use T = 10, or approxi-\\nmately a third of a second). The early and direct connectiv-\\nity to pixel data allows the network to precisely detect local\\nmotion direction and speed.\\nLate Fusion. The Late Fusion model places two sepa-\\nrate single-frame networks (as described above, up to last\\nconvolutional layer C(256, 3, 1) with shared parameters a\\ndistance of 15 frames apart and then merges the two streams\\nin the ﬁrst fully connected layer. Therefore, neither single-\\nframe tower alone can detect any motion, but the ﬁrst fully\\nconnected layer can compute global motion characteristics\\nby comparing outputs of both towers.\\nSlow Fusion.\\nThe Slow Fusion model is a balanced\\nmix between the two approaches that slowly fuses temporal\\ninformation throughout the network such that higher lay-\\ners get access to progressively more global information in\\nboth spatial and temporal dimensions. This is implemented\\nby extending the connectivity of all convolutional layers\\nin time and carrying out temporal convolutions in addition\\nto spatial convolutions to compute activations, as seen in\\n[1, 10]. In the model we use, the ﬁrst convolutional layer is\\nextended to apply every ﬁlter of temporal extent T = 4 on\\nan input clip of 10 frames through valid convolution with\\nstride 2 and produces 4 responses in time. The second and\\nthird layers above iterate this process with ﬁlters of tempo-\\nral extent T = 2 and stride 2. Thus, the third convolutional\\nlayer has access to information across all 10 input frames.\\n3.2. Multiresolution CNNs\\nSince CNNs normally take on orders of weeks to train on\\nlarge-scale datasets even on the fastest available GPUs, the\\nruntime performance is a critical component to our ability\\nto experiment with different architecture and hyperparame-\\nter settings. This motivates approaches for speeding up the\\nmodels while still retaining their performance. There are\\nmultiple fronts to these endeavors, including improvements\\nin hardware, weight quantization schemes, better optimiza-\\ntion algorithms and initialization strategies, but in this work\\nwe focus on changes in the architecture that enable faster\\nrunning times without sacriﬁcing performance.\\nOne approach to speeding up the networks is to reduce\\nthe number of layers and neurons in each layer, but simi-\\nlar to [28] we found that this consistently lowers the per-\\nformance. Instead of reducing the size of the network, we\\nconducted further experiments on training with images of\\nlower resolution. However, while this improved the run-\\nning time of the network, the high-frequency detail in the\\nimages proved critical to achieving good accuracy.\\nFovea and context streams. The proposed multiresolu-\\ntion architecture aims to strike a compromise by having two\\nseparate streams of processing over two spatial resolutions\\n(Figure 2). A 178 × 178 frame video clip forms an input\\nto the network. The context stream receives the downsam-\\npled frames at half the original spatial resolution (89 × 89\\npixels), while the fovea stream receives the center 89 × 89\\nregion at the original resolution. In this way, the the total\\ninput dimensionality is halved. Notably, this design takes\\nadvantage of the camera bias present in many online videos,\\nsince the object of interest often occupies the center region.\\nArchitecture changes. Both streams are processed by\\nidentical network as the full frame models, but starting at'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-04-14T20:40:22-07:00', 'source': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'file_path': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': 'Large-scale Video Classification with Convolutional Neural Networks', 'author': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', 'subject': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'moddate': '2014-04-14T20:40:22-07:00', 'trapped': '', 'modDate': \"D:20140414204022-07'00'\", 'creationDate': \"D:20140414204022-07'00'\", 'page': 3}, page_content='Figure 2: Multiresolution CNN architecture. Input frames\\nare fed into two separate streams of processing: a con-\\ntext stream that models low-resolution image and a fovea\\nstream that processes high-resolution center crop.\\nBoth\\nstreams consist of alternating convolution (red), normaliza-\\ntion (green) and pooling (blue) layers. Both streams con-\\nverge to two fully connected layers (yellow).\\n89 × 89 clips of video. Since the input is only of half the\\nspatial size as the full-frame models, we take out the last\\npooling layer to ensure that both streams still terminate in a\\nlayer of size 7×7×256. The activations from both streams\\nare concatenated and fed into the ﬁrst fully connected layer\\nwith dense connections.\\n3.3. Learning\\nOptimization. We use Downpour Stochastic Gradient\\nDescent [6] to optimize our models across a computing\\ncluster. The number of replicas for each model varies be-\\ntween 10 and 50 and every model is further split across 4\\nto 32 partitions. We use mini-batches of 32 examples, mo-\\nmentum of 0.9 and weight decay of 0.0005. All models are\\ninitialized with learning rates of 1e−3 and this value is fur-\\nther reduced by hand whenever the validation error stops\\nimproving.\\nData augmentation and preprocessing.\\nFollowing\\n[11], we take advantage of data augmentation to reduce the\\neffects of overﬁtting. Before presenting an example to a net-\\nwork, we preprocess all images by ﬁrst cropping to center\\nregion, resizing them to 200 × 200 pixels, randomly sam-\\npling a 170 × 170 region, and ﬁnally randomly ﬂipping the\\nimages horizontally with 50% probability. These prepro-\\ncessing steps are applied consistently to all frames that are\\npart of the same clip. As a last step of preprocessing we sub-\\ntract a constant value of 117 from raw pixel values, which\\nis the approximate value of the mean of all pixels in our\\nimages.\\n4. Results\\nWe ﬁrst present results on our Sports-1M dataset and\\nqualitatively analyze the learned features and network pre-\\ndictions.\\nWe then describe our transfer learning experi-\\nments on UCF-101.\\n4.1. Experiments on Sports-1M\\nDataset. The Sports-1M dataset consists of 1 million\\nYouTube videos annotated with 487 classes. The classes\\nare arranged in a manually-curated taxonomy that contains\\ninternal nodes such as Aquatic Sports, Team Sports, Winter\\nSports, Ball Sports, Combat Sports, Sports with Animals,\\nand generally becomes ﬁne-grained by the leaf level. For\\nexample, our dataset contains 6 different types of bowling,\\n7 different types of American football and 23 types of bil-\\nliards.\\nThere are 1000-3000 videos per class and approximately\\n5% of the videos are annotated with more than one class.\\nThe annotations are produced automatically by analyzing\\nthe text metadata surrounding the videos. Thus, our data is\\nweakly annotated on two levels: ﬁrst, the label of a video\\nmay be wrong if the tag prediction algorithm fails or if the\\nprovided description does not match the video content, and\\nsecond, even when a video is correctly annotated it may still\\nexhibit signiﬁcant variation on the frame level. For exam-\\nple, a video tagged as soccer may contain several shots of\\nthe scoreboard, interviews, news anchors, the crowd, etc.\\nWe split the dataset by assigning 70% of the videos to\\nthe training set, 10% to a validation set and 20% to a test\\nset. As YouTube may contain duplicate videos, it is pos-\\nsible that the same video could appear in both the training\\nand test set. To get an idea about the extent of this prob-\\nlem we processed all videos with a near-duplicate ﬁnding\\nalgorithm on the frame level and determined that only 1755\\nvideos (out of 1 million) contain a signiﬁcant fraction of\\nnear-duplicate frames. Furthermore, since we only use a\\nrandom collection of up to 100 half-second clips from ev-\\nery video and our videos are 5 minutes and 36 seconds in\\nlength on average, it is unlikely that the same frames occur\\nacross data splits.\\nTraining. We trained our models over a period of one\\nmonth, with models processing approximately 5 clips per\\nsecond for full-frame networks and up to 20 clips per sec-\\nond for multiresolution networks on a single model replica.\\nThe rate of 5 clips per second is roughly 20 times slower\\nthan what one could expect from a high-end GPU, but we\\nexpect to reach comparable speeds overall given that we use\\n10-50 model replicas. We further estimate the size of our\\ndataset of sampled frames to be on the order of 50 million\\nexamples and that our networks have each seen approxi-\\nmately 500 million examples throughout the training period\\nin total.\\nVideo-level predictions. To produce predictions for an\\nentire video we randomly sample 20 clips and present each\\nclip individually to the network. Every clip is propagated\\nthrough the network 4 times (with different crops and ﬂips)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-04-14T20:40:22-07:00', 'source': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'file_path': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': 'Large-scale Video Classification with Convolutional Neural Networks', 'author': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', 'subject': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'moddate': '2014-04-14T20:40:22-07:00', 'trapped': '', 'modDate': \"D:20140414204022-07'00'\", 'creationDate': \"D:20140414204022-07'00'\", 'page': 4}, page_content='Figure 4: Predictions on Sports-1M test data. Blue (ﬁrst row) indicates ground truth label and the bars below show model\\npredictions sorted in decreasing conﬁdence. Green and red distinguish correct and incorrect predictions, respectively.\\nModel\\nClip Hit@1\\nVideo Hit@1\\nVideo Hit@5\\nFeature Histograms + Neural Net\\n-\\n55.3\\n-\\nSingle-Frame\\n41.1\\n59.3\\n77.7\\nSingle-Frame + Multires\\n42.4\\n60.0\\n78.5\\nSingle-Frame Fovea Only\\n30.0\\n49.9\\n72.8\\nSingle-Frame Context Only\\n38.1\\n56.0\\n77.2\\nEarly Fusion\\n38.9\\n57.7\\n76.8\\nLate Fusion\\n40.7\\n59.3\\n78.7\\nSlow Fusion\\n41.9\\n60.9\\n80.2\\nCNN Average (Single+Early+Late+Slow)\\n41.4\\n63.9\\n82.4\\nTable 1: Results on the 200,000 videos of the Sports-1M test set. Hit@k values indicate the fraction of test samples that\\ncontained at least one of the ground truth labels in the top k predictions.\\nand the network class predictions are averaged to produce a\\nmore robust estimate of the class probabilities. To produce\\nvideo-level predictions we opted for the simplest approach\\nof averaging individual clip predictions over the durations\\nof each video. We expect more elaborate techniques to fur-\\nther improve performance but consider these to be outside\\nof the scope of the paper.\\nFeature histogram baselines. In addition to comparing\\nCNN architectures among each other, we also report the ac-\\ncuracy of a feature-based approach. Following a standard\\nbag-of-words pipeline we extract several types of features\\nat all frames of our videos, discretize them using k-means\\nvector quantization and accumulate words into histograms\\nwith spatial pyramid encoding and soft quantization. Ev-\\nery histogram is normalized to sum to 1 and all histograms\\nare concatenated into a 25,000 dimensional video-level fea-\\nture vector. Our features are similar to Yang & Toderici\\n[27] and consist of local features (HOG [5], Texton [24],\\nCuboids [8], etc.)\\nextracted both densely and at sparse\\ninterest points, as well as global features (such as Hue-\\nSaturation, Color moments, number of faces detected). As\\na classiﬁer we use a multilayer neural network with Rec-\\ntiﬁed Linear Units followed by a Softmax classiﬁer. We\\nfound that a multilayer network performs consistently and\\nsigniﬁcantly better than linear models on separate validation\\nexperiments. Furthermore, we performed extensive cross-\\nvalidations across many of the network’s hyperparameters\\nby training multiple models and choosing the one with best\\nperformance on a validation set. The tuned hyper parame-\\nters include the learning rate, weight decay, the number of\\nhidden layers (between 1-2), dropout probabilities and the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-04-14T20:40:22-07:00', 'source': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'file_path': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': 'Large-scale Video Classification with Convolutional Neural Networks', 'author': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', 'subject': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'moddate': '2014-04-14T20:40:22-07:00', 'trapped': '', 'modDate': \"D:20140414204022-07'00'\", 'creationDate': \"D:20140414204022-07'00'\", 'page': 5}, page_content='Figure 5: Examples that illustrate qualitative differences between single-frame network and Slow Fusion (motion-aware)\\nnetwork in the same color scheme as Figure 4. A few classes are easier to disambiguate with motion information (left three).\\nFigure 3: Filters learned on ﬁrst layer of a multiresolution\\nnetwork. Left: context stream, Right: fovea stream. No-\\ntably, the fovea stream learns grayscale, high-frequency fea-\\ntures while the context stream models lower frequencies and\\ncolors. GIFs of moving video features can be found on our\\nwebsite (linked on ﬁrst page).\\nnumber of nodes in all layers.\\nQuantitative results.\\nThe results for the Sports-1M\\ndataset test set, which consists of 200,000 videos and\\n4,000,000 clips, are summarized in Table 1.\\nAs can be\\nseen from the table, our networks consistently and signif-\\nicantly outperform the feature-based baseline. We empha-\\nsize that the feature-based approach computes visual words\\ndensely over the duration of the video and produces predic-\\ntions based on the entire video-level feature vector, while\\nour networks only see 20 randomly sampled clips individ-\\nually. Moreover, our networks seem to learn well despite\\nsigniﬁcant label noise: the training videos are subject to\\nincorrect annotations and even the correctly-labeled videos\\noften contain a large amount of artifacts such as text, ef-\\nfects, cuts, and logos, none of which we attempted to ﬁlter\\nout explicitly.\\nCompared to the wide gap relative to the feature-based\\nbaseline, the variation among different CNN architectures\\nturns out to be surprisingly insigniﬁcant.\\nNotably, the\\nsingle-frame model already displays strong performance.\\nFurthermore, we observe that the foveated architectures are\\nbetween 2-4× faster in practice due to reduced input dimen-\\nsionality. The precise speedups are in part a function of the\\ndetails of model partitioning and our implementation, but in\\nour experiments we observe a speedup during training of 6\\nto 21 clips per second (3.5x) for the single-frame model and\\n5 to 10 clips per second (2x) for the Slow Fusion model.\\nContributions of motion. We conduct further exper-\\nSports class\\n∆AP\\n∆AP\\nSports class\\nJuggling Club\\n0.12\\n-0.07\\nShort Track Motor Racing\\nPole Climbing\\n0.10\\n-0.07\\nRoad Racing\\nMountain Unicycling\\n0.08\\n-0.07\\nJeet Kune Do\\nTricking\\n0.07\\n-0.06\\nPaintball\\nFootbag\\n0.07\\n-0.06\\nFreeride\\nSkipping Rope\\n0.06\\n-0.06\\nCricket\\nRope Climbing\\n0.06\\n-0.06\\nWrestling\\nSlacklining\\n0.05\\n-0.06\\nModern Pentathlon\\nTee Ball\\n0.05\\n-0.06\\nKrav Maga\\nSheepdog Trial\\n0.05\\n-0.05\\nRally Cross\\nTable 2: Classes for which a (motion-aware) Slow Fusion\\nCNN performs better than the single-frame CNN (left) and\\nvice versa (right), as measured by difference in per-class\\naverage precision.\\niments to understand the differences between the single-\\nframe network and networks that have access to motion in-\\nformation. We choose the Slow Fusion network as a rep-\\nresentative motion-aware network because it performs best.\\nWe compute and compare the per-class average precision\\nfor all Sports classes and highlight the ones that exhibit\\nlargest differences (Table 2). Manually inspecting some of\\nthe associated clips (Figure 5), we qualitatively observe that\\nthe motion-aware network clearly beneﬁts from motion in-\\nformation in some cases, but these seem to be relatively un-\\ncommon. On the other hand, balancing the improvements\\nfrom access to motion information, we observe that motion-\\naware networks are more likely to underperform when there\\nis camera motion present. We hypothesize that the CNNs\\nstruggle to learn complete invariance across all possible an-\\ngles and speeds of camera translation and zoom.\\nQualitative analysis. Our learned features for the ﬁrst\\nconvolutional layer can be inspected on Figure 3.\\nIn-\\nterestingly, the context stream learns more color features\\nwhile the high-resolution fovea stream learns high fre-\\nquency grayscale ﬁlters.\\nAs can be seen on Figure 4, our networks produce in-\\nterpretable predictions and generally make reasonable mis-\\ntakes. Further analysis of the confusion matrix (attached\\nin the supplementary material) reveals that most errors are\\namong the ﬁne-grained classes of our dataset. For exam-\\nple, the top 5 most commonly confused pairs of classes are\\ndeer hunting vs. hunting, hiking vs. backpacking, powered\\nparagliding vs. paragliding, sledding vs. toboggan, and bu-\\njinkan vs. ninjutsu.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-04-14T20:40:22-07:00', 'source': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'file_path': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': 'Large-scale Video Classification with Convolutional Neural Networks', 'author': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', 'subject': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'moddate': '2014-04-14T20:40:22-07:00', 'trapped': '', 'modDate': \"D:20140414204022-07'00'\", 'creationDate': \"D:20140414204022-07'00'\", 'page': 6}, page_content='Model\\n3-fold Accuracy\\nSoomro et al [22]\\n43.9%\\nFeature Histograms + Neural Net\\n59.0%\\nTrain from scratch\\n41.3%\\nFine-tune top layer\\n64.1%\\nFine-tune top 3 layers\\n65.4%\\nFine-tune all layers\\n62.2%\\nTable 3: Results on UCF-101 for various Transfer Learning\\napproaches using the Slow Fusion network.\\n4.2. Transfer Learning Experiments on UCF-101\\nThe results of our analysis on the Sports-1M dataset in-\\ndicate that the networks learn powerful motion features. A\\nnatural question that arises is whether these features also\\ngeneralize to other datasets and class categories. We ex-\\namine this question in detail by performing transfer learn-\\ning experiments on the UCF-101 [22] Activity Recognition\\ndataset. The dataset consists of 13,320 videos belonging\\nto 101 categories that are separated into 5 broad groups:\\nHuman-Object interaction (Applying eye makeup, brush-\\ning teeth, hammering, etc.), Body-Motion (Baby crawling,\\npush ups, blowing candles, etc.), Human-Human interac-\\ntion (Head massage, salsa spin, haircut, etc.), Playing In-\\nstruments (ﬂute, guitar, piano, etc.) and Sports. This group-\\ning allows us to separately study the performance improve-\\nments on Sports classes relative to classes from unrelated\\nvideos that are less numerous in our training data.\\nTransfer learning. Since we expect that CNNs learn\\nmore generic features on the bottom of the network (such\\nas edges, local shapes) and more intricate, dataset-speciﬁc\\nfeatures near the top of the network, we consider the fol-\\nlowing scenarios for our transfer learning experiments:\\nFine-tune top layer. We treat the CNN as a ﬁxed feature\\nextractor and train a classiﬁer on the last 4096-dimensional\\nlayer, with dropout regularization. We found that as little as\\n10% chance of keeping each unit active to be effective.\\nFine-tune top 3 layers. Instead of only retraining the ﬁ-\\nnal classiﬁer layer, we consider also retraining both fully\\nconnected layers. We initialize with a fully trained Sports\\nCNN and then begin training the top 3 layers. We intro-\\nduce dropout before all trained layers, with as little as 10%\\nchance of keeping units active.\\nFine-tune all layers. In this scenario we retrain all net-\\nwork parameters, including all convolutional layers on the\\nbottom of the network.\\nTrain from scratch. As a baseline we train the full net-\\nwork from scratch on UCF-101 alone.\\nResults. To prepare UCF-101 data for classiﬁcation we\\nsampled 50 clips from every video and followed the same\\nevaluation protocol as for Sports across the 3 suggested\\nfolds. We reached out to the authors of [22] to obtain the\\nYouTube video IDs of UCF-101 videos, but unfortunately\\nGroup\\nmAP\\nfrom\\nscratch\\nmAP\\nﬁne-tune\\ntop 3\\nmAP\\nﬁne-tune\\ntop\\nHuman-Object Interaction\\n0.26\\n0.55\\n0.52\\nBody-Motion Only\\n0.32\\n0.57\\n0.52\\nHuman-Human Interaction\\n0.40\\n0.68\\n0.65\\nPlaying Musical Instruments\\n0.42\\n0.65\\n0.46\\nSports\\n0.57\\n0.79\\n0.80\\nAll groups\\n0.44\\n0.68\\n0.66\\nTable 4: Mean Average Precision of the Slow Fusion net-\\nwork on UCF-101 classes broken down by category groups.\\nthese were not available and hence we cannot guarantee that\\nthe Sports-1M dataset has no overlap with UCF-101. How-\\never, these concerns are somewhat mitigated as we only use\\na few sampled clips from every video.\\nWe use the Slow Fusion network in our UCF-101 exper-\\niments as it provides the best performance on Sports-1M.\\nThe results of the experiments can be seen on Table 3. In-\\nterestingly, retraining the softmax layer alone does not per-\\nform best (possibly because the high-level features are too\\nspeciﬁc to sports) and the other extreme of ﬁne-tuning all\\nlayers is also not adequate (likely due to overﬁtting). In-\\nstead, the best performance is obtained by taking a balanced\\napproach and retraining the top few layers of the network.\\nLastly, training the entire network from scratch consistently\\nleads to massive overﬁtting and dismal performance.\\nPerformance by group. We further break down our per-\\nformance by 5 broad groups of classes present in the UCF-\\n101 dataset. We compute the average precision of every\\nclass and then compute the mean average precision over\\nclasses in each group. As can be seen from Table 4, large\\nfractions of our performance can be attributed to the Sports\\ncategories in UCF-101, but the other groups still display im-\\npressive performance considering that the only way to ob-\\nserve these types of frames in the training data is due to label\\nnoise. Moreover, the gain in performance when retraining\\nonly the top to retraining the top 3 layers is almost entirely\\ndue to improvements on non-Sports categories: Sports per-\\nformance only decreases from 0.80 to 0.79, while mAP im-\\nproves on all other categories.\\n5. Conclusions\\nWe studied the performance of convolutional neural net-\\nworks in large-scale video classiﬁcation.\\nWe found that\\nCNN architectures are capable of learning powerful fea-\\ntures from weakly-labeled data that far surpass feature-\\nbased methods in performance and that these beneﬁts are\\nsurprisingly robust to details of the connectivity of the ar-\\nchitectures in time. Qualitative examination of network out-\\nputs and confusion matrices reveals interpretable errors.\\nOur results indicate that while the performance is not\\nparticularly sensitive to the architectural details of the con-\\nnectivity in time, a Slow Fusion model consistently per-\\nforms better than the early and late fusion alternatives. Sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.10', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-04-14T20:40:22-07:00', 'source': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'file_path': '../data/pdf_files/06.Large-scale Video Classification with Convolutional Neural Networks.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': 'Large-scale Video Classification with Convolutional Neural Networks', 'author': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', 'subject': '2013 IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'moddate': '2014-04-14T20:40:22-07:00', 'trapped': '', 'modDate': \"D:20140414204022-07'00'\", 'creationDate': \"D:20140414204022-07'00'\", 'page': 7}, page_content='prisingly, we ﬁnd that a single-frame model already dis-\\nplays very strong performance, suggesting that local motion\\ncues may not be critically important, even for a dynamic\\ndataset such as Sports. An alternative theory is that more\\ncareful treatment of camera motion may be necessary (for\\nexample by extracting features in the local coordinate sys-\\ntem of a tracked point, as seen in [25]), but this requires\\nsigniﬁcant changes to a CNN architecture that we leave for\\nfuture work. We also identiﬁed mixed-resolution architec-\\ntures that consist of a low-resolution context and a high-\\nresolution fovea stream as an effective way of speeding up\\nCNNs without sacriﬁcing accuracy.\\nOur transfer learning experiments on UCF-101 suggest\\nthat the learned features are generic and generalize other\\nvideo classiﬁcation tasks. In particular, we achieved the\\nhighest transfer learning performance by retraining the top\\n3 layers of the network.\\nIn future work we hope to incorporate broader categories\\nin the dataset to obtain more powerful and generic fea-\\ntures, investigate approaches that explicitly reason about\\ncamera motion, and explore recurrent neural networks as\\na more powerful technique for combining clip-level predic-\\ntions into global video-level predictions.\\nAcknowledgments: We thank Saurabh Singh, Abhinav\\nShrivastava, Jay Yagnik, Alex Krizhevsky, Quoc Le, Jeff\\nDean and Rajat Monga for helpful discussions.\\nReferences\\n[1] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and\\nA. Baskurt.\\nSequential deep learning for human action\\nrecognition. In Human Behavior Understanding, pages 29–\\n39. Springer, 2011. 2, 3\\n[2] D. Ciresan, A. Giusti, J. Schmidhuber, et al. Deep neural net-\\nworks segment neuronal membranes in electron microscopy\\nimages. In NIPS, 2012. 1\\n[3] L. N. Clement Farabet, Camille Couprie and Y. LeCun.\\nLearning hierarchical features for scene labeling.\\nPAMI,\\n35(8), 2013. 1, 2\\n[4] C. Couprie, C. Farabet, L. Najman, and Y. LeCun. Indoor\\nsemantic segmentation using depth information. Internatinal\\nConference on Learning Representation, 2013. 2\\n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In CVPR, volume 1, 2005. 5\\n[6] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. V.\\nLe, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang,\\nand A. Y. Ng. Large scale distributed deep networks. In\\nNIPS, 2012. 4\\n[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. Imagenet: A large-scale hierarchical image database. In\\nCVPR, 2009. 2\\n[8] P. Doll´ar, V. Rabaud, G. Cottrell, and S. Belongie. Behav-\\nior recognition via sparse spatio-temporal features. In Inter-\\nnational Workshop on Visual Surveillance and Performance\\nEvaluation of Tracking and Surveillance, 2005. 2, 5\\n[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\\nture hierarchies for accurate object detection and semantic\\nsegmentation. In CVPR, 2014. 1, 2\\n[10] S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neural\\nnetworks for human action recognition. PAMI, 35(1):221–\\n231, 2013. 2, 3\\n[11] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet clas-\\nsiﬁcation with deep convolutional neural networks. In NIPS,\\n2012. 1, 2, 3, 4\\n[12] I. Laptev. On space-time interest points. IJCV, 64(2-3):107–\\n123, 2005. 2\\n[13] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld.\\nLearning realistic human actions from movies.\\nIn CVPR,\\n2008. 2\\n[14] Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learn-\\ning hierarchical invariant spatio-temporal features for action\\nrecognition with independent subspace analysis. In CVPR,\\n2011. 2\\n[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proceed-\\nings of the IEEE, 86(11):2278–2324, 1998. 1, 2\\n[16] J. Liu, J. Luo, and M. Shah. Recognizing realistic actions\\nfrom videos “in the wild”. In CVPR, 2009. 2\\n[17] J. C. Niebles, C.-W. Chen, and L. Fei-Fei. Modeling tempo-\\nral structure of decomposable motion segments for activity\\nclassiﬁcation. In ECCV, pages 392–405. Springer, 2010. 2\\n[18] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-\\nson. CNN features off-the-shelf: an astounding baseline for\\nrecognition. arXiv preprint arXiv:1403.6382, 2014. 1, 2\\n[19] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neu-\\nral networks applied to house numbers digit classiﬁcation. In\\nICPR, 2012. 2\\n[20] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y. LeCun. OverFeat: Integrated recognition, localization\\nand detection using convolutional networks. arXiv preprint\\narXiv:1312.6229, 2013. 1, 2\\n[21] J. Sivic and A. Zisserman. Video Google: A text retrieval\\napproach to object matching in videos. In ICCV, 2003. 2\\n[22] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset\\nof 101 human actions classes from videos in the wild. arXiv\\npreprint arXiv:1212.0402, 2012. 2, 7\\n[23] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Con-\\nvolutional learning of spatio-temporal features. In ECCV.\\nSpringer, 2010. 2\\n[24] M. Varma and A. Zisserman. A statistical approach to tex-\\nture classiﬁcation from single images. IJCV, 62(1-2):61–81,\\n2005. 5\\n[25] H. Wang, A. Klaser, C. Schmid, and C.-L. Liu. Action recog-\\nnition by dense trajectories. In CVPR. IEEE, 2011. 2, 8\\n[26] H. Wang, M. M. Ullah, A. Klaser, I. Laptev, C. Schmid, et al.\\nEvaluation of local spatio-temporal features for action recog-\\nnition. In BMVC, 2009. 2\\n[27] W. Yang and G. Toderici.\\nDiscriminative tag learning on\\nyoutube videos with latent sub-tags. In CVPR, 2011. 5\\n[28] M. D. Zeiler and R. Fergus.\\nVisualizing and under-\\nstanding convolutional neural networks.\\narXiv preprint\\narXiv:1311.2901, 2013. 1, 3'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 0}, page_content='Research Article\\nFootball Game Video Analysis Method with Deep Learning\\nNian Liu\\n,1 Lu Liu,2 and Zengjun Sun\\n2\\n1Department of Sports, Zhejiang Gongshang University, Hangzhou 310000, Zhejiang, China\\n2Business & Public Administration, Namseoul University, Cheonan 31020, Republic of Korea\\nCorrespondence should be addressed to Zengjun Sun; xiangyu0714@hotmail.com\\nReceived 8 April 2022; Accepted 17 May 2022; Published 8 June 2022\\nAcademic Editor: Jun Ye\\nCopyright © 2022 Nian Liu et al. Tis is an open access article distributed under the Creative Commons Attribution License, which\\npermits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\\nFootball is a beloved sport, and its wide audience makes football video one of the most analytically valuable types of video.\\nResearchers have achieved certain research results in football video content analysis. How to locate interesting event clips from a\\ncomplete long video is an urgent issue to be addressed in football game video analysis. Te granularity of sports event detection\\nresults with traditional machine learning is relatively coarse, and the types of events that can be detected are limited. In recent\\nyears, deep learning has made good progress in the research of video single-person events and action detection, but there are few\\nachievements in the detection of sports video events. In response to this problem, this work uses a deep learning method to build\\nan event detection model to detect events contained in football videos. Te whole model is divided into two stages, in which the\\nﬁrst stage is utilized to generate candidate event fragments. It divides the football video to be detected into a sequence of frames of\\na certain length and scans using a sliding window. Multiple frame sequences within a sliding window form a segment, and each\\nsegment is a prediction unit. Te frame sequence features within the segment are obtained through a three-dimensional\\nconvolutional neural network, which is used as the input of each time point of the bidirectional recurrent neural network and\\nfurther integrated to generate the event prediction of the segment. Te second stage is to further process the above results to\\nremove all segments predicted as nonevents. Te thresholds are set according to the detection eﬀect of various events to ﬁlter out\\nevent fragments with higher probability values, obtain the start and end positions of the events through merging, classify and mark\\nthem, and ﬁnally output complete event fragments. Tis work has carried out comprehensive and systematic experiments to verify\\ncorrectness of the proposed method.\\n1. Introduction\\nPeople’s production and lifestyle have undergone tremen-\\ndous changes. Traditional information media, such as books,\\ndocuments, newspapers, records, movies, and TV programs,\\nare increasingly appearing in people’s daily life in digital\\nform. Compared with other information sources, video has\\nthe characteristics of intuition and large amount of infor-\\nmation and occupies a large share in practical applications.\\nWhile bringing a rich audio-visual experience, the rapid\\nexpansion of the number of digital videos has also made the\\nproblem of insuﬃcient video processing capacity more\\nprominent, posing new challenges to the management,\\nediting, and distribution of digital media. How to intelli-\\ngently analyze the content of video clips through a computer,\\nso as to automatically retrieve and locate relevant video clips\\nthrough a computer according to user requirements, has\\nbecome an urgent issue to be solved in industry and aca-\\ndemia. As a result, video content analysis technology has\\ngradually become one of the current research hotspots with\\nimportant theoretical and practical signiﬁcance [1–5].\\nResearchers are trying to ﬁgure out how low-level fea-\\ntures can be mapped to higher-level semantics so that users\\nmay get more convenient content acquisition services from\\nvideo analysis technologies, which is the primary purpose of\\nthe research. Tere is a wide variety of video content, and the\\ncontent found in videos from various professions varies\\ngreatly. Since video content analysis technology cannot be\\nuniversal, an appropriate identiﬁcation procedure must be\\ndevised based on the examined content’s features. Football\\nvideo has a huge fan base and a lot of money riding on it, so\\nresearchers have ﬂocked to it because of the wealth of data it\\nprovides for content analysis. Football video content analysis\\nis in high demand since football fans want to view only the\\nHindawi\\nComputational Intelligence and Neuroscience\\nVolume 2022, Article ID 3284156, 12 pages\\nhttps://doi.org/10.1155/2022/3284156'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 1}, page_content='most relevant clips, such as highlights and key moments.\\nMany football fans and professionals want an easy way to\\nrapidly ﬁnd speciﬁc attack types among a big number of\\nassault footage. In order to better comprehend the gaming\\nprocess and achieve better results, these users anticipate to\\nevaluate the techniques and tactics used in past games.\\nFootball video strategy and tactical analysis research is sparse,\\nand the relevant needs have not been fully satisﬁed [6–10].\\nFootball video event detection has gradually become an\\nimportant research content in the current sports video analysis\\nﬁeld, as illustrated in Figure 1. When detecting events in sports\\nvideos based on the traditional machine learning methods, the\\nlow-level features of the videos are mainly extracted.\\nHowever, the color of the football video itself is relatively\\nsimple, and the events mainly occur in the medium and long\\nshots. Te appearance outline of some things is diﬃcult to\\naccurately capture, so it cannot show a good detection eﬀect.\\nLater, an artiﬁcial mechanism was added, using human prior\\nknowledge and established rules to assist. Although this\\nimproves the detection performance to a certain extent, be-\\ncause the rules are relatively simple, it cannot fully express the\\nrelationship between objects and people’s cognitive emotions.\\nTe quality of the detection results largely depends on human\\nexperience, and the scalability of the model is poor. Based on\\nthe successful application of semantic analysis in the ﬁeld of\\nartiﬁcial intelligence, some scholars have further introduced it\\ninto the ﬁeld of video analysis. From the perspective of visual\\nunderstanding, the original data in the video are abstracted\\ninto a concept that conforms to human vision. Te idea of\\nsemantic analysis is the same as people’s habit of cognition of\\nthings. Te semantic expression of video is the deep inter-\\npretation of video content by people. Te emergence of deep\\nlearning has enabled videos to achieve better results in se-\\nmantic analysis. Deep learning has made signiﬁcant progress\\nin image recognition, video analysis, and other ﬁelds. It is a\\nneural network that can simulate human learning behavior.\\nBy constructing a multilayer model and using a large amount\\nof data for training, it can automatically learn more useful\\nfeatures, which can fully express the internal information of\\nvideo data. Deep learning is an important branch [11–15].\\nTe purpose of this research is to analyze the semantic\\nfeatures in football videos and use the deep learning method to\\ncomplete the event detection and classiﬁcation of football\\nmatch videos. Te main work includes the following aspects:\\n(1) event classiﬁcation, using the convolutional neural network\\nto extract features from existing video clip sequences, including\\nglobal features and local features. By further integrating the\\nfeatures, the event type to which the segment belongs is\\npredicted. (2) Event detection, adding a recurrent neural\\nnetwork with a classiﬁcation network to further learn sequence\\nfeatures and determine the time boundary of the event. Ten,\\nby ﬁltering and merging fragments containing events, complete\\nevents of diﬀerent lengths are obtained and types are marked.\\n2. Related Work\\nReference [16] used CNN to separately extract features from\\nimages of video frames and integrate features of all frames to\\nform video descriptors. It concatenated the video’s RGB\\ndescriptor and optical ﬂow descriptor to form a video de-\\nscriptor. Reference [17] proposed a three-dimensional con-\\nvolution method that could convolve from the space of the\\nvideo image and the time of the frame sequence at the same\\ntime. By concatenating multiple convolution layers and\\ndownsampling layers to form an action recognition network,\\nit had achieved good results in human action recognition in\\nsurveillance videos. Reference [18] also proposed a 3D con-\\nvolution method, which could be well integrated with 2D\\nconvolution network to form a C3D network. Reference [19]\\nproposed a network structure that can perform feature ex-\\ntraction in spatial and temporal information of video. Dif-\\nferent from the above two kinds of 3DCNNs that performed\\nconvolution in space and time at the same time, this method\\nseparated the convolution in space and space, ﬁrst convolving\\nin space, and then convolving in time dimension. Reference\\n[20] utilized 2D CNN as well as 3D CNN to integrate features\\nof diﬀerent granularities on video frame sequences and then\\ncombined LSTM for action recognition. References [21, 22]\\nused 2D convolution and 3D convolution to process video\\nimages and fused two branches at fully connected layer to\\nobtain action type. In reference [23], a new network con-\\nvolution method was proposed, which made the image feature\\nsequence and optical ﬂow image sequence better fuse and\\nrealized the end-to-end training of the network. Reference\\n[24] ﬁne-tuned the existing CNN model and only fused all the\\nframes of the video through the maximum pooling layer at the\\nend to obtain the description features of the video. Reference\\n[25] used Fisher vector or VLAD encoding to further re-\\nencode the video features extracted by CNN, which made the\\nfeature utilization more eﬃcient. In reference [26], in addition\\nto using FC features, features of multiple convolutional layers\\nin the middle of the network were also used to construct video\\nfeatures. Reference [27] used CNN to extract features of\\noriginal frame image and optical ﬂow image respectively,\\nobtained the image features and optical ﬂow features of the\\nentire video through pooling operation, and used LSTM to\\nprocess the two features respectively. Ten, it combined the\\nresults of the two to-do video classiﬁcation. Reference [28]\\nalso combined CNN and multilayer LSTM for video event\\nanalysis and set the output of LSTM as text, so that the model\\ncould ﬁnally generate a simple text description of the video.\\nReference [29] used multibox to detect the player’s\\nposition area in the video frame for the basketball game\\nvideo and then used the inception network to extract the\\nvideo frame and the features of each player respectively. It\\nused BLSTM to process the frame sequence and player\\nfeature sequence respectively and used the output of these\\ntwo networks as input to construct an event recognition\\nLSTM. Reference [30] also combined CNN and LSTM to\\ndetect volleyball video events, used CNN to extract player\\nfeatures, and built LSTM network for each player separately.\\nTe output of the single-person LSTM was pooled to obtain\\nthe overall state, and the LSTM was constructed based on\\nthis input to identify the overall activity state of the team.\\nReference [31] combined CNN and RNN-based visual at-\\ntention models to focus visual attention on keyframes for\\nevent prediction. It used reinforcement learning to train the\\nmodel, enabling the system to automatically learn during the\\n2\\nComputational Intelligence and Neuroscience'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 2}, page_content='interaction with the video image sequence, and directly\\noutput the time boundary of the action. Te next step of the\\nlocalization model should focus on the processed video\\nframes. Te research object of reference [32] was the ﬁrst-\\nperson motion video, and they used two CNN network\\nstructures to process the video at the same time. One was a\\ntraditional 2D CNN that analyzes the spatial static features of\\nvideo frames. Te other was 3D CNN, which analyzed dy-\\nnamic features in the temporal dimension of video frame\\nsequences. A sorting algorithm was proposed to sort the\\noutputs of the two networks and select the highlights in the\\nvideo. Reference [33] improved the C3D network, so that the\\nconvolution step size of the network in the time dimension\\ncould be adjusted according to the needs, and based on this,\\nthree networks with diﬀerent functions of preselection,\\nclassiﬁcation, and positioning were constructed to locate the\\naction time boundary. Reference [34] fused DT and CNN\\nfeatures of frame images and used a ﬁxed-length sliding\\nwindow to sequentially process the video to obtain the de-\\ntection results of action clips. References [35, 36] proposed a\\nmethod of extracting and integrating the dynamic features of\\nvideo clips in multiple temporal widths, centered on a certain\\nframe, and combining RNN to detect action temporal\\nboundaries. An action detection problem had been solved\\nutilizing a new CNN structure that uses CNN to identify the\\naction type and the sliding window to identify the boundary.\\nIn reference [37], the action detection network used C3D and\\na longer RNN as the core model of the action detection\\nnetwork and predicted time boundaries based on output\\nhidden states of the RNN. In addition, diﬀerent lengths of\\npredictions were generated for each forecast. Although the\\nboundary was predicted with RNN, although the boundary\\ndetermination of the sliding window is avoided in the de-\\ntection process, the longer RNN network structure was\\ndeeper. During the learning process, problems such as gra-\\ndient disappearance and diﬃculty in parameter learning were\\nprone to occur, which aﬀected the ﬁnal detection result.\\n3. Football Events Classification\\nEvent classiﬁcation is one of the research contents of event\\ndetection, and accurate classiﬁcation of events can improve\\nthe eﬀect of event detection to a certain extent. Te most\\nimportant thing in classiﬁcation is to obtain eﬀective in-\\nformation about events, so how to learn and describe event\\nfeatures are the main research contents of this section. Using\\ndeep learning methods, two-dimensional convolutional\\nnetworks and three-dimensional convolutional networks are\\nused to extract features from events.\\n3.1. Feature Extraction. Feature extraction is one of the\\nimportant research issues in the ﬁeld of video analysis. Te\\ntraditional hand-designed features are relatively indepen-\\ndent and cannot represent the deep content of the video well.\\nDeep learning adopts a multilayer learning method, which\\ncan automatically learn features from big data and combine\\nthem to form a more eﬀective expression. Te most im-\\nportant thing in deep learning is feature learning. Terefore,\\nthe reasonable selection of models for feature extraction and\\ndescription plays an important role in the eﬀect of event\\nclassiﬁcation. Since the application of convolutional neural\\nnetworks to handwritten digit recognition, CNN has made\\nsigniﬁcant progress in the ﬁeld of image recognition, and a\\nseries of improved models have emerged. Compared with\\nother models, when GoogLeNet achieves better classiﬁcation\\nresults, it uses fewer hierarchical structures and requires a\\nrelatively small scale of parameters. Terefore, this section\\nadopts Inception v2 from GoogLeNet as a candidate model\\nfor 2D convolutional networks. Te basic structure of In-\\nception v2 is the inception module, as shown in Figure 2.\\nDiﬀerent from the traditional method of directly stacking\\nconvolutional layers, Inception v2 is composed of multiple\\ninception modules. Two kinds of ﬁlters such as 1 × 1 and 3 × 3\\nare used in each module. Tis structure increases the width\\nand depth of the network without increasing the computa-\\ntional load. In addition, the 5 × 5 convolution kernel in In-\\nception v1 is replaced with two 3 × 3 convolution kernels,\\nwhich reduces the number of parameters and improves the\\ncalculation speed, while ensuring the same number of fea-\\ntures. Another improvement is to parallelize a 3 × 3 pooling\\noperation with convolution to reduce feature loss caused by\\npooling when compressing features. In order to solve the\\nproblem that the input distribution also changes when the\\nneural network is iterated due to parameter update during the\\ntraining process, Inception v2 normalizes the input data\\nbefore each layer is input. Tis avoids additional computa-\\ntional overhead for relearning diﬀerent distributions and\\nspeeds up neural network training.\\nAlthough CNN has an excellent performance in image\\nrecognition, there are certain problems when it is introduced\\ninto the ﬁeld of video analysis. Tere is a large correlation\\nbetween the front and rear frames in the video, and when\\nCNN processes video frames, it generally extracts features\\nfor each frame of image separately. Tis convolution method\\ndoes not take into account the interframe motion in the time\\ndimension, and it is easy to ignore the relevant information\\nbetween frames. Te frame processing process of 3D CNN is\\nsimilar to that of CNN, and the diﬀerences are that it adds a\\ntime dimension to CNN and can extract features of multiple\\nFigure 1: Te football event detection.\\nComputational Intelligence and Neuroscience\\n3'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 3}, page_content='frame images at the same time. When extracting video se-\\nquence features, it is possible to save the motion state\\nchanges between the frames before and after, thereby im-\\nproving the accuracy of event classiﬁcation. Terefore, this\\nsection adopts 3D CNN as a candidate model for 3D\\nconvolutional networks. When 3D CNN is used to extract\\nfeatures from a video frame sequence, the frame sequence\\ncan be regarded as a cube. It is convolved with a 3D con-\\nvolution kernel, which captures the motion information of\\nthe video. Since the convolution kernel weights of each layer\\nare shared throughout the convolution process, only one\\ntype of feature can be extracted. When multiple features are\\nrequired, multiple convolution kernels can be set.\\n3.2. Event Classiﬁcation Model. Te event classiﬁcation\\nmodel combines a 3D CNN and a Softmax classiﬁer, as\\nillustrated in Figure 3. Te input is an event segment, and the\\nsequence features are extracted from the original frame\\nimage through 3D CNN. Te probability value is calculated\\nusing the Softmax classiﬁer to get the predicted classiﬁcation\\nof the event.\\nTe 3D CNN consists of a convolution layer, activation\\nfunction, pooling layer, fully connected layer, and dropout\\nlayer. Te speciﬁc network structure is shown in Figure 4.\\nTe input is split into two parts: the full-frame image and the\\ncentral area of the frame. Tis is because the camera always\\nplaces the current important moving scene in the center of\\nthe camera during the shooting process. Terefore, the\\nprobability of occurrence of events in this part is higher, and\\nit is easier to capture the characteristics of events. Te central\\narea of the frame is half the size of the full frame.\\nConvolutional layers are used to extract features from\\nconsecutive frame sequences. Each layer uses a smaller\\nconvolution kernel to locally sense the input. In this way, each\\nneuron in a convolutional layer only needs to connect local\\nadjacent regions of the input unit, extracting local informa-\\ntion. Global information can then be obtained by integrating\\nat a high level. Compared with the traditional full connection\\nmethod, more redundant information will not be generated,\\nthereby reducing the complexity of network computing. In\\naddition, the convolution kernel parameters of each layer are\\nshared to reduce the number of parameters and improve the\\neﬃciency of feature extraction.\\nTe output of the convolutional layer is a linear function,\\nwhile in reality, it deals with more nonlinear problems.\\nTerefore, an activation function is added to convert a linear\\nfunction into a nonlinear function, so that the neural net-\\nwork has the ability to learn nonlinearly and solve more\\ncomplex problems. Te feature map output by the con-\\nvolutional layer will maintain the same size as the original\\ninput, so a pooling layer is set after each convolutional layer\\nto downsample the feature output of the convolutional layer.\\nAfter the local features are extracted, the features need to be\\nfurther integrated to obtain global information. Te fully\\nconnected layer adopts a fully connected method to connect\\nall neurons to each node of the previous layer, which can\\nfully learn all features and classify them. However, the fully\\nconnected layer has many parameters, so only a two-layer\\nstructure is used.\\nTe Softmax classiﬁer is a generalization of logistic re-\\ngression models for multiclassiﬁcation problems. Since there\\nare many diﬀerent event types in this experiment, the\\nSoftmax classiﬁer was chosen. Te calculation method is as\\nfollows:\\nze,c \\x88 Wcf + bc,\\np ye \\x88 c\\n\\x00\\U0010ff01\\x88\\neze,c\\n\\U0010ff50C\\nc\\x881 eze,c,\\n(1)\\nwhere ye is the event type predicted for event e and p(ye \\x88\\nc) is the probability value that event e is predicted to be\\nclass c.\\n3.3.ClassiﬁcationModelTraining. Te classiﬁcation model is\\nﬁne-tuned on the basis of the existing parameter model.\\nTerefore, in the 3D CNN network, the existing parameter\\nmodel is used as the initialization parameter, and the\\nlearning rate is 0.0001. Te size of the convolution kernel of\\neach convolutional layer is 3 × 3 × 3, and the sliding step size\\nis 1 in both time and space. Te pooling window size in the\\npooling layer is 2 × 2 × 2, and the sliding step size is 2. Te\\nInput\\n1×1 Conv\\n3×3 Maxpool\\n1×1 Conv\\n1×1 Conv\\n3×3 Conv\\n1×1 Conv\\n3×3 Conv\\nConcatnation\\n3×3 Conv\\nFigure 2: Te structure of inception.\\n4\\nComputational Intelligence and Neuroscience'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 4}, page_content='dimensions of the output of each layer of the fully connected\\nlayer are 8192 and 4096 dimensions, respectively. When\\nextracting features, 32 consecutive frames are randomly\\nselected from each event segment, and a sliding window of\\nsize 16 is used to divide the 32-frame sequence into multiple\\nsubsequences. Each subsequence is used as an input to the\\nmodel alone, with a sliding step size of 8. All parameters in\\nthe Softmax classiﬁer are randomly initialized with an initial\\nlearning rate of 0.01.\\nA neural network with only convolutional layers is a\\nsimple linear regression model, and when the input is\\ncomplex data types such as images, videos, and audio, it\\ncannot fully learn the intrinsic properties of these data.\\nTerefore, adding an activation function after the con-\\nvolutional layer can realize the nonlinear function mapping\\nof features to enhance the learning ability of the neural\\nnetwork. Tere are three choices of activation function:\\nsigmoid function, tanh function, and ReLU function. Al-\\nthough the sigmoid function is easy to understand and\\napply, it has slow convergence and is prone to saturation.\\nTere will be a gradient disappearance problem during\\nbackpropagation, resulting in the parameters not being\\nupdated. Te tanh function is improved on the basis of the\\nsigmoid function, but there is also the problem of gradient\\ndisappearance. ReLU is a piecewise function, and when the\\nfunction input is positive, it will keep the size of the value\\nunchanged. When the input is negative, it becomes 0,\\nmaking a subset of neurons in the neural network inactive to\\navoid overﬁtting the training set. In addition, since the ReLU\\nfunction is a linear function in the non-negative interval and\\nthe gradient is equal, the problem of gradient disappearance\\nis also avoided and the convergence speed of the model is\\nguaranteed. Taken together, in the training of the classiﬁ-\\ncation model, the ReLU function is used as the activation\\nfunction. Te three activation function formulas are as\\nfollows:\\nSigmoid(x) \\x88\\n1\\n(1 + exp(−x)),\\nTanh(x) \\x88 (exp(x) −exp(−x))\\n(exp(x) + exp(−x)),\\nReLU(x) \\x88 max(x, 0),\\n(2)\\nwhere x is the input feature.\\nFine-tuning the existing parameter model can save a lot\\nof training time. Te most important thing in ﬁne-tuning is\\nto select the parameter tuning layer. In the experiment, the\\n3D CNN was trained by three schemes, and the training\\neﬀects were compared. Te ﬁrst method is to ﬁx the pa-\\nrameters of all layers unchanged. Te second method is to ﬁx\\nall the parameters of the fully connected layer unchanged\\nand ﬁne-tune the parameters of other layers. Te third\\nmethod is to ﬁne-tune the parameters of all layers.\\n4. Football Event Detection\\nEvent detection is the process of locating the time boundary\\nof an event in a complete football video and then classifying\\nit. Te event detection model designed in this section is\\nbased on the classiﬁcation model and adds a time series\\nfeature integration module. In the model, the video is di-\\nvided into frame sequences of a speciﬁc length, the entire\\nvideo is scanned by a sliding window, and the starting\\nposition of the event is predicted by extracting and inte-\\ngrating the features of multiple frame sequences. Te entire\\nevent detection process is shown in Figure 5.\\n4.1. Timing Feature Integration. Given that 3D CNN has\\nachieved good results in event classiﬁcation, when building\\nan event detection model, 3D CNN is still used to extract\\nInput\\nConv1a\\nPool1\\nConv2a\\nPool2\\nConv3a\\nConv3b\\nPool3\\nConv4a\\nConv4b\\nPool4\\nConv5a\\nConv5b\\nPool5\\nFC6\\nFC7\\nOutput\\nFigure 4: Te structure of 3D CNN.\\nInput\\n3D CNN\\nSoftmax\\nEvent \\nClassification\\nFigure 3: Te event classiﬁcation model.\\nComputational Intelligence and Neuroscience\\n5'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 5}, page_content='features from frame sequences in videos. However, a\\ncomplete football video is usually 45–50 minutes long and\\ncontains tens of thousands of frames, and the correlation\\nbetween frames is relatively large. When only 3D CNN is\\nused, the sequence length that can be processed is limited,\\nand it is easy to ignore the dynamic information contained in\\nthe video. Terefore, in the event model, RNN is added to\\nintegrate the frame sequence features to further extract the\\nevent information contained in the video.\\nIn the RNN network structure, nodes between hidden\\nlayers are fully connected. It adds the output of the hidden\\nlayer at the previous time point to the current input to realize\\nthe memory of the previous information. In this way, the\\ncorrelation between the data before and after can be pre-\\nserved, and the sequence data can be processed more eﬃ-\\nciently. Tere are currently three popular RNN structures:\\ntraditional RNN, LSTM, and bidirectional recurrent net-\\nwork BRNN.\\nTe hidden unit is the memory unit of the traditional\\nRNN network. In addition to the input and output at each\\ntime point, it also receives the hidden state information from\\nthe previous time point and calculates it together with the\\ncurrent input. Te emergence of traditional RNN networks\\nhas achieved great success in machine translation, image\\ndescription generation, etc. but also has major shortcomings.\\nWhen it is trained by the backpropagation algorithm, the\\nRNN has a long time span and a deep level. Te calculated\\ngradient will show an exponential decrease or increase as the\\nlevel increases, and the gradient will disappear or the gra-\\ndient will explode, resulting in training failure. LSTM adds\\ncell states and control gates to the traditional RNN structure.\\nTere are three control gates in the structure, which control\\nwhether the input, output, and hidden layer states are added\\nto the unit state and the degree of inﬂuence on the unit state.\\nIn this way, the LSTM can selectively remember the key\\ninformation it needs. Whether it is a traditional RNN\\nnetwork or an improved LSTM network, forward memory is\\nused. In the video, what happens in the future has a strong\\ncorrelation with the current state. In this regard, this section\\nuses two layers of LSTM to form BLSTM to integrate se-\\nquence features, which can simultaneously access past and\\nfuture contextual information.\\n4.2. Time Boundary Detection. Te time for various football\\nevents to occur is generally 3 to 4 seconds, and they appear in\\nany time period in the game video. Terefore, in event\\ndetection, the video is ﬁrst divided into a sequence of frames\\nof a certain length. It uses a sliding window to scan the video\\nand multiple frame sequences in a sliding window form a\\ncandidate segment, so that after one scan, multiple candidate\\nsegments are generated. Each segment is regarded as an\\nindependent prediction unit. When multiple prediction re-\\nsults are generated for the same event, the segment with a\\nprobability value greater than a certain threshold is selected to\\nenter the postprocessing stage. Trough the sliding method,\\nthe entire input video can be uniformly processed and the\\ncontext information of each time point can be preserved.\\nAfter candidate segments are generated, multiple frame\\nsequences in each segment are input into 3D CNN to extract\\nfeatures. After BLSTM integration, the prediction result of\\nthis segment is generated, and the speciﬁc process is shown\\nin Figure 6.\\nAmong the existing algorithms for event detection or\\naction detection using sliding windows, most of them treat\\nthe entire sequence as a whole and extract features for all\\nframes. For short videos, better results can be achieved, while\\nfor football videos containing tens of thousands of frames,\\nthe amount of computation is greatly increased. Trough\\nanalysis, it is found that there is little change between two\\nadjacent frames. Terefore, in response to this problem, this\\nsection proposes to take one frame every two frames to form\\na frame sequence and then extract features. When a com-\\nplete video is detected, the number of frames processed is\\nreduced to one-half of the original, thus improving the\\ncomputational eﬃciency.\\nAfter extracting the sequence features, it is necessary to\\nuse LSTM to further integrate the features. After the\\nInput\\n3D CNN\\n3D CNN\\n3D CNN\\n3D CNN\\n3D CNN\\nBLSTM\\nBLSTM\\nBLSTM\\nEvent1\\nEvent2\\nEvent3\\nFigure 5: Football event detection process.\\n6\\nComputational Intelligence and Neuroscience'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 6}, page_content='sequence features are integrated, the Softmax classiﬁer is\\nused to calculate the probability value of the segment cor-\\nresponding to each category. Te category with the highest\\nprobability is selected as the predicted category of the seg-\\nment. Te results output by the event detection model are\\nﬁxed-size fragments that require a postprocessing stage to\\nfurther determine the boundaries where the event occurred.\\nSegments predicted to be nonevents in the output are ﬁrst\\nremoved, and other segments are classiﬁed by event type. Ten,\\naccording to the detection eﬀect of diﬀerent events, diﬀerent\\nprobability thresholds are set, and all segments whose predicted\\nprobability value is greater than the threshold are respectively\\nscreened out. Finally, in the candidate segments of each cat-\\negory, the adjacent segments whose frame diﬀerence is less\\nthan 32 frames are merged to obtain a complete event.\\n4.3. Model Training. Football events often occur in speciﬁc\\nscenes, which have a lot to do with the state of motion before\\nand after it. Terefore, before training, the dataset is ﬁrst\\naugmented. When using sliding windows of diﬀerent lengths\\nto segment long videos, the event will be included in segments\\nof diﬀerent lengths, and start and end frames will be recorded.\\nTe intersection ratio of each segment and annotated event\\nsegments is calculated, and the segments with IoU > 0.65 are\\nretained as positive samples to join the training set. Among\\nthem, the sliding window takes 32, 64, and 128 frames, and\\nIoU is used to calculate the overlap between the candidate\\nsegment and the annotated segment. In addition, nonevent\\nsamples of diﬀerent lengths are added as negative samples in\\nthe training set to improve the generalization ability of the\\nmodel.\\nTe event detection model is further trained on the basis\\nof the classiﬁcation model. Since the output category has more\\nnonevent samples than the classiﬁcation, when the parame-\\nters are initialized, the 3D CNN directly uses the corre-\\nsponding parameter values that have been trained in the\\nclassiﬁcation model as the initialization parameters. Te\\nBLSTM and Softmax classiﬁers use random initialization\\nparameters, the initial learning rates are set to 0.001 and 0.01,\\nrespectively, and SGD is used to update the parameters. In 3D\\nCNN, the kernel size is 3 × 3 × 3, the stride is 1, the pooling\\nwindow size is 2 × 2 × 2, the stride is 2, and the dropout rate is\\n0.5. Tere are 256 hidden units in the hidden layer in BLSTM.\\nWhen generating candidate segments, the sliding window size\\nis 64 frames and the sliding step size is 16 frames. Te\\nsequence size used to extract features is 16 frames and the\\nsliding step size is 8. In the training process, it is hoped to\\nobtain a model with relatively small parameters, so that even if\\nthe input data has a certain deviation, it will not have a great\\nimpact on the results. To a certain extent, the phenomenon of\\noverﬁtting is avoided. To this end, when setting the loss\\nfunction, in addition to calculating the classiﬁcation loss, L2\\nregularization is also added to limit certain parameters in the\\nloss function to penalize the weights of unimportant features.\\nTe calculation method is as follows:\\nL \\x88 L0 + λ\\n2m \\U0010ff58w2,\\n(3)\\nwhere L0 is the classiﬁcation loss.\\n5. Experiment and Discussion\\n5.1. Dataset. Te dataset used in this design is a self-made\\nsoccer match video dataset, with a total of 200 soccer match\\nvideos, including FIFA World Cup 2014, AFC Asian Cup\\n2015, and UEFA EURO 2016. Each game video is about 45\\nminutes long and has a frame rate of 25 fps. Te dataset\\ncontains shots and events. Only the data part about the event\\nis used in this design. Six types of events and corresponding\\nplayback shots are deﬁned in the data set, and the types and\\nquantities of each event are listed in Table 1.\\nPrecision and recall are utilized as evaluation metrics in\\nthis work:\\nPrecision \\x88\\nCorrect\\nCorrect + False,\\nrecall \\x88\\nCorrect\\nCorrect + Miss .\\n(4)\\n5.2. Evaluation on Football Events Classiﬁcation. First, the\\nfootball event classiﬁcation model proposed in this work is\\nevaluated, and the experimental results are illustrated in\\nFigure 7 and Table 2.\\nIt can be seen from the detection results that better\\ndetection results can be achieved when performing classi-\\nﬁcation prediction. Among them, the classiﬁcation eﬀect of\\ncorner kicks is the best, because corner kick events mainly\\noccur in the four corner kick areas of the court, the positions\\nare relatively ﬁxed, and the rules are clear, which has more\\nobvious distinguishing characteristics than other events.\\nAccuracy and recall are also higher for yellow cards and\\nfouls. Fouls usually cause players to fall to the ground, and\\nthe movement characteristics between players are relatively\\nobvious. After a yellow card occurs after a foul, the referee\\nshows the action of showing a yellow card continuously on\\nthe camera. Te referee’s jersey is diﬀerent from the players\\nand has a certain degree of recognition. Shooting incidents\\nand goal-scoring incidents occurred in very similar scenes,\\nand there were many misjudgments in the test. Te goal\\nevent can be regarded as a kind of shooting event, and the\\nonly diﬀerence is the position of the ball. In football games,\\nthere are many players and complex sports scenes, which are\\nBLSTM\\nInput\\nInput\\nInput\\n3D CNN\\n3D CNN\\n3D CNN\\nP\\nFigure 6: Event fragment prediction process.\\nComputational Intelligence and Neuroscience\\n7'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 7}, page_content='prone to occlusion, so the position of the ball cannot be\\naccurately determined. Coupled with the fact that the\\ntraining samples of shooting events are far more than the\\ntraining samples of goal events, it is easy to predict goal\\nevents as shooting events during classiﬁcation. Te classi-\\nﬁcation eﬀect of free kicks is poor, and there are two lo-\\ncations where free-kick events occur: inside the penalty area\\nand outside the penalty area. When the incident occurs in\\nthe penalty area, it has obvious characteristics due to the\\ncertain rules of the positional arrangement of the players.\\nWhen the incident occurs outside the penalty area, the scene\\nhas certain similarities with fouls, shots, etc., which will\\ncause misjudgment.\\nTis work uses the 3D CNN to extract features and verify\\nthe eﬀectiveness of this strategy, and we compare it with the\\nperformance of the Inception v2 network. Te experimental\\nresults are illustrated in Figure 8.\\nWhen using 3D CNN to extract features for event\\nsegments, the classiﬁcation performance is better than using\\nInception v2 to extract single frame features. Tis is because\\n3D CNN better preserves the dynamic information between\\nframes by extracting features from multiple frames at the\\nsame time. Tis dynamic information can eﬀectively im-\\nprove the eﬀect of event classiﬁcation.\\nTis work uses the ReLU activation function to perform\\nnonlinear activation of network features. To verify the ef-\\nfectiveness of this strategy, this work compares diﬀerent\\nactivation functions. Te experimental results are illustrated\\nin Figure 9.\\nIt is obvious that the football event classiﬁcation per-\\nformance when using the ReLU activation function is better\\nthan the sigmoid function and the tanh function.\\nAs mentioned earlier, when training the event classiﬁ-\\ncation network, three diﬀerent parameter tuning methods\\nare proposed, which are named PA, PB, and PC, respectively.\\nIn order to verify the impact of diﬀerent parameter tuning\\nmethods on network performance, a comparative experi-\\nment is carried out in this work, and the results are illus-\\ntrated in Figure 10.\\nIt can be seen that using the third parameter tuning\\nmethod to ﬁne-tune the parameters of all layers can achieve\\nthe best performance, which can constrain the network to\\nlearn more discriminative features.\\n5.3. Evaluation on Football Event Detection. To verify the\\nperformance of the football event detection network pro-\\nposed in this work, diﬀerent events are detected. Te ex-\\nperimental results are listed in Table 3, and the training loss\\nis illustrated in Figure 11.\\nIt can be seen that the football event detection algorithm\\nproposed in this work has obtained relatively good per-\\nformance on diﬀerent events. Tis proves the eﬀectiveness\\nand feasibility of this work.\\nTable 1: Distribution of various events.\\nEvent\\nTraining set\\nTest set\\nShot\\n1350\\n611\\nCorner kick\\n691\\n276\\nFree kick\\n652\\n259\\nYellow card\\n225\\n152\\nFoul\\n1683\\n690\\nGoal\\n155\\n72\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n1.2\\n1.4\\n1.6\\n1.8\\n2\\nTraining loss\\n50\\n100\\n150\\n200\\n250\\n300\\n0\\nEpoch\\nFigure 7: Te training loss on football events classiﬁcation.\\nTable 2: Football event classiﬁcation results.\\nEvent\\nPrecision\\nRecall\\nShot\\n78.8\\n84.9\\nCorner kick\\n90.1\\n95.6\\nFree kick\\n78.9\\n63.8\\nYellow card\\n97.9\\n81.2\\nFoul\\n87.1\\n89.9\\nGoal\\n30.6\\n14.9\\nAverage\\n77.2\\n71.7\\nPrecision\\nRecall\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n74\\n76\\n78\\n80\\nPerformance\\nInception v2\\n3D CNN\\nFigure 8: Comparison of Inception v2 and 3D CNN.\\n8\\nComputational Intelligence and Neuroscience'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 8}, page_content='As mentioned earlier, this work uses BLSTM to extract\\nvideo features. To verify the eﬀectiveness of this strategy, it is\\ncompared with the traditional LSTM method. Te experi-\\nmental results are illustrated in Figure 12.\\nIt is obvious that higher football event detection per-\\nformance can be obtained using BLSTM. Tis is because the\\nBLSTM network is a bidirectional network that can extract\\nmore robust features.\\nAs mentioned earlier, this work adopts the corre-\\nsponding dataset expansion strategy (DE) when training the\\nevent detection network. To verify the eﬀectiveness of this\\nstrategy, this work conducts comparative experiments to\\ncompare the event detection performance without DE and\\nwith DE, respectively. Te experimental results are illus-\\ntrated in Figure 13.\\nObviously, after using the dataset expansion strategy, the\\nprecision and recall indicators of the event detection model\\nhave been improved to a certain extent. Tis proves the\\nfeasibility of using this strategy in this work.\\nSimilarly, L2 regularization strategy is also used when\\noptimizing the network. In order to verify the improvement\\nof network performance by this strategy, corresponding\\nPrecision\\nRecall\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n74\\n76\\n78\\n80\\nPerformance\\nSigmoid\\nTanh\\nReLU\\nFigure 9: Comparison of diﬀerent activation functions.\\nPrecision\\nRecall\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n74\\n76\\n78\\n80\\nPerformance\\nPA\\nPB\\nPC\\nFigure 10: Comparison of diﬀerent tuning methods.\\nTable 3: Football event detection results.\\nEvent\\nPrecision\\nRecall\\nShot\\n19.7\\n85.1\\nCorner kick\\n39.2\\n86.9\\nFree kick\\n18.2\\n45.6\\nYellow card\\n10.5\\n45.2\\nFoul\\n12.3\\n55.7\\nGoal\\n18.5\\n6.7\\nAverage\\n19.7\\n54.2\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n1.2\\n1.4\\n1.6\\n1.8\\n2\\nTraining loss\\n50\\n100\\n150\\n200\\n250\\n300\\n0\\nEpoch\\nFigure 11: Te training loss on football event detection.\\nPrecision\\nRecall\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nPerformance\\nLSTM\\nBLSTM\\nFigure 12: Comparison of BLSTM and LSTM.\\nComputational Intelligence and Neuroscience\\n9'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 9}, page_content='comparative experiments are carried out. Te experimental\\nresults are illustrated in Figure 14.\\nIt is not diﬃcult to see that after using the L2 regula-\\nrization strategy, the optimization of the network is more\\neﬀective. Tis improves the relative performance of football\\nevent detection.\\nFinally, this work presents a visual example of football\\nvideo analysis, as shown in Figure 15.\\nPrecision\\nRecall\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nPerformance\\nL2\\nNo L2\\nFigure 14: Evaluation on L2 regularization strategy.\\nPrecision\\nRecall\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nPerformance\\nNo DE\\nDE\\nFigure 13: Evaluation on dataset expansion strategy.\\n10\\nComputational Intelligence and Neuroscience'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 10}, page_content='6. Conclusion\\nIn this work, a deep learning method is used to design a\\nfootball event detection algorithm for football game video\\nanalysis. Te algorithm can automatically detect and classify\\nvarious events in football game videos. Among them, the\\nthree-dimensional convolutional network is used for feature\\nextraction, which can process multiple frames of images at\\nthe same time, so as to retain relevant information between\\nframes. It uses a bidirectional recurrent network to integrate\\nfeatures from both positive and negative directions to obtain\\npast and future contextual information to improve the eﬀect\\nof event detection. Te main content is divided into two\\nparts: (1) classiﬁcation of football events. Te classiﬁcation\\nmodel employs the 3D CNN network and the Softmax\\nclassiﬁer for feature extraction and predictive classiﬁcation\\nfor event segments, respectively. According to the charac-\\nteristics of the football game video, the model input is di-\\nvided into a full-frame image and a central area of the frame,\\nwhich are respectively put into 3D CNN to extract features,\\nand feature fusion is performed. Te Softmax classiﬁer\\ncalculates the predicted value of each category for the event\\nsegment and selects the one with the largest predicted value\\nas the predicted category of the event. (2) Football event\\ndetection: the event detection model is based on the clas-\\nsiﬁcation model by adding the BLSTM structure to better\\nobtain dynamic information between multiple frame se-\\nquences. During training, the dataset is expanded ﬁrst, and\\nthen the event detection model is optimized using the SGD\\nalgorithm. During testing, a sliding window is used to seg-\\nment the video, input each segment into the model, and\\ncalculate the predicted values for all the corresponding cat-\\negories. Trough ﬁltering and merging, the start and end\\nboundaries of events are further conﬁrmed, and category\\nlabels are generated. To verify the validity and correctness of\\nproposed method, comprehensive and systematic experi-\\nments are carried out, and the model is analyzed from dif-\\nferent aspects. Te experimental results conﬁrm the feasibility\\nof this work.\\nData Availability\\nTe datasets used during the current study are available from\\nthe corresponding author on reasonable request.\\nConflicts of Interest\\nTe authors declare that they have no conﬂicts of interest.\\nReferences\\n[1] B. B. Benuwa, Y. Zhan, A. Monney, B. Ghansah, and\\nE. K Ansah, “Video semantic analysis based kernel locality-\\nsensitive discriminative sparse representation,” Expert Sys-\\ntems with Applications, vol. 119, pp. 429–440, 2019.\\n[2] M. M. Elgamml, F. S. Abas, and H. A. Goh, “Semantic analysis\\nin soccer videos using support vector machine,” International\\nJournal of Pattern Recognition and Artiﬁcial Intelligence,\\nvol. 34, no. 09, Article ID 2055018, 2020.\\n[3] B. Ji, Y. Li, D. Cao, C. Li, S. Mumtaz, and D. Wang, “Secrecy\\nperformance analysis of UAV assisted relay transmission for\\ncognitive network with energy harvesting,” IEEE Transactions\\non Vehicular Technology, vol. 69, no. 7, pp. 7404–7415, 2020.\\n[4] Z. Wang, “Semantic analysis based on fusion of audio/visual\\nfeatures for soccer video,” Procedia Computer Science,\\nvol. 183, pp. 563–571, 2021.\\n[5] L. L. Dias, E. Barr´ere, and J. F. de Souza, “Te impact of\\nsemantic annotation techniques on content-based video lecture\\nrecommendation,” Journal of Information Science, vol. 47,\\nno. 6, pp. 740–752, 2021.\\n[6] F. Della Villa, M. Buckthorpe, A. Grassi et al., “Systematic\\nvideo analysis of ACL injuries in professional male football\\n(soccer): injury mechanisms, situational patterns and bio-\\nmechanics study on 134 consecutive cases,” British Journal of\\nSports Medicine, vol. 54, no. 23, pp. 1423–1432, 2020.\\n[7] X. Lin, J. Wu, S. Mumtaz, S. Garg, J. Li, and M Guizani,\\n“Blockchain-based on-demand computing resource trading in\\nIoV-assisted smart city,” IEEE Transactions on Emerging Topics\\nin Computing, vol. 9, no. 3, pp. 1373–1385, 2021.\\n[8] A. Serner, A. B. Mosler, J. L. Tol, R. Bahr, and A Weir,\\n“Mechanisms of acute adductor longus injuries in male football\\nplayers: a systematic visual video analysis,” British Journal of\\nSports Medicine, vol. 53, no. 3, pp. 158–164, 2019.\\n[9] G. J. Tierney, C. Kuo, L. Wu, D. Weaving, and D. Camarillo,\\n“Analysis of head acceleration events in collegiate-level\\nAmerican football: a combination of qualitative video analysis\\nand in-vivo head kinematic measurement,” Journal of Bio-\\nmechanics, vol. 110, Article ID 109969, 2020.\\n[10] C. Klein, P. Luig, T. Henke, H. Bloch, and P. Platen, “Nine\\ntypical injury patterns in German professional male football\\n(soccer): a systematic visual video analysis of 345 match\\nMatch 2\\nMatch 1\\nFigure 15: Te visual example of football video analysis.\\nComputational Intelligence and Neuroscience\\n11'),\n",
       " Document(metadata={'producer': 'PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp™ 5.4.0 ©2000-2012 1T3XT BVBA (AGPL-version))', 'creator': 'Aspose Ltd.', 'creationdate': '2022-06-07T14:06:51+05:30', 'source': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'file_path': '../data/pdf_files/03.A Deep Learning Algorithm for Special Action Recognition of Football.pdf', 'total_pages': 12, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-06-08T14:17:31+00:00', 'trapped': '', 'modDate': 'D:20220608141731Z', 'creationDate': \"D:20220607140651+05'30'\", 'page': 11}, page_content='injuries,” British Journal of Sports Medicine, vol. 55, no. 7,\\npp. 390–396, 2021.\\n[11] K. Br¨ummer, “Coordination in sports teams – ethnographic\\ninsights into practices of video analysis in football,” European\\njournal for sport and society, vol. 16, no. 1, pp. 27–43, 2019.\\n[12] A. V. Austin, P. Sasser, K. Tanabe, J. M. MacKnight, and\\nJ. B. Kent, “Video analysis of concussion exposures in a national\\ncollegiate athletic association division I football team,” Or-\\nthopaedic journal of sports medicine, vol. 8, no. 2, 2020.\\n[13] B. Mahaseni, E. R. M. Faizal, and R. G. Raj, “Spotting football\\nevents using two-stream convolutional neural network and\\ndilated recurrent neural network,” IEEE Access, vol. 9,\\npp. 61929–61942, 2021.\\n[14] V. Ellapan and R. Rajkumar, “Event detection in sports video\\nbased on audio-visual and support vector machine. Case-\\nstudy: football,” International Journal of Internet Technology\\nand Secured Transactions, vol. 9, no. 1/2, pp. 26–36, 2019.\\n[15] P. Bauer and G. Anzer, “Data-driven detection of counter-\\npressing in professional football,” Data Mining and Knowl-\\nedge Discovery, vol. 35, no. 5, pp. 2009–2049, 2021.\\n[16] A. Ullah, J. Ahmad, K. Muhammad, M. Sajjad, and S. W Baik,\\n“Action recognition in video sequences using deep Bi-di-\\nrectional LSTM with CNN features,” IEEE Access, vol. 6,\\npp. 1155–1166, 2018.\\n[17] S. Ji, W. Xu, K. Yu, M. Yang et al., “3D convolutional neural\\nnetworks for human action recognition,” IEEE Transactions\\non Pattern Analysis and Machine Intelligence, vol. 35, no. 1,\\npp. 221–231, 2013.\\n[18] F. U. M. Ullah, A. Ullah, K. Muhammad, I. U. Haq, and\\nS. W Baik, “Violence detection using spatiotemporal features\\nwith 3D convolutional neural network,” Sensors, vol. 19,\\nno. 11, p. 2472, 2019.\\n[19] Z. Sun, Q. Ke, H. Rahmani, and G. Wang, “Human action\\nrecognition from various data modalities: a review,” 2020,\\nhttps://arxiv.org/abs/2012.11866.\\n[20] G. Yao, T. Lei, and J. Zhong, “A review of Convolutional-\\nNeural-Network-based action recognition,” Pattern Recog-\\nnition Letters, vol. 118, pp. 14–22, 2019.\\n[21] Y. Han, P. Zhang, T. Zhuo, and W Huang, “Going deeper with\\ntwo-stream ConvNets for action recognition in video sur-\\nveillance,” Pattern Recognition Letters, vol.107, pp. 83–90, 2018.\\n[22] Y. Hu, A. Soltoggio, R. Lock, and S Carter, “A fully con-\\nvolutional two-stream fusion network for interactive image\\nsegmentation,” Neural Networks, vol. 109, pp. 31–42, 2019.\\n[23] F. Xiao, Y. J. Lee, K. Grauman, and J. Malik, “Audiovisual\\nslowfast networks for video recognition,” 2020, https://arxiv.\\norg/abs/2001.08740.\\n[24] K. Kumar and D. D. Shrimankar, “Deep event learning boosT-\\nup approach: delta,” Multimedia Tools and Applications,\\nvol. 77, no. 20, pp. 26635–26655, 2018.\\n[25] Z. Dong, C. Jing, M. Pei, and Y Jia, “Deep CNN based binary\\nhash video representations for face retrieval,” Pattern Recog-\\nnition, vol. 81, pp. 357–369, 2018.\\n[26] Y. G. Jiang, Z. Wu, J. Wang, X. Xue, and S. F. Chang,\\n“Exploiting feature and class relationships in video categori-\\nzation with regularized deep neural networks,” IEEE Trans-\\nactions on Pattern Analysis and Machine Intelligence, vol. 40,\\nno. 2, pp. 352–364, 2018.\\n[27] Z. Wu, Y. G. Jiang, X. Wang, and H. Ye, “Fusing multi-stream\\ndeep networks for video classiﬁcation,” 2015, https://arxiv.\\norg/abs/1509.06086.\\n[28] H. H. Pham, L. Khoudour, A. Crouzil, P. Zegers, and\\nS. A. Velastin, “Learning to recognise 3D human action from a\\nnew skeleton-based representation using deep convolutional\\nneural networks,” IET Computer Vision, vol. 13, no. 3,\\npp. 319–328, 2019.\\n[29] I. Ramos Vidal, “Detecting key actors in interorganizational\\nnetworks,” Cuadernos de Gesti´on, vol. 17, no. 2, pp. 63–86,\\n2017.\\n[30] M. Ziaeefard and R. Bergevin, “Semantic human activity\\nrecognition: a literature review,” Pattern Recognition, vol. 48,\\nno. 8, pp. 2329–2345, 2015.\\n[31] A. Newell, Z. Huang, and J. Deng, “Associative embedding:\\nend-to-end learning for joint detection and grouping [J],”\\nAdvances in Neural Information Processing Systems, p. 30,\\n2017.\\n[32] M. Sridevi and M. Kharde, “Video summarization using\\nhighlight detection and pairwise deep ranking model,” Pro-\\ncedia Computer Science, vol. 167, pp. 1839–1848, 2020.\\n[33] L. Yang, H. Peng, D. Zhang, J. Fu, and J. Han, “Revisiting\\nanchor mechanisms for temporal action localization,” IEEE\\nTransactions on Image Processing, vol. 29, pp. 8535–8548,\\n2020.\\n[34] H. Xia and Y. Zhan, “A survey on temporal action locali-\\nzation,” IEEE Access, vol. 8, pp. 70477–70487, 2020.\\n[35] P. Chen, C. Gan, G. Shen, W. Huang, R. Zeng, and M. Tan,\\n“Relation attention for temporal action localization,” IEEE\\nTransactions on Multimedia, vol. 22, no. 10, pp. 2723–2733,\\n2020.\\n[36] D. Zhang, L. He, Z. Tu, S. Zhang, F. Han, and B. Yang,\\n“Learning motion representation for real-time spatio-tem-\\nporal action localization,” Pattern Recognition, vol. 103, Ar-\\nticle ID 107312, 2020.\\n[37] L. Huang, Y. Huang, W. Ouyang, and L. Wang, “Modeling\\nsub-actions for weakly supervised temporal action localiza-\\ntion,” IEEE Transactions on Image Processing, vol. 30,\\npp. 5154–5167, 2021.\\n12\\nComputational Intelligence and Neuroscience')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import  PDFMinerLoader, PyMuPDFLoader\n",
    "\n",
    "\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf_files\", \n",
    "    glob=\"*.pdf\", \n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    loader_kwargs={},\n",
    "    show_progress=False\n",
    "    )\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
